var tipuesearch = {"pages": [ {"title":"About","text":" About I am a Reader in Computing Science at the University of Glasgow. My research interests are compilers and runtime systems for heterogeneous architectures. I am particularly interested in FPGAs and acceleration of climate and weather simulations. My homepage at University of Glasgow Why I do Computing Science research My homepage at University of Glasgow Why I do Computing Science research Updated September 25, 2021 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"about/index.html"}, {"title":"Imagine we had better weather forecasts","text":" Imagine we had better weather forecasts The last few days have shown that current weather forecasts are at the same time very advanced and yet not good enough. We had little warning of the very large amount of snow that would cripple our infrastructure. Earlier and more accurate warnings could help to limit the damage of such events (estimated at £470m a day just from the travel disruption). Also, better long-range predictions about the probabilities of such events in the future could help with investment and planning of infrastructure and services: should councils invest in more snow ploughs; should rail operators invest in making the network more resilient to extreme cold weather; how can the emergency services be kept running in such extreme conditions, etc.? So why are our forecasts not better? One of the main reasons is that the resolution of the weather forecasting computer models is at the moment still quite coarse. For example the MetOffice forecasting model, which is considered amongst the best in the world, divides the UK in squares of 1.5 km at its highest resolution, i.e. the simulation produces a single averaged value anywhere within this 1.5 km x 1.5 km area. The time resolution for the shortest-term forecast is 50 seconds. In contrast, for accurate simulation of local weather, a resolution of hundred metres and a time step of about a second are required. This would require a supercomputer a thousand times more powerful than the one currently in use by the MetOffice. A key problem with supercomputers is that they consume a lot of power. The current MetOffice supercomputer consumes 2.7 MW of electricity. A supercomputer a thousand times more powerful would need 2.7GW which is more than twice as much as all the electricity produced by the UK's largest nuclear power station, Hinkley Point B. To reduce the power consumption, new supercomputers have started using special hardware called accelerators. Already, both the fastest and most power-efficient supercomputers in the world use this technology. Unfortunately writing programs for such an accelerator-based supercomputer is very complicated. And existing programs can't benefit from accelerators without major changes. Weather forecasting models are very large and complex, with around a million lines of code. Rewriting such a program is extremely difficult and time consuming. So what's hindering progress? The holy grail is to develop a software technology that can automatically change legacy programs to make them suitable to the new, accelerator-based supercomputers. Many research groups, including my own, are working on such approaches. There is however a huge gap between a research proof-of-concept and a ready-for-use product and it takes considerable investment to bridge this gap. Unfortunately, a funding gap exists in this area: on the one hand, creating a product from a research proof-of-concept is not core research and can therefore not be funded by the Research Councils or through EU research funding. On the other hand, as there is no perceived commercial value in such a product (because the potential marked is very small), commercial funding is not an option. Imagine So imagine that the UK took a more joined-up view with investment to speed up the adoption of research. In the case of weather forecasting, this would help to minimize the impact on people and the economy of severe weather events like we experienced recently. It would be a thousandfold return on the investment. Updated March 04, 2018 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/better-forecasts/index.html"}, {"title":"The Winds of Kyoto","text":" The Winds of Kyoto I wrote a very short story for the Fifth Annual Kyoto Writing Competition. At the Disaster Prevention Research Institute of Kyoto University, researchers are modelling the flow of the wind over the city. The wind that blows over the tiny machiyas of Gion and around Kyoto station with its tall hotels and the Tower. In spring, it scatters a storm of sakura petals all over the city. In the rainy season, it blows down from Arashiyama along the waters of the Katsuragawa, and carries streams of clouds over the hills. When the temple ponds are full of lotus flowers, it stirs the lanterns and chases the incense smoke, and cools the faces of the teams that pull the Yamaboko along Shijou Doori. At the end of summer, when the higanbana is blooming, that same wind follows the railway line to Uji and blows in through the open window of the researchers’ offices, rifling the stacks of diagrams that reveal its very flow. But sometimes that flow gathers to a tremendous strength, and then a typhoon will lacerate the city. The models of the wind predicts this, to make sure the people of Kyoto are prepared. Then Kyoto hunkers down, and waits. When the storm has passed, the sky is a transparent clear and blue. The wind of Kyoto is once again cool and mild, blowing gently over the city. [1] Yoshida, T., Takemi, T. & Horiguchi, M. Large-Eddy-Simulation Study of the Effects of Building-Height Variability on Turbulent Flows over an Actual Urban Area. Boundary-Layer Meteorol 168, 127–153 (2018). Updated March 28, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/the-winds-of-kyoto/index.html"}, {"title":"Musings of an Accidental Computing Scientist","text":" Musings of an Accidental Computing Scientist September 20, 2021 Haku: a Japanese programming language Haku is a natural language functional programming language based on literary Japanese and written in Raku June 29, 2021 Frugal computing On the need for low-carbon and sustainable computing and the path towards zero-carbon computing. December 02, 2020 Writing faster Raku code Sometimes you want your Raku code to be faster. What does it take? October 05, 2020 Reconstructing Raku's Junctions A reconstruction of Raku's junctions as an algebraic data with higher-order functions. October 04, 2020 The strange case of the greedy junction An illustration that Raku's junctions are greedy by design, and a proposal. September 13, 2020 A universal interpreter The Böhm-Berarducci encoding of a type can be considered as a universal interpreter. We illustrate this in Raku with an evaluator and pretty printer. September 12, 2020 Encoding types as functions in Raku The Böhm-Berarducci encoding is a way to express an algebraic data type as a function type. We show how to do this in Raku using roles. August 07, 2020 Function types A brief introduction into function types, with a way to implement them in Raku and examples in many languages. July 18, 2020 Cleaner code with functional programming An introduction to some powerful functional programming techniques in Raku and Python. July 03, 2020 Everything is a function Although it might seem that a language like Haskell has a lot of different objects and constructs, they can all be reduced to functions. June 22, 2020 List-based parser combinators in Haskell and Raku This is a follow-on of my article on algebraic data types, with list-based parser combinators as a practical application. June 05, 2020 Roles as Algebraic Data Types in Raku Algebraic data types are great for building complex data structures, and easy to implement in Raku using roles. March 28, 2020 The Winds of Kyoto A very short story for the Fifth Annual Kyoto Writing Competition. April 27, 2019 Writing faster Perl code Sometimes your pure Perl code needs to be as fast as possible. What does it take? October 21, 2018 Why Europe needs hurricane contingency planning Hurricanes will become common in Europe in the next few decades. I discuss the evidence, the impact and the need for proper contingency planning. April 19, 2018 Hacking the Pleroma: Elixir, Phoenix and a bit of ActivityPub A brief guide into hacking Pleroma, a federated microblogging server software. March 13, 2018 A little maths puzzle in two parts Of which the outcome is a way to construct a regular pentagon using compass and ruler. March 05, 2018 A strategy for articles/debugging For a long time it has been my contention that for a developer, more that programming, debugging should be treated as a core skill. March 04, 2018 Imagine we had better weather forecasts Current weather forecasts are at the same time very advanced and yet not good enough. Earlier and more accurate warnings could help to limit the damage of su... Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/index.html"}, {"title":"A little maths puzzle in two parts","text":" A little maths puzzle in two parts I like solving little maths puzzles, deriving known results using nothing more complicated than secondary school level trigonometry, algebra and maybe a little calculus. The one below is actually two puzzles, but both have to do with regular polygons. Part 1 Approximating π by the perimeter of a regular polygon Can one approximate $\\pi$ by the perimeter of regular polygon with increasing numbers of sides inscribed in a unit circle, so starting with a triangle, then a hexagon, a dodecagon,... ? You may say that is obvious: the more sides, the closer the polygon approximates a circle. But I still wanted to work out the proof. Part 2 How to construct a pentagon using a compass and a ruler People have worked out how to construct a pentagon using only a compass and a ruler long ago. Nevertheless, I wanted to derive the construction from first principles. From Figure 2 we can write down some straightforward relationships between the length of a side of a pentagon (b from the previous part) and the angle of the arc, $\\frac{2\\pi}{5}$. $$\\begin{equation} x=\\frac{b}{2}=sin\\frac{\\pi}{5}\\label{eq:2.1} \\end{equation}$$ $$\\begin{equation} y=cos\\frac{\\pi}{5}\\label{eq:2.2} \\end{equation}$$ $$\\begin{equation} x^{2}+y^{2}=1\\label{eq:2.3} \\end{equation}$$ $$\\begin{equation} q = 2sin\\frac{2\\pi}{5}=4sin\\frac{\\pi}{5}cos\\frac{\\pi}{5}\\label{eq:2.4}\\end{equation}$$ Substitution of Eqs. $\\ref{eq:2.1}$ and $\\ref{eq:2.2}$ gives $$\\begin{equation} q = 4xy\\label{eq:2.5}\\end{equation}$$ Now we consider the right triangle with hypothenuse q: $$\\begin{equation} q^{2} = x^{2}+(1+y)^{2}\\label{eq:2.6}\\end{equation}$$ Substitution of Eq. $\\ref{eq:2.3}$ in the RHS of Eq. $\\ref{eq:2.6}$ and refactoring gives: $$\\begin{equation} q^{2} = 2+2y\\label{eq:2.7}\\end{equation}$$ Substitution of Eq. $\\ref{eq:2.3}$ and Eq. $\\ref{eq:2.5}$ in the LHS of Eq. $\\ref{eq:2.7}$ and refactoring gives: $$\\begin{equation} (4-(2y)^{2})(2y)^{2} = 2+2y\\label{eq:2.8}\\end{equation}$$ Now we define $z=2y$ and rewrite Eq. $\\ref{eq:2.8}$ as: $$\\begin{equation} (4-z^{2}).z^{2}=2+z\\label{eq:2.9} \\end{equation}$$ Which after more refactoring finally gives $$\\begin{equation} z^{2}(2-z)-1=0\\label{eq:2.10} \\end{equation}$$ This is a third-order equation but fortunately there is an obvious root for $z=1$. After some factorization we obtain the remaining second-order equation: $$\\begin{equation} z^{2}-z-1=0\\label{eq:2.11} \\end{equation}$$ The roots of this equation are: $$\\begin{align} z & = & \\frac{1\\pm\\sqrt{(-1)^{2}-4.1.(-1)}}{2.1}\\label{eq:2.12}\\ & = & \\frac{1\\pm\\sqrt{5}}{2}\ onumber \\end{align}$$ This is actually a very famous equation and its positive root is known as the Golden ratio. $$\\begin{align} \\phi & = & \\frac{1}{a} & = & \\frac{a}{1-a}\\label{eq:goldenratio} \\end{align}$$ Clearly y as defined is positive so $y=\\frac{z}{2}=\\frac{\\phi}{2}$ or $$\\begin{equation} y=\\frac{1+\\sqrt{5}}{4}\\label{eq:2.13} \\end{equation}$$ From Eq. $\\ref{eq:2.13}$ we can express b in terms of y using Eqns $\\ref{eq:2.1}$, $\\ref{eq:2.2}$ and $\\ref{eq:2.3}$: $$\\begin{equation} b=2\\sqrt{1-y^{2}}\\label{eq:2.14} \\end{equation}$$ And so we obtain the expression for the length of the side of a pentagon as $$\\begin{equation} b=\\sqrt{\\frac{5-\\sqrt{5}}{2}}\\label{eq:2.15} \\end{equation}$$ The remaining question is then, how do we construct a line of length b using a rules and compass? We do this indirectly, by constructing a line of length y as shown in Figure 3. First, we construct a line of length 1/2. Then the hypothenuse of the right triangle with sides 1/2 and 1 has a lenght of $\\frac{\\sqrt{5}}{2}$. We add this to the 1/2 by drawing an arc of radius $\\frac{\\sqrt{5}}{2}$ using the compass. This way we get a line of length $\\phi = \\frac{1+\\sqrt{5}}{2}$. Dividing this into two gives y and through the way we constructed this, we immediately get b as well and so we can construct the pentagon using arcs of radius b. The equation we solved to obtain y (Eq. $\\ref{eq:2.10}$) is a third order equation, and its other positive root is $z=1$. This shows in a way the danger of transforming a geometry problem into algebra: only one of these roots, $z=\\phi$, corresponds to a solution of our geometric problem. But there is also a geometric interpretation for the root $z=1$. Substitution of $y=1/2$ in the equations for results in $q=\\sqrt{3}$ and $b=\\sqrt{3}$, in other words a regular triangle inscribed in the unit circle. Updated March 13, 2018 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/constructing-a-pentagon/index.html"}, {"title":"A strategy for articles/debugging","text":" A strategy for debugging For a long time it has been my contention that for a developer, more than programming, debugging should be treated as a core skill. A developer typically spends more time debugging code than writing code so it makes sense to try and optimize this process. Over the years I have developed a strategy for debugging. I see debugging as a generic, transferable skill that is applicable not only to coding but to any form of systems design. The mental models To debug a system we need a mental model, an understanding of the system in our mind. I believe this is the real cornerstone of debugging, and the common mistake is to spend too little time in constructing these models. The mental model should cover all aspects of the system that you need to understand. For example, if you want to understand why a program is slow, your mental model of the system should allow you to reason about the performance trade-offs. I distinguish three types of mental models, each of them corresponds to a different view of the system. The understanding of what the system does (or should do), how it should behave, is the behavioural model. A bug is observed through this model: the system behaves in a way that does not conform to the behavioural model. Usually (or at least if you're lucky) the behavioural system model is codified in a specification. The operational system model is your understanding of how the system works. This model allows us to formulate hypotheses about why the system does not behave as expected. This is the most important mental system model, and part of the debugging process is actually improving and refining this model. In many cases, the operational model is actually your model of how a program in a given language is compiled/interpreted and executed on the hardware. This model starts from the syntax and semantics of the programming language, and includes a model for any API used in the code. As a trivial example, in Python, the keys in a dictionary are unordered, whereas the default in a C++ map is ordered. The closer you are to the bare metal, or the more you care about performance or memory footprint, the more details your mental model will have to include about the actual hardware, to the extent that for e.g. running code on FPGAs you even need to have a detailed mental model for the memory controllers. For debugging in higher-level languages, usually the model can be much more abstract, with a basic notion of memory management and code execution. The structural system model is the model of where we should look to trace and fix a bug. For software, this model is our understanding of the code structure. In general, the structure of software systems tends to be hierarchical and relatively loosely coupled. This means we only need to focus on a fraction of the codebase at a time. If this were not the case, debugging time would grow more than linearly with the code size. Fortunately for most systems it's closer to logarithmic. The debugging activity Given the above mental system models, the activity of debugging is an iterative process involving several steps, and during the process we often jump between these steps. First, identify the bug. Is it really a bug or is your behavioural model incorrect, not specific enough or ambiguous? If necessary, adapt the model and re-iterate. Then there are essentially three stages in the process of finding the bug. We start by narrowing down through a process of exclusion: \"This bug can't be caused by X because of reason Y\". This process relies mostly on the operational system model, but sometimes also on the structural model, especially if you're not 100% certain: \"it is unlikely that the bug is in module X because of reason Y\". For example, it is unlikely that the cause of the bug is located in a standard library, compiler or interpreter. The chances that the bug is in your own code is much higher, so that possibility should be explored first. Once we cannot proceed any further through exclusion, we switch to the most interesting stage. We formulate a hypothesis \"Let's assume that the bug is caused by X\" and then we use this as the basis for further investigation. The main difference between exclusion and formulating a hypothesis is that when we formulate a hypothesis, we don't know if it is true or false, so we need to test it. With the exclusion process, we do know that our stated reason holds -- or at least we have a high degree of confidence -- so we don't test it. Quite frequently our hypothesis will prove to be false, and then we have one fewer possible cause for the bug. Equally frequently, when our hypothesis proves to be false, this indicates that our operational model is incomplete. In that case we should formulate additional hypotheses to improve our mental model. I believe this is an important step that is often skipped because it seems to detract from the real task, i.e. finding the bug. But without an accurate operational model, it is much harder to find bugs, so the time spent in improving your system knowledge is always well spent. To test a hypothesis we can either use emulation or observation of the system behaviour. This requires the structural model to tell us where to look. By emulation I mean that we mentally run part of a program using our operational model. In that case we assume that our operational model is accurate enough to produce the same result as the actual system. In general, this is a tricky approach to debugging because if our mental model is inaccurate we won't find the bug. However, it is generally the approach taken when we have narrowed down the location of the bug sufficiently. We can observe the system behaviour through compiler or interpreter warnings, by using a debugger, or by making the code generate additional information. This requires a good structural model to guide us to the locations that we want to inspect using the debugger or where we want to add the code to generate the debugging information. Either way, the result should be some information that helps to test the hypothesis. By emulation I mean that we mentally run part of a program using our operational model. In that case we assume that our operational model is accurate enough to produce the same result as the actual system. In general, this is a tricky approach to debugging because if our mental model is inaccurate we won't find the bug. However, it is generally the approach taken when we have narrowed down the location of the bug sufficiently. We can observe the system behaviour through compiler or interpreter warnings, by using a debugger, or by making the code generate additional information. This requires a good structural model to guide us to the locations that we want to inspect using the debugger or where we want to add the code to generate the debugging information. Either way, the result should be some information that helps to test the hypothesis. Example scenarios Debugging code you wrote and understand, and whose use case is intimately familiar to you, should be the easiest type of debugging. However, the problem with this kind of code is often that, precisely because it is your own code and you have a very precise behavioural model, and of course a perfect structural model, you never bothered to create an accurate operational model of the code. This may sound strange because after all, if you wrote it, you should know how it works. But the reality is that we often perform very limited mental verification, esp. of corner cases, on our own code. Debugging someone else's code is much harder because you typically lack all of the mental models. I often have to debug code written by my students, usually long after they have graduated. The main conclusion is that we should teach our students how to write maintainable code, i.e. code that makes it easy to understand the structural model. If the code is a Minimal, Complete, and Verifiable example MCVE then building your mental models is relatively easy because the code base should be small and self-contained. For such examples, the operational model is usually defined at the level of language semantics and standard library APIs. There is a nice detailed post about debugging small programs on Eric Lippert's blog. The main challenge with debugging a truly huge codebase (millions of lines of code) is that you need to build mental models that cover the overall system, even if you are looking to debug a very specific aspect of the system behaviour. For example, some years ago I modified the Weather Research and Forecasting model to run on GPUs, and debugged the changes. This is a numerical weather simulator with a codebase of about two million lines of Fortran 90. It is very well architected and there is reasonably good documentation. The main challenge in this system was actually to understand the build system first, because a large amount of code is generated at build time. Apart from that, I had to learn how a weather simulator works at the level of the physics, and how the code was parallelised. I modified the part of the code known as the advection kernel, to make it work on GPUs. As expected, the changes were not first-time-right, and debugging GPU code is difficult because it is hard to observe what happens inside the GPU. Nevertheless, I followed essentially the approach outlined above. In this case, the original, unmodified code provided the reference behavioural model. I built the structural model through the process of working out which part of the code needed to be modified. So the difficulty was as usual with the operational model, and in this case the bugs mostly originated from the fact that the GPU code is essentially C, and the host code Fortran, and they have different views on arrays and argument passing. Conclusion Debugging is difficult and time consuming but a strategy based on behavioural, operational and structural mental models can make the process more efficient in a variety of scenarios. I would like to thank Ahmed Fasih for motivating me to write this article and suggesting the example scenarios. Updated March 05, 2018 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"debugging/index.html"}, {"title":"The strange case of the greedy junction","text":" The strange case of the greedy junction Raku has a neat feature called Junctions. In this short article I want to highlight a peculiar consequence of the interaction of junctions with functions: they are greedy, by which I mean that they inadvertently turn other arguments of functions into junctions. If you don't know Raku or are unfamiliar with the functional style of programming, I suggest you read my introductory article \"Cleaner code with functional programming\". To illustrate the greedy behaviour, let's create a pair data structure that can take two values of different types, using a closure. So pair takes two arguments of arbitrary type and returns a closure which takes a function as argument. We will use this function to access the values stored in the pair. I will call these accessor functions fst and snd: The function that does the actual selection is the anonymous subroutine returned by fst and snd, this is purely so that I can apply them to the pair rather than have to pass them as an argument. Let's look at an example, a pair of an Int and an enum RGB: So we create a pair by calling pair with two values, and use fst and snd to access the values in the pair. This is an immutable data structure so updates are not possible. Now let's use a junction for one of the arguments. What has happened here is that the original argument R has been irrevocably turned into a junction with itself. This happens despite the fact that we never explicitly created a junction on R. This is a consequence of the application of a junction type to a function, and it is not a bug, simply an effect of junction behaviour. For a more detailed explanation, see my article \"Reconstructing Raku's Junctions\". The Raku documentation of junctions says that you should not really try to get values out of a junction: \"Junctions are meant to be used as matchers in Boolean context; introspection of junctions is not supported. If you feel the urge to introspect a junction, use a Set or a related type instead.\" Luckily, there is a FAQ that grudgingly shows you how to do it. The FAQ once again warns against doing this: \"If you want to extract the values (eigenstates) from a Junction, you are probably doing something wrong and should be using a Set instead.\" However, as demonstrated by the example I have given, there is a clear use case for recovering values from junctions. It is of course not the intention that one of the values stored in the pair becomes inaccessible simply because the other value happens to be a junction. I therefore propose the addition of a collapse function which will allow to collapse these inadvertent junction values into their original values. The implementation of this function is taken from the above-mentioned FAQ, with the addition of a check to ensure that all values on the junction are identical. In the first draft of this article, which I shared as a gist, I wrote that it would be nice if this collapse would be added as an additional method to the Junction class. And thanks to Elizabeth Mattijsen, there is already a pull request implementing this feature in Rakudo! I will update the post when it has made it into a release. Updated October 04, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/greedy-junctions/index.html"}, {"title":"Musings of an Accidental Computing Scientist","text":" Musings of an Accidental Computing Scientist September 20, 2021 Haku: a Japanese programming language Haku is a natural language functional programming language based on literary Japanese and written in Raku June 29, 2021 Frugal computing On the need for low-carbon and sustainable computing and the path towards zero-carbon computing. December 02, 2020 Writing faster Raku code Sometimes you want your Raku code to be faster. What does it take? October 05, 2020 Reconstructing Raku's Junctions A reconstruction of Raku's junctions as an algebraic data with higher-order functions. October 04, 2020 The strange case of the greedy junction An illustration that Raku's junctions are greedy by design, and a proposal. September 13, 2020 A universal interpreter The Böhm-Berarducci encoding of a type can be considered as a universal interpreter. We illustrate this in Raku with an evaluator and pretty printer. September 12, 2020 Encoding types as functions in Raku The Böhm-Berarducci encoding is a way to express an algebraic data type as a function type. We show how to do this in Raku using roles. August 07, 2020 Function types A brief introduction into function types, with a way to implement them in Raku and examples in many languages. July 18, 2020 Cleaner code with functional programming An introduction to some powerful functional programming techniques in Raku and Python. July 03, 2020 Everything is a function Although it might seem that a language like Haskell has a lot of different objects and constructs, they can all be reduced to functions. June 22, 2020 List-based parser combinators in Haskell and Raku This is a follow-on of my article on algebraic data types, with list-based parser combinators as a practical application. June 05, 2020 Roles as Algebraic Data Types in Raku Algebraic data types are great for building complex data structures, and easy to implement in Raku using roles. March 28, 2020 The Winds of Kyoto A very short story for the Fifth Annual Kyoto Writing Competition. April 27, 2019 Writing faster Perl code Sometimes your pure Perl code needs to be as fast as possible. What does it take? October 21, 2018 Why Europe needs hurricane contingency planning Hurricanes will become common in Europe in the next few decades. I discuss the evidence, the impact and the need for proper contingency planning. April 19, 2018 Hacking the Pleroma: Elixir, Phoenix and a bit of ActivityPub A brief guide into hacking Pleroma, a federated microblogging server software. March 13, 2018 A little maths puzzle in two parts Of which the outcome is a way to construct a regular pentagon using compass and ruler. March 05, 2018 A strategy for articles/debugging For a long time it has been my contention that for a developer, more that programming, debugging should be treated as a core skill. March 04, 2018 Imagine we had better weather forecasts Current weather forecasts are at the same time very advanced and yet not good enough. Earlier and more accurate warnings could help to limit the damage of su... Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"index.html"}, {"title":"Why Europe needs hurricane contingency planning","text":" Why Europe needs hurricane contingency planning As a result of global warming, hurricanes will become common in Europe in the next few decades. I discuss the evidence, the impact and the need for proper contingency planning. Some terminology A tropical cyclone is a rapidly rotating storm system with a low-pressure centre, strong winds, and a spiral arrangement of thunderstorms that produce heavy rain. They are called \"tropical\" because they form almost exclusively over tropical or sub-tropical waters. \"Cyclone\" refers to their winds moving in a circle, rotating around a central clear eye. The winds blow counterclockwise in the Northern Hemisphere and clockwise in the Southern Hemisphere. For completeness, an anticyclone is a large-scale circulation of winds around a central region of high atmospheric pressure, clockwise in the Northern Hemisphere, counterclockwise in the Southern Hemisphere\". Anticyclones are not storm systems, and in Belgium, where I used to live, the famous[^1] Azores Anticyclone usually lead to nice weather. The terms \"hurricane\" and \"typhoon\" refer both to powerful tropical cyclones. A hurricane[^2] occurs in the Atlantic Ocean and northeastern Pacific Ocean, and a typhoon[^3] occurs in the northwestern Pacific Ocean[^4]. The diameter of the hurricane is typically of the order of hundreds of kilometers. Last month's hurricane Florence was over 600 km in diameter, and typhoon Trami about 500 km. A \"tropical storm\" is \"tropical cyclone\" that is less strong than a hurricane or typhoon. Strength and potential damage The Saffir-Simpson Hurricane Wind Scale is a scale from 1 to 5 based on a hurricane's sustained wind speed, used to estimate potential property damage. Hurricanes reaching Category 3 and higher are considered major hurricanes because of their potential for significant loss of life and damage. However, Category 1 and 2 hurricanes are still much more dangerous than ordinary tropical storms, and require preventive measures. The amount of energy in a hurricane is very large. NOAA (US National Oceanic and Atmospheric Administration) notes that the energy released by an average hurricane \"is equivalent to 200 times the world-wide electrical generating capacity\". The table below is adapted from NOAA page linked above. Note that this table uses sustained wind speeds. The gusts occuring during a tropical cyclone can be a lot stronger than this (typically about 30% stronger), and are usually what is quoted in the media. Global warming causes hurricanes to get stronger Slightly simplifying, hurricanes get their energy from the heat in the surface sea water. As the planet gets warmer, the sea surface gets warmer, which leads to stronger tropical cyclones. There is plenty evidence for this trend. For example, very recent work by my friend and colleague Prof. Takemi of the Disaster Prevention Research Institute (DPRI) of Kyoto University [1] used observation data of historical typhoons to reproduce them in simulation, and then simulated the effects of future warmer conditions. They conclude that both wind speed and precipitation would increase significantly. When I visited DPRI in September, a researcher explained me their latest simulations. With 2 degrees warming, wind speeds and rainfall during a typhoon could double. The recent Guardian article \"Is climate change making hurricanes worse?\" provides a good overview. Note that this trend refers to all tropical cyclones, not just these that made landfall. Hurricanes are coming to Europe Historically, none of the tropical storms in Europe in the last century except hurricane Vince in 2005 have been hurricanes, no matter how severe they might have seemed. However, this is about to change. As hurricanes get more powerful and last longer, the chance that they can reach Europe grows. Again, there is evidence for this. Already in 2013 Reindert Haarsma, Senior Scientist at the Royal Netherlands Meteorological Institute wrote the article \"The future will bring hurricanes to Europe\" which gives a good overview. The article is based on Haarsma's scientific research [2]. Other researchers predict similar trends [3,4,5]. Impact and contingency planning Hurricanes do not only cause damage because of the strong winds. They also lead to flash floods because of the very heavy rainfall that they cause (often 30-50 cm/h), and because of the storm surges which are several meters in height (e.g. 5 m for hurricane Michael a few weeks ago). To put this into perspective, in December 2015 storm Desmond broke the United Kingdom's 24-hour rainfall record with 34 cm of rainfall in 24 hours, and led to widespread flooding in the UK and Ireland. The only storm surge on record comparable to those caused by hurricanes was the notorious Great Storm of 1953. In Scotland, where storms are common, this was the worst storm[^5] in 500 years. Better flood defenses are therefore absolutely crucial to deal with future hurricanes. As a result of the 1953 disaster, much was done in the UK, the Netherlands and Belgium to strenghten flood defenses, but these focus on the North Sea. Similar works on the coasts facing the Atlantic will be necessary. Furthermore because of the increased damage, power outages and disruption of supplies will last much longer than for the storms we have now and therefore contingency plans will have to be put in place. A capability for accurate forecasting of hurricane trajectories is necessary for timely evacuation of people in the affected areas. The good news is that Europe can benefit from the extensive know-how developed for example in the US and Japan, both in predictions and in dealing with the effects of such severe weather events. [1] Kanada S, Takemi T, Kato M, Yamasaki S, Fudeyasu H, Tsuboki K, Arakawa O, Takayabu I. A multimodel intercomparison of an intense typhoon in future, warmer climates by four 5-km-mesh models. Journal of Climate. 2017 Aug;30(15):6017-36. [2] Haarsma RJ, Hazeleger W, Severijns C, De Vries H, Sterl A, Bintanja R, Van Oldenborgh GJ, van den Brink HW. More hurricanes to hit western Europe due to global warming. Geophysical Research Letters. 2013 May 16;40(9):1783-8. [3] Baker A, Hodges K, Schiemann R, Vidale PL. North Atlantic post-tropical cyclones in reanalysis datasets. In EGU General Assembly Conference Abstracts, 2018 Apr (Vol. 20, p. 14606). [4] Dekker MM, Haarsma RJ, de Vries H, Baatsen M, van Delden AJ. Characteristics and development of European cyclones with tropical origin in reanalysis data. Climate Dynamics. 2018 Jan 1;50(1-2):445-55. [5] Mousavi ME, Irish JL, Frey AE, Olivera F, Edge BL. Global warming and hurricanes: the potential impact of hurricane intensification and sea level rise on coastal flooding. Climatic Change. 2011 Feb 1;104(3-4):575-97. The banner image shows typhoon Halong approaching Japan in September 2014, © NASA Terra/MODIS 2014 [^1]: It is so famous that a travel book shop in Brussels took it as its name. [^2]: The term \"hurricane\" derives from the Spanish word huracán, which in turn probably derives from the Taino (an indigenous people of the Caribbean) word hurakán \"god of the storm\". [^3]: In Japan they are called 台風 (taifuu) and are given numbers rather than names. [^4]: In the northwestern Pacific, the term \"super typhoon\" is used for tropical cyclones with sustained winds exceeding 240 km/h. [^5]: This was a European windstorm, a type of extratropical cyclone, caused by different weather phenomena than hurricanes. There is evidence that this type of storms is also getting stronger [5]. Updated October 21, 2018 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/prepare-for-hurricanes/index.html"}, {"title":"Frugal computing","text":" Frugal computing On the need for low-carbon and sustainable computing and the path towards zero-carbon computing. Lisez en Français Lea en Español Lisez en Français Lea en Español Key points The current emissions from computing are about 2% of the world total but are projected to rise steeply over the next two decades. By 2040 emissions from computing alone will be close to half the emissions level acceptable to keep global warming below 1.5°C. This growth in computing emissions is unsustainable: it would make it virtually impossible to meet the emissions warming limit. The emissions from production of computing devices far exceed the emissions from operating them, so even if devices are more energy efficient producing more of them will make the emissions problem worse. Therefore we must extend the useful life of our computing devices. The current emissions from computing are about 2% of the world total but are projected to rise steeply over the next two decades. By 2040 emissions from computing alone will be close to half the emissions level acceptable to keep global warming below 1.5°C. This growth in computing emissions is unsustainable: it would make it virtually impossible to meet the emissions warming limit. The emissions from production of computing devices far exceed the emissions from operating them, so even if devices are more energy efficient producing more of them will make the emissions problem worse. Therefore we must extend the useful life of our computing devices. As a society we need to start treating computational resources as finite and precious, to be utilised only when necessary, and as effectively as possible. We need frugal computing: achieving the same results for less energy. As a society we need to start treating computational resources as finite and precious, to be utilised only when necessary, and as effectively as possible. We need frugal computing: achieving the same results for less energy. Imagine we can extend the useful life of our devices and even increase their capabilities without any increase in energy consumption. Meanwhile, we will develop the technologies for the next generation of devices, designed for energy efficiency as well as long life. Every subsequent cycle will last longer, until finally the world will have computing resources that last forever and hardly use any energy. Imagine we can extend the useful life of our devices and even increase their capabilities without any increase in energy consumption. Meanwhile, we will develop the technologies for the next generation of devices, designed for energy efficiency as well as long life. Every subsequent cycle will last longer, until finally the world will have computing resources that last forever and hardly use any energy. Defining computational resources Computational resources are all resources of energy and material that are involved in any given task that requires computing. For example, when you perform a web search on your phone or participate in a video conference on your laptop, the computational resources involved are those for production and running of your phone or laptop, the mobile network or WiFi you are connected to, the fixed network it connects to, the data centres that perform the search or video delivery operations. If you are a scientist running a simulator in a supercomputer, then the computational resources involved are your desktop computer, the network and the supercomputer. For an industrial process control system, it is the production and operation of the Programmable Logic Controllers. Computational resources are finite Since the start of general purpose computing in the 1970s, our society has been using increasing amounts of computational resources. For a long time the growth in computational capability as a function of device power consumption has literally been exponential, a trend expressed by Moore's law. With this growth in computational capability, increasing use of computational resources has become pervasive in today's society. Until recently, the total energy budget and carbon footprint resulting from the use of computational resources has been small compared to the world total. As a result, computational resources have until recently effectively been treated as unlimited. Because of this, the economics of hardware and software development have been built on the assumption that with every generation, performance would double for free. Now, this unlimited growth is no longer sustainable because of a combination of technological limitations and the climate emergency. Therefore, we need to do more with less. Moore's law has effectively come to an end as integrated circuits can't be scaled down any more. As a result, the performance per Watt is no longer increasing exponentially. On the other hand, the demand for computational resources is set to increase considerably. The consequence is that at least for the next decades, growth in demand for computational resources will not be offset by increased power efficiency. Therefore with business as usual, the total energy budget and carbon footprint resulting from the use of computational resources will grow dramatically to become a major contributor to the world total. Furthermore, the resources required to create the compute devices and infrastructure are also finite, and the total energy budget and carbon footprint of production of compute devices is huge. Moore's Law has conditioned us to doubling of performance ever two years, which has led to very short effective lifetimes of compute hardware. This rate of obsolescence of compute devices and software is entirely unsustainable. Therefore, as a society we need to start treating computational resources as finite and precious, to be utilised only when necessary, and as frugally as possible. And as computing scientists, we need to ensure that computing has the lowest possible energy consumption. And we should achieve this with the currently available technologies because the lifetimes of compute devices needs to be extended dramatically. I would like to call this \"frugal computing\": achieving the same results for less energy by being more frugal with our computing resources. The scale of the problem To limit global warming to 1.5°C, within the next decade a global reduction from 55 gigatonnes CO₂ equivalent (GtCO₂e) by 32 GtCO₂e to 23 GtCO₂e per year is needed [5]. So by 2030 that would mean a necessary reduction in overall CO₂ emissions of more than 50%. According to the International Energy Agency [10], emissions from electricity are currently estimated at about 10 GtCO₂e. The global proportion of electricity from renewables is projected to rise from the current figure of 22% to slightly more than 30% by 2040 [15]. In other words, we cannot count on renewables to eliminate CO₂ emissions from electricity in time to meet the climate targets. Reducing the energy consumption is the only option. The consequence of the end of Moore's law was expressed most dramatically in a 2015 report by the Semiconductor Industry Association (SIA) \"Rebooting the IT Revolution: a call to action\" [1], which calculated that, based on projected growth rates and on the 2015 ITRS roadmap for CMOS chip engineering technologies [16], computing will not be sustainable by 2040, when the energy required for computing will exceed the estimated world's energy production. It must be noted that this is purely the energy of the computing device, as explained in the report. The energy required by e.g. the data centre infrastructure and the network is not included. The SIA has reiterated this in their 2020 \"Decadal Plan for Semiconductors\" [2], although they have revised the projection based on a \"market dynamics argument\": If the exponential growth in compute energy is left unchecked, market dynamics will limit the growth of the computational capacity which would cause a flattening out the energy curve. This is merely an acknowledgement of the reality that the world's energy production is not set to rise dramatically, and therefore increased demand will result in higher prices which will damp the demand. So computation is not actually going to exceed the world's energy production. Ever-rising energy demand for computing vs. global energy production is creating new risk, and new computing paradigms offer opportunities to dramatically improve energy efficiency. In the countries where most of the computational resources are consumed (US and EU), electricity production accounts currently for 25% of the total emissions [4]. According to the SIA's estimates, computation accounts currently for a little less than 10% of the total electricity production but is set to rise to about 30% by 2040. This would mean that, with business as usual, computational resources would be responsible for at least 10% of all global CO₂ emissions by 2040. The independent study \"Assessing ICT global emissions footprint: Trends to 2040 & recommendations\" [3] corroborates the SIA figures: they estimate the computing greenhouse gas emissions for 2020 between 3.0% and 3.5% of the total, which is a bit higher than the SIA estimate of 2.5% because it does take into account networks and datacentres. Their projection for 2040 is 14% rather than 10%, which means a growth of 4x rather than 3x. To put it in absolute values, based on the above estimate, by 2040 energy consumption of compute devices would be responsible for 5 GtCO₂e, whereas the target for world total emissions from all sources is 23 GtCO₂e. To make matters worse, the carbon emissions resulting from the production of computing devices exceeds those incurred during operation. This is a crucial point, because it means that we can't rely on next-generation hardware technologies to save energy: the production of this next generation of devices will create more emissions than any operational gains can offset. It does not mean research into more efficient technologies should stop. But their deployment cycles should be much slower. Extending the useful life of compute technologies must become our priority. The report about the cost of planned obsolescence by the European Environmental Bureau [7] makes the scale of the problem very clear. For laptops and similar computers, manufacturing, distribution and disposal account for 52% of their Global Warming Potential (i.e. the amount of CO₂-equivalent emissions caused). For mobile phones, this is 72%. The report calculates that the lifetime of these devices should be at least 25 years to limit their Global Warming Potential. Currently, for laptops it is about 5 years and for mobile phones 3 years. According to [8], the typical lifetime for servers in data centres is also 3-5 years, which again falls short of these minimal requirements. According to this paper, the impact of manufacturing of the servers is 20% of the total, which would require an extension of the useful life to 11-18 years. Taking into account the carbon cost of both operation and production, computing would be responsible for 10 GtCO₂e by 2040, almost half of the acceptable CO₂ emissions budget [2,3,14]. To decide on the required actions to reduce emissions, it is important to look at the numbers of different types of devices and their energy usage. If we consider mobile phones as one category, laptops and desktops as another and servers as a third category, the questions are: how many devices are there in each category, and what is their energy consumption. The absolute numbers of devices in use are quite difficult to estimate, but the yearly sales figures [10] and estimates for the energy consumption for each category [11,12,13,14] are readily available from various sources. The tables below show the 2020 sales and yearly energy consumption estimates for each category of devices. A detailed analysis is presented in [14]. The energy consumption of all communication and computation technology currently in use in the world is currently around 3,000 TWh, about 11% of the world's electricity consumption, projected to rise by 3-4 times by 2040 with business as usual according to [2]. This is a conservative estimate: the study in [14] includes a worst-case projection of a rise to 30,000 TWh (exceeding the current world electricity consumption) by 2030. The above data make it clear which actions are necessary: the main carbon cost of phones, tablets and IoT devices is their production and the use of the mobile network, so we must extend their useful life very considerably and reduce network utilisation. Extending the life time is also the key action for datacentres and desktop computers, but their energy consumption also needs to be reduced considerably, as does the energy consumption of the wired, WiFi and mobile networks. A vision for low carbon and sustainable computing It is clear that urgent action is needed: in less than two decades, the global use of computational resources needs to be transformed radically. Otherwise, the world will fail to meet its climate targets, even with significant reductions in other emission areas. The carbon cost of both production and operation of the devices must be considerably reduced. To use devices for longer, a change in business models as well as consumer attitudes is needed. This requires raising awareness and education but also providing incentives for behavioural change. And to support devices for a long time, an infrastructure for repair and maintenance is needed, with long-term availability of parts, open repair manuals and training. To make all this happen, economic incentives and policies will be needed (e.g. taxation, regulation). Therefore we need to convince key decision makers in society, politics and business. Imagine that we can extend the useful life of our devices and even increase their capabilities, purely through better computing science. With every improvement, the computational capacity will in effect increase without any increase in energy consumption. Meanwhile, we will develop the technologies for the next generation of devices, designed for energy efficiency as well as long life. Every subsequent cycle will last longer, until finally the world will have computing resources that last forever and hardly use any energy. This is a very challenging vision, spanning all aspects of computing science. To name just a few challenges: We must design software so that it supports devices with extended lifetimes. We need software engineering strategies to handle the extended software life cycles, and in particular deal with technical debt. Longer life means more opportunities to exploit vulnerabilities, so we need better cyber security. We need to develop new approaches to reduce overall energy consumption across the entire system. We must design software so that it supports devices with extended lifetimes. We need software engineering strategies to handle the extended software life cycles, and in particular deal with technical debt. Longer life means more opportunities to exploit vulnerabilities, so we need better cyber security. We need to develop new approaches to reduce overall energy consumption across the entire system. To address these challenges, action is needed on many fronts. What will you do to make frugal computing a reality? References [1] \"Rebooting the IT revolution: a call to action\", Semiconductor Industry Association/Semiconductor Research Corporation, Sept 2015 [2] \"Full Report for the Decadal Plan for Semiconductors\", Semiconductor Industry Association/Semiconductor Research Corporation, Jan 2021 [3] \"Assessing ICT global emissions footprint: Trends to 2040 & recommendations\", Lotﬁ Belkhir, Ahmed Elmeligi, Journal of Cleaner Production 177 (2018) 448--463 [4] \"Sources of Greenhouse Gas Emissions\", United States Environmental Protection Agency, Last updated on April 14, 2021 [5] \"Emissions Gap Report 2020\", UN Environment Programme, December 2020 [6] \"The link between product service lifetime and GHG emissions: A comparative study for different consumer products\", Simon Glöser-Chahoud, Matthias Pfaff, Frank Schultmann, Journal of Industrial Ecology, 25 (2), pp 465-478, March 2021 [7] \"Cool products don’t cost the Earth – Report\", European Environmental Bureau, September 2019 [8] \"The life cycle assessment of a UK data centre\", Beth Whitehead, Deborah Andrews, Amip Shah, Graeme Maidment, Building and Environment 93 (2015) 395--405, January 2015 [9] Statista, retrieved June 2021 [10] \"Global Energy & CO₂ Status Report\", International Energy Agency, March 2019 [11] \"Redefining scope: the true environmental impact of smartphones?\", James Suckling, Jacquetta Lee, The International Journal of Life Cycle Assessment volume 20, pages 1181–1196 (2015) [12] \"Server Rack Power Consumption Calculator\", Rack Solutions, Inc., July 2019 [13] \"Analysis of energy consumption and potential energy savings of an institutional building in Malaysia\", Siti Birkha Mohd Ali, M.Hasanuzzaman, N.A.Rahim, M.A.A.Mamun, U.H.Obaidellah, Alexandria Engineering Journal, Volume 60, Issue 1, February 2021, Pages 805-820 [14] \"On Global Electricity Usage of Communication Technology: Trends to 2030\", Anders S. G. Andrae, Tomas Edler, Challenges 2015, 6(1), 117-157 [15] \"BP Energy Outlook: 2020 Edition\",BP plc [16] \"2015 International Technology Roadmap for Semiconductors (ITRS)\", Semiconductor Industry Association, June 2015 Updated June 29, 2021 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/frugal-computing/index.html"}, {"title":"Haku: a Japanese programming language","text":" Haku: a Japanese programming language Haku is a natural language functional programming language based on literary Japanese. This article is about the implementation of Haku in Raku. You don't need to know Japanese or have read the Haku documentation. I you are not familiar with Raku, you might want to read my quick introduction. I do assume familiarity with the concepts of parsing, syntax tree and code generation. I you find you lack background for what follows, I recommend Andrew Shitov's series of posts Creating a Compiler with Raku which takes a step-by-step approach. Haku Haku aims to be close to written Japanese, so it is written in a combination of the three Japanese writing systems kanji (Chinese characters), hiragana and katakana, and Japanese punctuation. There are no spaces, and Haku does not use Arabic (or even Roman) digits nor any operators. The design of the language is explained in more detail in the documentation. Here is an example of a small Haku program (for more examples see the repo): This translates as \"main is: to show 'From Haku to Raku'\" And the Raku version would be The strings \"本とは\" and \"の事です。\" indicate the start and end of the main program. \"「魄から楽まで」\" is a string constant. \"見せる\" is the print function. The 'を' indicates that anything before it is an argument of the function. The newlines in the example code are optional and purely there for readability. A Haku program is a single string without whitespace or newlines. The actual generated Raku code for this example is To be even closer to literary Japanese, Haku programs can be written vertically from right to left: The generated Raku code for this Haku program is again quite simple: Haku is implemented in Raku. The Haku compiler is a source-to-source compiler (sometimes called transpiler) which generates Raku source from the Haku source and executes it. Raku makes writing such a compiler easy in many ways: Parsing using Grammars I decided to implement Haku in Raku mostly because I wanted to use Raku's Grammars feature, and it did not disappoint. A grammar is like a class, but instead of methods it has rules or tokens, which are the building blocks of the parser. Any token can be used in the definition of another token by enclosing it in <...>, for example: The tokens i-adjective and na-adjective have been defined separately and adjective matches one or the other. I have always liked parser combinators (like Parsec in Haskell) and from a certain angle, Raku's Grammar's are quite similar. They are both scannerless, i.e. there is no separate tokenisation step, and highly composable. Many of the features offered by Parsec (e.g. many, oneOf, sepBy) are available courtesy of Raku's regexes. There are several features of Raku's Grammars that helped to make the parser for Haku easy to implement. I think Raku's Unicode support is really excellent. For example, thanks to the support for Unicode blocks, I can simply write rather than having to enumerate them all (there are 92,865 kanji in that block!). In fact, the <:...> syntax works for any Unicode property, not just for Blocks. Even better: I have some kanji that are reserved as keywords: To make sure these are excluded from the valid kanji for Haku, I can simply use a set difference: (One detail that bit me is that the equivalent syntax for a user-defined character class requires an explicit '+' : token set-difference { < +set1 -set2> } ) Luckily, Raku does not assume by default that you want to parse something where whitespace can be ignored, or that you want to tokenise on whitespace. If you want to ignore whitespace, you can use a rule. But in Haku, extraneous whitespace is not allowed (except for newlines at certain locations). So I use token everywhere. (There is also regex, which backtracks. In Haku's grammar I have not needed it.) As a lambdacamel, I've always been fond of Perl's regexes, the now ubiquitous PCREs. Yet, Raku's regexes go way beyond that in power, expressiveness and readability. For one thing, they are composable: you can defined a named regex with the regex type and use it in subsequent regexes with the <...> syntax. Also, the care with which they have been designed makes them very easy to use. For example, a negative look-ahead assertion is simply <no> <!before <koto> >; and the availability of both a try-in-order alternation (||) and longest-token match alternation (|) is a huge boon. Another thing I like very much is the ability to make a character class non-capturing: Only <variable-list> and <expression> will be captured, so a lot of the concrete syntax can be removed at parse time. Roles ('mixins' in Ruby, 'traits' in Rust) define interfaces and/or implementation of those interfaces. I found this a better fit for my purpose than the also-supported class inheritance. For example: (Although I would like a list syntax for this, something like role Identifiers does Verbs, Nouns, Adjectives, Variables {...}.) There is a lot more to grammars and regexes. The nice Raku folks on Twitter recommended me the book \"Parsing with Perl 6 Regexes and Grammars\" by Moritz Lenz and it was very useful in particular for articles/debugging of the grammar and handling of error messages. Abstract syntax tree using roles I like to implement the abstract syntax tree (AST) as an algebraic data type, the way it is usually done in Haskell. In Raku, one way to do this is to use parametrised Roles as I explained in an earlier post. Most of the AST maps directly to the toplevel parser for each role in my grammar, for example the lambda expression: From parse tree to abstract syntax tree Raku's grammars provide a very convenient mechanism for turning the parse tree into an AST, called Actions. Essentially, you create a class with a method with the same name as the token or rule in the Grammar. Each method gets the Match object ($/) created by the token as a positional argument. For example, to populate the AST node for a lambda expression from the parse tree: The capturing tokens used in the lambda-expression token are accessible via the notation $<...> which is shorthand for $/<...>, i.e. they are named attributes of the current match object. In the Haku grammar, there are several tokens where the match is one from a list of alternatives, for example the expression token, which enumerates anything that is an expression in Haku. For such tokens I use the following code to \"inherit\" from the constituent tokens: Because every match is a map with as keys the names of the capturing tokens, and because we know that in this case there will be only one token selected, we know the first element in the corresponding values list will be the match for that particular token. Code generation The haku.raku main program essentially does this: The Haku program string is parsed using the Haku grammar and the methods defined in the corresponding HakuActions class are used to populate the AST. The toplevel parse tree node must be $<haku-program>, and the made method of this node returns the AST node HakuProgram. The routine ppHakuProgram is the toplevel routine in the module Raku, which is the Raku emitter for Haku. (There is also a Scheme emitter, in the module Scheme.) So ppHakuProgram($hon_parse.made) pretty-prints the HakuProgram AST node and thus the entire Haku program as Raku code. What I like about the role-based AST is that you can pattern match against the variants of a type using given/when: The Raku code corresponding to the Haku AST is quite straightforward, but there are a few things worth noting: Because Haku's variables are immutable, I use the \\ notation which means I don't have to build a variable table with the sigils. Because Haku is functional, let and if are expressions, so in Raku I wrap them in a do {} block. For partial application I use .assuming(). In Haku, strings are lists. In Raku they aren't. I created a small Prelude of functions, and the list manipulation functions in that Prelude use pattern matching on the type with given/when to see if the argument is a string or a list. Because Haku's variables are immutable, I use the \\ notation which means I don't have to build a variable table with the sigils. Because Haku is functional, let and if are expressions, so in Raku I wrap them in a do {} block. For partial application I use .assuming(). In Haku, strings are lists. In Raku they aren't. I created a small Prelude of functions, and the list manipulation functions in that Prelude use pattern matching on the type with given/when to see if the argument is a string or a list. Running the generated Raku code Running the generated Raku code is simple: I write the generated Raku code to a module and require it. The generated code ends with a call to hon(), the main function in a Haku program, so this automatically executes the program. Other things Haku makes really easy is to create command-line flags and document their usage: USAGE is called when MAIN is called with the wrong (or no) arguments. Arguments of MAIN prefixed with : are flags. unit sub means that anything after this declaration is part of the MAIN program, so no need for {...}. To conclude This article shows the lazy programmer's way to creating your own programming language: let Raku do all the hard work. Or to express it with a Haku program: the truth: write the compiler, write the program, run the program. Updated September 20, 2021 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/haku-in-raku/index.html"}, {"title":"Reconstructing Raku's Junctions","text":" Reconstructing Raku's Junctions Junctions in Raku are cool but at first glance they do not follow the rules for static typing. I was curious about their formal typing semantics, so I deconstructed and then reconstructed junctions from a functional, static typing perspective. If you don't know Raku or are unfamiliar with the functional style of programming, I suggest you read my introductory article \"Cleaner code with functional programming\". If you have not heard of algebraic data types before, I suggest my article \"Roles as Algebraic Data Types in Raku\". Junctions in Raku Raku has this neat feature called Junctions. A junction is an unordered composite value. When a junction is used instead of a value, the operation is carried out for each junction element, and the result is the junction of the return values of all those operators. Junctions collapse into a single value when used in a Boolean context. Junctions can be of type all (&), any (|), one (^) or none (empty junction). For example, The function so forces the Boolean context. Junctions have type Junction, and I was curious about the typing rules, because at first sight there is something strange. Let's say we have a function sq from Int to Int : Now let's define a junction of type any of Int values: When we apply sq to $j, we do not get a type error, even though the functions has type :(Int --> Int) and the junction has type Junction. Instead, we get a junction of the results: If we assign this to a variable of type Int as before, we get a type error: Instead, the return value is now of type Junction: So the Junction type can take the place of any other type but then the operation becomes a junction as well. On the other hand, junctions are implicitly typed by their constituent values, even though they seem to be of the opaque type Junction. For example, if we create a junction of Str values, and try to pass this junction value into sq, we get a type error: Do junctions follow static typing rules? Although this kind of makes sense (we don't want it to work with Str if the original function expects Int), this does flout the rules for static typing, even with subtyping. If an argument is of type Int then any type below it in the type graph can be used instead. But the simplified type graph for Int and Junction is as follows: So a Junction is never a subtype of anything below Any. Therefore putting a junction in a slot of type Any or subtype thereof should be a type error. Furthermore, because the Junction type is opaque (i.e. it is not a parametrised type), it should not hold any information about the type of the values inside the junction. And yet it does type check against these invisible, inaccessible values. So what is happening here? A working hypothesis A working hypothesis is that a Junction type does not really take the place of any other type: it is merely a syntactic sugar that makes it seem so. Reconstructing junctions part 1: types Let's try and reconstruct this. The aim is to come up with a data type and some actions that will replicate the observed behaviour of Raku's junctions. First we discuss the types, using Haskell notation for clarity. Then I present the implementation in Raku. This implementation will behave like Raku's native junctions but without the magic syntactic sugar. In this way I show that Raku's junctions do follow proper typing rules after all. A Junction is a data structure consisting of a junction type JType and a set of values. I restrict this set of values to a single type for convenience and also because a junction of mixed types does actually not make much sense. I use a list to model the set, again for convenience. Because a Junction can contain values of any type, it is a polymorphic algebraic data type: Doing anything with a junction means applying a function to it. We can consider three cases, and I introduce an ad-hoc custom operator for each of them: Apply a non-Junction function to a Junction expression Apply a non-Junction function to a Junction expression Apply a Junction function to a non-Junction expression Apply a Junction function to a non-Junction expression Apply a Junction function to a Junction expression, creating a nested junction Apply a Junction function to a Junction expression, creating a nested junction For convenience, we can also create custom comparison operators between Junction a and a: Then we have so, the Boolean coercion function. What it does is to collapse a junction of Booleans into a single Boolean. Finally we have collapse, which returns the value from a junction, provided that it is a junction where all stored values are the same. This may seem like a strange function but it is necessary because of the behaviour of junctions. As we will see, the above semantics imply that junctions are greedy: if a single argument of a function is a junction, then all other arguments also become junctions, but all values in the junction are identical. I have discussed this in \"The strange case of the greedy junction\", but we can now formalise this behaviour. Suppose we have a function of two arguments f :: a -> b -> c, and we apply a junction j :: Junction a to the first argument, f •￮ j. Then the result is a partially applied function wrapped in a Junction: fp :: Junction b -> c. If we now want to apply this function a non-Junction value v :: b using fp ￮• v, the result is of type Junction c. So already we see that the non-Junction value v is assimilated into the junction. Now, let's consider the particular case where the type c is forall d . (a -> b -> d) -> d, so we have Junction (forall d . (a->b->d) -> d). This is a function which takes a function argument and returns something of the return type of that function. We use the forall so that d can be anything, but in practice we want it to be either a or b. Let's assume we apply this function (call it p) to fst :: a->b->a, using p ￮• fst, then we get Junction a. But if we apply it to snd :: a->b->b, using p ￮• snd, then we get Junction b. Recall that we applied the original function f to a Junction a and a non-Junction b. Yet whatever we do, we can't recover the b. The result is always wrapped in a junction. This is the formal type-based analysis of why we can't return a non-Junction value from a pair as explained in \"The strange case of the greedy junction\". And this is why we need the collapse function. Reconstructing junctions part 2: Raku implementation We start by creating the Junction type, using an enum for the four types of junctions, and a role for the actual Junction data type: Next, the constructors for the four types of junctions (underscore to avoid the name conflict with the builtins): To apply a (single-argument) function to a junction argument To apply a function inside a junction to a non-junction an argument To apply a function to two junction arguments is equivalent to applying a function inside a junction to a junction. There is a complication here: Raku imposes an ordering on the nesting such that all is always the outer nest. Therefore we must check the types of the junctions and swap the maps if required. For completeness, here is the definition of ○==●. Definitions of ○!=●, ○>●. etc are analogous. Next we have so, which turns a junction of Booleans into a Boolean: And finally we have collapse, as defined in the article on greedy junctions. collapse returns the value form a junction provided they are all the same: Junctions desugared Let's now look at our working hypothesis again, the interpretation of actions on Raku's junctions as syntactic sugar for the above type and operators. Desugared this becomes: Similarly, becomes and similar for other Boolean contexts. If we look closer at the pair example from the greedy junctions article, then applying a junction to a function with multiple arguments is desugared as We use .assuming() because we need partial application. It does not matter whether we apply first the non-junction argument or the junction argument: Finally, an example where both arguments are junctions. Because of the definition of ○○, the order of application does not matter. Conclusion Starting from the hypothesis that the magic typing behaviour of Raku's junctions is actually syntactic sugar, I have reconstructed the Junction type and its actions using a polymorphic algebraic data type operating on functions via a set of higher-order functions. I have shown that the interpretation of Raku's behaviour as syntactic sugar holds for the presented implementation. In other words, Raku's Junctions do follow static typing rules. Updated October 05, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/reconstructing-raku-junctions/index.html"}, {"title":"Encoding types as functions in Raku","text":" Encoding types as functions in Raku This is the first part of an article in my series about functional programming in general and algebraic data types and function types in particular in Raku. It builds on my earlier articles on algebraic data types in Raku and their use in the practical example of list-based parser combinators. It also makes heavily use of function types. If you are not familiar with functional programming or with Raku, I suggest you read my introduction \"Cleaner code with functional programming\". If you are not familiar with algebraic data types or function types, you might want to read the other articles as well. In this article, I want to explain a technique called Böhm-Berarducci encoding of algebraic data types. The link above is to Oleg Kiselyov's explanation, which makes for interesting reading but is not required for what follows. Oleg says: \"Boehm-Berarducci's paper has many great insights. Alas, the generality of the presentation makes the paper very hard to understand. It has a Zen-like quality: it is incomprehensible unless you already know its results.\" Fortunately, to follow the explanation in this article, you don't need to read either Böhm and Berarducci's original paper or Oleg's explanation. For the purpose of this article, it is sufficient to say that the Böhm-Berarducci encoding is a way to encode an algebraic data type as a function type. This means that the data itself is also encoded as a function. As a result, the function encoding the data type becomes a \"universal interpreter\". This makes it is easy to create various interpreters for algebraic data types. In this first part, I will explain a way to implement Böhm-Berarducci (BB) encoding using roles in Raku, with basic examples. In the second part I will show how to use BB encoding to construct a 'universal interpreter' which makes it very easy to create specific interpreters for complex data structures. The basic idea behind the Böhm-Berarducci encoding The basic idea behind the Böhm-Berarducci (BB) encoding is to create a type which represents a function with an argument for every alternative in a sum type. Every argument is itself a function which takes as arguments the arguments of each alternative product type, and returns a polymorphic type. Because the return type is polymorphic, we decide what it will be when we use the BB type. In this way a BB-encoded data structure is a generator for whatever type we like, in other words it is a universal interpreter. For example, if we define a sum type S with three alternatives A1, A2 and A3, using the same notation as in the article on algebraic data types in Raku: then the corresponding BB type will be I have put parentheses to show which part of the type is the function type corresponding to each alterative. Because the constructor for A3 takes no arguments, the corresponding function signature in the BB encoding is simply a: a function which takes no arguments and returns something of type a. The final a is the return value of the top-level function: every type alternative is an argument to the function. When applying the function, it must return a value of a given type. This type is a because a is the return type of every function representing an alternative. I will explain the ∀ a . later. Some simple examples Let's look at a few examples to see how this works in practice. In a previous post I showed how you can use Raku's role feature to implement algebraic data types. I gave the example of OpinionatedBool: which in Raku becomes This is a sum type with two alternatives. The type declaration of the BB type lists the types of all the arguments representing the alternatives. As in this case the constructors for the alternatives take no arguments, the corresponding functions also take no arguments: In Haskell, we would implement this type as follows: You don't need to know any Haskell for what follows, but as the Raku implementation is closely modeled on the Haskell one, it is worth explaining a bit. The newtype keyword in Haskell is used to declare types with a single constructor. What we have here is a record type with a single field, and this field has the accessor function unBoolBB, which is a convenience to allow easy access to the function encoded in the type. The ∀ a or forall a allows us to introduce a type parameter that is only in scope in the expression on the right-hand side. Because the Haskell notation is so close to the formal notation, I will from now on use the Haskell notation. In Raku, we can implement this BB type minimally as a parametric role with a method with a typed signature: This tells us a lot: the parameter to the role has an & sigil so it of type Callable (i.e. it is a function) the method's type tells us that there are two arguments of type Any. The method itself also returns a value of type Any, i.e. there is no constraint on the type of the return value. the parameter to the role has an & sigil so it of type Callable (i.e. it is a function) the method's type tells us that there are two arguments of type Any. The method itself also returns a value of type Any, i.e. there is no constraint on the type of the return value. With this implementation, the type safety is not quite as strong as in Haskell, where we guarantee that all these return values will be of the same type. The main purpose for using the types here is to make it provide documentation. We can enforce the type safety at a different point if desired. Now, the whole idea is that this role BoolBB will serve the same purpose as my OpinionatedBool. So instead of saying I want something like So in this example, BBTrue will be an instance of BoolBB with a specific function as parameter. Let's call that function true, so we have and similar for the false case. We can make this a little nicer using a helper function to create BoolBB instances: In this way we can write In this particular case, because none of the constructors takes any arguments, we can also write this as The question is then: what are the functions true and false? We know they are of type a ⟶ a ⟶ a; an obvious choice is: This is the same choice we made in the article \"Everything is a function\". In fact, these are simply selector functions which select the first or second argument. In practice, we often want to convert between BB types and their algebraic counterparts. To turn a Bool into a BoolBB: To turn a Bool into a BoolBB: To turn the BB Boolean into an actual Boolean: To turn the BB Boolean into an actual Boolean: So we have: (Note that this works with either way of defining BBTrue and BBFalse because calling a function without arguments in Raku does not require parentheses.) We can do this more OO-like by making bool a method of BoolBB: Then we can say and I'm sure those dots will make some people happy. Note however that we do not really need the bool method, instead we can simply compare the types: We can generalise this approach as an alternative to arbitrary enums. For example, and RGB enum can be written very easily as a BB type: with selector functions However, the main reason for using BB types is to make it easier to perform computations on the data structure encoded in the type. Constant sum types like Bool and RGB don't store data to compute on, except in the most trivial way, and are therefore not the main target of this encoding. I presented them only because they are the easiest ones to explain. The second part of the article presents a worked example. But first, let's look at a few more simple examples explaining more features of the BB approach. The Boolean type above had two constructors without arguments. A simple algebraic data type where one of the constructors has an argument is the Maybe type: This type is used to express that a function does not always return a value of a given type. For example, if we look up a key in a map, it is possible that there is no entry for that key. So using Maybe we could write a safe lookup function: The Maybe type is polymorphic, so we have instances of Maybe for any type we like. In Haskell: and in Raku: We use a Callable (&j) for the Just variant but a (sigil-less) scalar (\ ) for the Nothing as it is a constant. As before for the BB Boolean, we create some helper functions. First we have the selectors: First we have the selectors: Then we have a wrapper to make role construction nicer: Then we have a wrapper to make role construction nicer: With these we can easily write the final BB type constructors: With these we can easily write the final BB type constructors: Now we can create values of our MayBB type, e.g. As you can see, the BB type now functions exactly as an ordinary algebraic data type. Let's make a simple printer for this type: As before, this function could be made a method of the MayBB role if desired. The point to note however is that to create this printer, all we had to do was provide the right arguments to unMayBB. We chose the concrete type Str for the type parameter in the BB type. Recall that to turn the BB Boolean into an actual Boolean, all we had to do was to provide arguments of type Bool to unBoolBB. These are simple examples that already illustrate some of the power of the BB encoding. The two previous examples were for sum types. Let's look at a simple product type, a pair of two values also known as a tuple. Assuming the tuple is polymorphic with type parameters t1 and t2, the BB type is in Haskell: and in Raku: The selectors (for convenience we reuse the true and false functions used for the BoolBB): The pair constructor takes the values x and y to be put in the pair, and uses them in an anonymous function used as the parameter for the role. The single argument of this anonymous function is a selector function &p, which is applied to x and y in its body. We can use this to build pairs e.g. As with the Boolean, we can do this a bit more OO-like if you prefer by making fst and snd methods of the PairBB role: Thus we can say An important point is that the BB-encoded data structures are immutable, so you can't update a field. Instead, you create a new variable: Now, let's assume for a moment that our PairBB represents a complex number and we want to convert it from (Real, Imaginary) into polar form (Modulus, Phase). Again we can use the same approach: Summary What we have learned so far is how to create sum (alternative) and product (record) types in Raku using a formalism called Böhm-Berarducci (BB) encoding, which uses functions to create data structures. We use Raku's roles to implement BB types, and I have illustrated this with three simple examples: a sum type with two alternative constructors that do not take arguments (a Boolean), a sum type with two alternative constructors where one of them takes an argument (the Maybe type) and a product type for a pair of two values. In the next part, we will see how BB types make it easy tointerpreter create interpreters for complex data structures. The complete code for both articles is in universal-interpreter.raku. Updated September 12, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/universal-interpreter-part-1/index.html"}, {"title":"Writing faster Perl code","text":" Writing faster Perl code As part of my research I have been developing a Fortran source-to-source compiler — in Perl. The compiler transforms legacy FORTRAN 77 scientific code into more modern Fortran 95. For the reasons to target FORTRAN 77, please read my paper. The compiler is written in Perl because it is available on any Linux-like system and can be executed without the need for a build toolchain. The compiler has no external dependencies at all, so that it is very simple to install and run. This is crucial because my target users are weather and climate scientists, not programmers or computer scientists. Python would have been a viable alternative but I personally prefer Perl. Perl performance as we know it An often-made argument is that if you want performance, you should not write your code in Perl. And it is of course true that compiled code will almost always be faster. However, often rewriting in a compiled language is not an option, so it is important to know how to get the best possible performance in pure Perl. The Perl documentation has perlperf which offers good advice and these tips provide some more detail. But for my needs I did not find the answers there, nor anywhere else. So I created some simple test cases to find out. I used Perl version 5.28, the most recent one, but the results should be quite similar for earlier versions. Testing some hunches Before going into the details on the performance bottleneck in my compiler, here are some results of performance comparisons that influenced design decisions for the compiler. The compiler is written in a functional style — I am after all a Haskell programmer — but performance matters more than functional purity. Fortran code essentially consists of a list of statements which can contain expressions, and the parser labels each of the statements once using a hashmap, ever the workhorse data structure in Perl. Every parsed line of code is stored as a pair with this hashmap (which I call $info): This means than in principle I can choose to match a pattern in $line using a regex or use one of the lables in $info. So I tested the performance of hash key testing versus regexp matching, using some genuine FORTRAN 77 code: Without the if-condidion, the loop takes 3.1 s on my laptop. The loop with the regexp match condition takes 10.1 s; with the hash key existence test it takes 5.6 s. So the actual condition evaluation takes 7 s for regexp and 2.5 s for hash key existence check. So testing hash keys is alsmost three times faster than simple regexp matching. I tested the cost of using higher-order functions for tree traversal. Basically, this is the choice between a generic traversal which takes an arbitrary function that operates on the tree nodes: or a custom traversal: For the case of the tree data structures in my compiler, the higher-order implementation takes twice as long as the custom traversal, so for performance this is not a good choice. Therefore I don't use higher-order functions in the parser, but I do use them in the later refactoring passes. Finally I tested the cost of using map instead of a foreach-loop: The foreach-loop version takes 2.6 s, the map version 3.3 s, so the map is 25% slower. For reference, the index-based for-loop version takes 3.8 s and the C-style for-loop version 4.4 s — don't do that! Because the map is slower, again I did not use it in the parser, and I implemented my own higher-order functions which use foreach-loops internally for the refactoring passes. Compiler bottleneck: expression parsing As the compiler grew in capabilities, it became noticeably slower. Perl has a great profiling tool, Devel::NYTProf, and I used it to identify the bottleneck. As you can see from the flame graph in the banner image, it turned out to be the expression parser. This part of the code was based on Math::Expression::Evaluator because it was convenient to reuse. But it was not built for performance, and also not to parse Fortran. So I finally bit the bullet and wrote my own. What I loosely call an expression parser is actually a combination of a lexer and a parser: it turns a string of source code into a tree-like datastructure which expresses the structure of the expression and the purpose of its constituents. For example if the expression is 2*v+1, the result of the expression parser will be a data structure which identifies the top-level expression as a sum of a multplication with the integer constant 1, and the multiplication of an integer constant 2 with a variable v. So how do we build a fast expression parser? It is not my intention to go into the computing science details, but instead to discuss the choices to be considered. Testing some more hunches First, the choice of the data structure matters. As we need a tree-like ordered data structure, it would have to be either an object or a list. But objects in Perl are slow, so I use a nested list. As it happens, Math::Expression already uses nested lists. Using the parser from Math::Expression, the above expression would be turned into: This data structure is fine if you don't need to do a lot of work on it. However, because every node is labeled with a string, testing against the node type is a string comparison. I did a quick test: On my laptop, the version with string comparison takes 5.3 s, the integer comparison 4.6 s. Without the if-statement, the code takes 3.1 s. In other words, the actual if with string comparison takes 2.2 s, with integer comparison 1.5 s. So doing string comparisons is 50% slower than doing integer comparisons. Therefore my data structure uses integer labels. Also, I label the constants so that I can have different labels for string, integer and real constants, and because in this way all nodes are arrays. This avoids having to test if a node is an array or a scalar, which is a slow operation. So the example becomes : Less readable, but faster and easier to extend. Then we have to decide how to parse the expression string. The traditional way to build an expression parser is using a Finite State Machine, consuming one character at a time (if needed with one or more characters look-ahead) and keeping track of the identified portion of the string. This is very fast in a language such as C but in Perl I was not too sure, because in Perl a character is actually a string of length one, so every test against a character is a string comparison. On the other hand, Perl has a famously efficient regular expression engine. So I created a little testbench to see which approach was faster: On my laptop, the FSM version takes 3.25 s, the regex version 1.45 s (mean over 10 runs), so the regexp version is twice as fast — the choice is clear. A faster expression parser With the choices of string parsing and data structure made, I focused on the structure of the overall algorithm. The basic approach is to loop through a number of states and in every state perform a specific action. This is very simple because we use regular expressions to identify tokens, so most of the state transitions are implicit: The matching rules and operations are very simple (I use <pattern> and <integer> as placeholders for the actual values): prefix operations: perl if ( $str=~s/^<pattern>// ) { $state=<integer>; } terms: perl if ( $str=~s/^(<pattern>)// ) { $expr_ast=[<integer>,$1]; } operators: perl $prev_lev=$lev; if ( $str=~s/^<pattern>// ) { $lev=<integer>; $op=<integer>; } prefix operations: perl if ( $str=~s/^<pattern>// ) { $state=<integer>; } terms: perl if ( $str=~s/^(<pattern>)// ) { $expr_ast=[<integer>,$1]; } operators: perl $prev_lev=$lev; if ( $str=~s/^<pattern>// ) { $lev=<integer>; $op=<integer>; } Operators have precedence and associativity, and Fortran requires twelve precedence levels. In the \"Append to AST\" state, the parser uses $lev and $prev_lev to work out how the previously matched $expr_ast and $op should be appended to the @ast array. The prefix operations are handled by setting a state which is checked after term matching. The actual code is a bit more complicated because we need to parse array index expressions and function calls as well. This is done recursively during term matching; if a function call has multiple arguments, the parser is put into a new $state. So the end result is a minimally recursive parser, i.e. it only uses recursion when it is really necessary. This is because Perl is not efficient in doing recursive function calls (nor in fact for non-recursive ones). There is a lot of repetition of the patterns for matching terms and operators because if I would instead abstract the <pattern> and <integer> values by e.g. storing them in an array, the array accesses would considerably reduce the performance. I do store the precedence levels in an array because there are so many of them that the logic for appending terms to the AST would otherwise become very hard to read and update. Expression parser performance I tested the new expression parser on a set of 50 different expressions taken from a weather simulation code. The old expression parser takes 45 s to run this test a thousand times; the new expression parser takes only 2 s. In other words, the new parser is more than twenty times faster than the old one. It is also quite easy to maintain and adapt despite its minimal use of abstractions, and because it is Fortran-specific, the rest of the code has become a lot cleaner too. You can find the code in my GitHub repo. Summary Here is a summary of all optimisations I tested. The tests were run using Perl v5.28 on a MacBook Pro (late 2013), timings are averages over 5 runs and measured using time. | --- | --- | | Optimisation | Speed-up | | --- | --- | | Hash key testing is faster than regexp matching | 3× | | Custom tree traversals are faster than generic ones | 2× | | foreach is faster than map | 1.3× | | foreach is faster than indexed for | 1.4× | | foreach is faster than C-style for | 1.7× | | Integer comparison is faster than string comparison | 1.5× | | Regexp matching is faster than successive string comparisons | 2.2× | Updated April 27, 2019 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/writing-faster-perl/index.html"}, {"title":"A universal interpreter","text":" A universal interpreter In the previous article I explained the basic idea behind a technique called Böhm-Berarducci encoding of algebraic data types, and showed a way to implement this technique in Raku. Unless you are already familiar with this formalism, I recommend you read that article first. In this article I want to illustrate how the Böhm-Berarducci (BB) encoding of a data structure based on algebraic data types can be considered as a universal interpreter. What this means is that it is easy to perform computations that turn the data structure into something else. As an example, I will demonstrate how to create an evaluator and pretty-printer for a parsed polynomial expression. A parse tree type Consider expressions of the form a*x^2+b*x+c or x^3+1 or x*y^2-x^2*y. Let's assume we have a parser for such an expression, for example built using parser combinators. Let's also assume that this parser returns the parsed data as an algebraic data type, defined in Haskell as: and in Raku: The additional complexity compared to the types discussed in the previous article is that this type is recursive: the Pow, Add and Mult roles take parameters of type Term. Before we look at the BB encoding, let's first write a pretty-printer for this type, using recursive multi subs. In the same way we can write an evaluator for this type: As an example, let's create the parse tree for a few expressions using the Term type. Calling the pretty-printer and evaluator on this term: BB encoding of the parse tree type The BB encoding of the Term algebraic data type in Raku is pleasingly compact: It would of course be even more compact without the signatures, but then we'd have no information about the encoded type. We could of course use this type directly, but instead I want to look at how we can convert between Term and TermBB. As before, we create our little helpers. Each of the functions below is a constructor which generates the TermBB instance for the corresponding alternative in the Term algebraic data type. (When Raku's macro language is more developed, we will be able to generate these automatically.) The interesting generators are PowBB, AddBB and MultBB because they are recursive. In PowBB, the function passed as parameter to the TermBB role constructor calls p which has a signature of :(Any,Int --> Any), but actually requires an argument of the same type as the return value (we need a -> Int -> a). The argument t is of type TermBB which is a wrapper around a function which, when applied, will return the right type. In the Raku implementation, this function is the method unTermBB. So we need to call t.unTermBB( ... ). In AddBB and MultBB, we have an Array[TermBB] so we need to call unTermBB on every element, hence the map call. Using these generators we can write a single function to convert the algebraic data type into its BB encoding. Unsurprisingly, it is very similar to the pretty-printer and evaluator we wrote for Term instances: Because PowBB, AddBB and MultBB require a TermBB, we need to call termToBB on the Term fields. And because AddBB and MultBB take an array of Term, we need a map. However, Raku's map returns values of type Seq, so we need an explicit conversion into Array. We can now convert any data structure of type Term into its BB encoding: To create a pretty-printer for the BB-encoded type, we write implementations for each alternative, and the unTermBB call magically combines these. Compared with ppTerm (copied below for convenience), the main differences are that there is no recursion and no need to map anything. We also don't need a multi sub to pattern match on the constructors, and there is no need to unpack the values stored in the type using attribute accessors. As a result, the BB version is markedly less cluttered. And an evaluator is equally simple: As with evalTerm below, we pass hashes for variable and parameter definitions as arguments to provide context for the evaluation. In the BB version we need to do this only once, rather than for every multi variant, so I have written it below using a given/when. Even then, the BB version is a lot cleaner, for the same reasons as above. Finally, let's look at converting TermBB to Term. This is yet another type of interpreter so we can follow exactly the same approach as before: Using the BB type directly In the examples above I have created the data structures using the Term type and converted the result to a TermBB type. We can of course also directly use the BB type. If we don't use strict typing and make the argument of Add and Mult slurpy, we get a nice and clean representation: This is structurally very similar to the examples using the Term type. We can obtain exactly the same representation by using a slurpy helper function to wrap the role constructors for Term. See the code in no-b-timing.raku` and ubb-timing.raku for details. The code as presented above is not entirely correct: I have not always typed everything explicitly, but the explicit signatures in the role definition will cause type errors unless everything is explicitly typed. See the code in tbb-timing.raku for details. The code in no-bb-timing and ubb-timing is comparable in terms of complexity. I ran a timing test, and the BB implementation of the algebraic data type is about 20% slower than the 'ordinary' implementation. However, the fully-typed version tbb-timing is three times slower. Types in Raku are clearly not zero-cost abstractions. For info, here are the profiling reports (raku --profile) for no-bb-timing and ubb-timing. Profiling tbb-timing proved infeasible. On the other hand, somewhat paradoxically, we don't really need this explicit typing. It is useful to write down the function types for the BB encoding, and I think it helps with the explanations, but the actual type safety comes from the algebraic data types that we created. Conclusion In this article and the previous one I have shown another way to implement algebraic data types in Raku. As with the approach discussed in 'Roles as Algebraic Data Types in Raku', I use a role to create the type. However, in this approach the entire data structure is encoded as a function using the Böhm-Berarducci encoding. From a type theoretical perspective, both approaches are precisely equivalent. In terms of coding effort and performance, both approaches are comparable. The advantage of the BB approach is that because the data is encoded as a function, it becomes easier to create interpreters for the data type, and I have illustrated this with a pretty-printer and evaluator for a parsed expression. All interpreters for BB types have the same structure, which is why I call it a universal interpreter. The key feature is that these interpreters do not require any explicit recursion. The complete code for both articles is in universal-interpreter.raku Updated September 13, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/universal-interpreter-part-2/index.html"}, {"title":"Everything is a function","text":" Everything is a function This is an article I wrote several years ago. It is part of the \"Functional Programming in Haskell\" online course. It discusses one of the aspects of functional programming that I like in particular, the fact that the entire language can be build starting from the lambda calculus. In a functional language, there are only functions Although it might seem that a language like Haskell has a lot of different objects and constructs, they can all be reduced to functions. We will demonstrate how variables, tuples, lists, conditionals, Booleans and numbers can all be constructed from lambda functions. The article assumes some familiarity with Haskell, but here is a quick introduction. Haskell is whitespace-sensitive like Python, but has a markedly different syntax. Because everything is a function, there is no keyword to mark a function; because there is only lexical scope, there is no need for any special scope identifiers. Function arguments are separated by spaces; anonymous functions are called lambda functions and have a special syntax to identify them: Several of the examples use the let ... in ... construct, which behaves as a lexically scoped block: x0, x1 and x2 are in scope only in the expression after the in keyword. Haskell is statically typed, and the type of a function or variable is written in a separate annotation, for example: The isEmpty function has a type signature, identified by ::, that reads \"isEmpty is a function from a list of anything to a Boolean\". Types must be written with an initial capital. The a is a type variable which can take on any type, as explained in my post on algebraic data types. Most of the time, you don't need to write type declarations as Haskell works them out automatically. In the rest of the article, I'm not focusing on the types so I have omitted them. Haskell uses let expressions to define variables used in a final expression, for example: We can rewrite this to use only one variable per let: Now we rewrite any named functions (f) as lambda functions: Then we rewrite the let expressions themselves as lambdas, first the inner let: We do this by turning the variable in the let part of the expression (f) into a parameter of a lambda function (\\f -> ...). The body of the function is the expression after the in (f n). Then we apply this lambda function to the expression bound to the variable (\\x -> x+1). Then we rewrite outer let in the same way: This expression consists only of lambda expressions, which shows that variables and let-expressions are just syntactic sugar for lambda expressions. Haskell has tuples, also called record types or product types, ordered collections of expressions of potentially different types: The tuple notation is syntactic sugar for a function application: The tuple construction function can again be defined purely using lambdas: What we do here is to use the elements of the tuple as the arguments of a lambda function. So what mkTup returns is also a lambda function, in other words mkTup is a higher-order function. Now we rewrite the mkTup named function as lambda function as well: So our tuples are now also encoded purely as lambda functions. The same goes for the tuple accessor functions: Let's see what happens here: the argument tp of fst is a function: \ -> t x' y' z'. We now apply this function to another function, \\x y z -> x: Applying the function gives: And so the result will of course be x', which is indeed the first element of the tuple. Lists can be defined in terms of the empty lists [] and the cons operation (:). Rewriting this using : and []: Or using cons explicitly: We can define cons using only lambda functions as We've used the same approach as for the tuples: cons returns a lambda function. So we can write a list as: We can also define head and tail using only lambdas, similar to what we did for fst and snd above: We can define the empty list as follows: This is a lambda function which always returns true, regardless of its argument. The definitions for true and false are given below under Booleans. With this definition we can check if a list is empty or not: Let's see how this works. A non-empty list is always defined as: which with our definition of (:) (i.e. cons) is: And therefore: And so we have a pure-lambda definition of lists, including construction, access and testing for empty. Now that we can test for the empty list we can define recursions on lists such as foldl, map etc.: and with The definitions of foldl and map use an if-then-else expression which is defined below under Conditionals. With foldl and reverse it is easy to express list concatenation: To compute the length of a list we need integers, they are defined below. We increment the lent counter for every element of the list consumed by the fold. We have used conditionals in the above expressions: Here cond is an expression returning either true or false, these are defined below. We can write the if-then-else clause as a pure function: To evaluate the condition we need to define Booleans as lambda functions: The Boolean is a function selecting the expression corresponding to true or false. With this definition, the if-then-else becomes simply: Using ifthenelse we can define and, or and not: We note that to test equality of Booleans we can use xnor, and we can of course define xor in terms of and, or and not: and The common way to define integers in the lambda calculus is as Church numerals. Here we take a different approach, but it is of course equivalent. We define an integer as a list of Booleans, using thermometer code, and with the following definitions: We define unsigned 0 as a 1-element list containing false. To get signed integers we simply define the first bit of the list as the sign bit. We define unsigned and signed versions of 0: For convenience we define also: The definition of 0 makes the integer equality (==) easier: We can also easily define negation: For convenience we define also define increment and decrement operations: General addition is quite easy: In the same way, subtraction is also straightforward: An easy way to define multiplication is by defining the replicate and sum operations: Then multiplication simply becomes In a similar way we can define integer division and modulo. We note that floating-point numbers and characters use an integer representation, and strings are simply lists of characters. So we don't need to do any additional work to represent them, and the operations on them are analogous to the ones defined above. Conclusion In this way, we have defined a language with variables, (higher-order) functions, conditionals and recursion. We can manipulate lists and tuples of integers, floats, chars and strings. And yet it consists of nothing more than lambda functions! Updated July 03, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/everything-is-a-function/index.html"}, {"title":"Function types","text":" Function types This article builds on my earlier articles on algebraic data types in on Raku and their use in the practical example of list-based parser combinators. In this article I want to look at function types in some detail, and show a way to create well typed functions in Raku. If you are not familiar with functional programming (or with Raku), I suggest you read my introduction \"Cleaner code with functional programming\". If you are not familiar with algebraic data types, you might want to read the other two articles as well. For most of the article, I provide examples in Raku, Python, Rust and Haskell. There is also some C and even some Fortran. Type signatures A function's type signature consists of the types of each of its arguments and the return type. In most typed languages, the type is part of the function signature. For example in C or in Fortran or in Rust or in Raku or in Python, using the typing module: The type of a function of functions But what happens if we want to provide a function argument that is itself a function, or return a function (so-called higher-order functions)? This is possible in most languages, but what I am interested in is the type information: what is the type signature of such a function of functions? C supports functions-of-functions indirectly through function pointers, by creating a function type through a typedef. Maybe surprisingly, venerable old Fortran does support passing functions and subroutines as arguments. Functions are typed by their return type; subroutines are not typed. In Rust you can provide the complete type of a function-as-argument: In Python we can use Callable, which also allow for the complete type to be expressed. The same example in Raku becomes So we can also pass the complete type. An equivalent way to write this is using the & sigil which imposes the Callable type constraint: The types can be nested too, e.g. ((Int, Int --> Int), (Int, Int --> Int) --> Int)) is a valid type signature. Introducing the arrow All of the above ways to express function type signatures are perfectly adequate in their respective languages. However, with the exception of Raku, they all share the problem that these function-of-function type signatures don't compose very well: what if we want to write a function-of-function-of-function type? This is less far-fetched than it may seem. I would like to introduce a notation used in type theory. It is at the same time simple and powerful. If you are familiar with functional languages like Haskell, Idris or Agda, you already know it. Instead of mixing the type with the function declaration, it is written separately. The name of the function is followed by a colon and the list types of the arguments and the return value. Each argument is separated by an arrow. The above example of a function of two integer arguments returning an integer would be: The function-of-a-function introduced above has as type: The parentheses group the type of the function that is the only argument of ten_times. In this notation, the arrow can be interpreted as an operator which creates a function type from the two types that are its arguments. The important property of this operator is that it is right associative. What this means is that for example is the same as and as and for completeness A detour into partial application The above groupings imply that our function f can be interpreted in three ways, as a function of: 3 arguments of types t1,t2,t3, returning a result of type t4; 2 arguments of types t1,t2, returning a result of type t3 -> t4; 1 argument of types t1, returning a result of type t2->t3->t4. 3 arguments of types t1,t2,t3, returning a result of type t4; 2 arguments of types t1,t2, returning a result of type t3 -> t4; 1 argument of types t1, returning a result of type t2->t3->t4. Let's say we have values v1,v2,v3 for the arguments and v4 as the result: But suppose we only apply v1 and v2: We get a new function pf1 which takes a single argument v3: And in the same way we can create pf2 and pf3: Because pf1, pf2 and pf3 are functions and the above is true for all values of v1, v2, v3 and v4, it follows that For completeness, we can also apply pf2 directly to two arguments: This concept of creating a new function by not providing values for some of the arguments is called partial application, and many languages support it. Here are examples in Haskell, Raku, Python and Rust. In case you are not familiar with Haskell, this is what you need to know: it is whitespace-sensitive like Python, but has a markedly different syntax. Because everything is a function, there is no keyword to mark a function. Because there is only lexical scope, there is no need for any special scope identifiers. Function arguments are separated by spaces. Lambda functions (anonymous functions) start with a \\, chosen because it looks a bit like the Greek letter lambda, λ. The type of sqsum is haskell sqsum :: Int -> Int -> Int (Haskell uses :: rather than : for the type signature) In Haskell, partial application works exactly as in the examples above. So our function sqsum can be partially applied like this: We can apply sqsum4 to the remaining argument: This is very neat. But suppose you want to apply the second argument, rather than the first one? The Haskell Prelude library provides the function flip, which simply flips the arguments: That is fine as far as it goes, but let's do a somewhat contrived example. Let's say we have a function of four arguments: and we want to apply the 1st and 4th argument but not the others, something like g v1 _ _ v4. One way to do this is to create yet another function (of course!): And with this function we can partially apply the 1st and 4th argument of g: This example mainly serves to illustrate the power of the arrow-based function type notation: it lays out the type of apply14 clearly and concisely. Raku provides the method assuming, which acts as a generalised version of our apply14: The return type of assuming is a Callable. This is a role for objects which support calling them. Thus, g14 can be called as if it was a regular function. Python's functools provide the partial function: The return type of partial is a partial object, which has an attribute partial.func, a callable object or function. Calls to the partial object will be forwarded to func with new arguments and keywords, so you can say g14(v2,v3) instead of g14.func(v2,v3). Rust provides the partial! macro via its partial_application crate. Its behaviour is very similar to our apply14: \"partial!(some_fn => arg0, _, arg2, _) returns the closure |x1, x3| some_fn(arg0, x1, arg2, x3)\". Back to the function types Suppose we want a type like the one we defined in C using a typedef, which encapsulates the function type: In Haskell, that would be and we can generalise this to be a generic function of two arguments by using type variables instead of concrete types: So how would we use this? Let's create an instance This is fine, but to apply the function we first must unwrap the type constructor: That is not very handy. A better way is to use the record type syntax which gives us an accessor function: Now I have applied this to integer, but the type of the function is Num a => a -> a -> a, so this works for any type in the Num typeclass. Named function types for Raku In Raku, we can follow a similar approach of wrapping a function signature in a type, and it is actually simpler than in Haskell. We create a parametric role which takes the function as a parameter, and has a method with the signature of the function: But what is the benefit of doing this? Surely we could just have done For this simple example, that would indeed be enough as we don't have functions of functions. But what we gain is that we can now create a function with arguments of type Fun2NumArgs: In other words, we can now have explicitly typed function signatures in Raku. Recall that without this approach, the type of a function would be Code or any dependant in the Code type graph. With the role-based type, the function must have the type of the method unF. Furthermore, these function types can be nested. Let's create another type, for a function with two arguments of any type: We create two instances of Fun2NumArgs: And a function of these two functions using Fun2Args: We can now call the returned function like this: Having to call the unF method is not optimal. A better way is to can make the object itself callable instead, by defining the submethod CALL-ME instead of the method unF: In this way, we can do: This is almost what we want. But we can remove the . as well, by making fof2 of type Callable. We can indicate this with the & sigil. But with the current definition of Fun2Args, this will result in a type error because Fun2Args is not callable. However, Callable is a role so all we need to do is mix it in: In this way we have created something very similar to a function object, but using a role rather than a class. And now we can write: To summarize, we create a parametric callable role where the parameter is the function to be called, and the signature of the CALL-ME submethod provides the type constraint to that function. Passing a function with a different signature will give a type error. I think this is a nice way to have some additional type safety in your functional Raku code. Bonus Tracks \"Call Me\" by Blondie \"CALL ME\" (「コール・ミー」) by Drop's \"Call Me\" by Blondie \"CALL ME\" (「コール・ミー」) by Drop's Updated August 07, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/function-types/index.html"}, {"title":"Hacking the Pleroma: Elixir, Phoenix and a bit of ActivityPub","text":" Hacking the Pleroma: Elixir, Phoenix and a bit of ActivityPub Pleroma \"is a microblogging server software that can federate (= exchange messages with) other servers that support the same federation standards (OStatus and ActivityPub). What that means is that you can host a server for yourself or your friends and stay in control of your online identity, but still exchange messages with people on larger servers. Pleroma will federate with all servers that implement either OStatus or ActivityPub, like GNU Social, Friendica, Hubzilla and Mastodon.\" (stolen from Lain's blog post). Recently I modified my Pleroma instance to support bot services: parse a posted message, take an action, post the result. To get there I had to learn Elixir, the language in which Pleroma is written, as well as Phoenix, the web framework Elixir uses, and a little bit about ActivityPub, the protocol for exchanging messages. What I want to explain here in particular is the architecture of Pleroma, so that you can hack it more easily, for fun or if you want to participate in the development. Elixir As Pleroma is written in Elixir you'll need to learn that language to some extent. If you are familiar with Ruby (or Perl, for that matter) and with the idea of functional programming (everything is a function), then it is quite easy to learn and understand. The documentation and guides are very good. If you've never hear of functional programming, the main difference with e.g. Ruby or Java is that Elixir does not use an object-oriented programming model. Instead, there are functions that manipulate data structures and other functions. A particular consequence of the functional model is that there are no for- or while-loops. Instead, there are what is called higher-order functions which e.g. apply another function to a list. Elixir programs also make a lot more use of recursion. Another point about Elixir as a web programming language is that it is built on a system where processes communicate by passing messages to one another, and it is built in such a way that if a process dies it will normally be restarted automatically. This approach makes it very easy to offload work to separate worker processes etc. All this comes courtesy of Erlang, the language on which Elixir is built, with its powerfull OTP framework for building applications and its BEAM virtual machine, which manages the processes. Phoenix A lot of the groundwork of Pleroma is done by Phoenix, a very easy-to-use web server framework. Essentially, what happens is that the end user accesses the application using a specific url, typically via a web browser, and based on this url the application performs a number of actions, which in the end result in a change in the state of the application and usually in what is shown in the browser window. In Phoenix, there are five stages or components between the connection and the resulting action by the application: The endpoint is the boundary where all requests to your web application start. It is also the interface your application provides to the underlying web servers. Pleroma's endpoint is web/endpoint.ex. If you look at the source you see several occurrences of plug(Plug...). Plug is a specification for composable modules in between web applications, and it is very heavily used in Pleroma. For example, to serve only specific static files/folders from priv/static: Another very nice feature of Phoenis is that you can edit your code while your server is running. It gets automatically recompiled and the affected processes are automatically restarted, courtesy of the Phoenix.CodeReloader: Routers are the main hubs of Phoenix applications. They match HTTP requests to controller actions, wire up real-time channel handlers, and define a series of pipeline transformations for scoping middleware to sets of routes. Pleroma's router is web/router.ex. The key function in the router is the pipeline which lets you create pipelines of plugs. Other functions are scope, get, post, pipe_through, all of these let you match on the url and whether you are dealing with a get or post request, and define appropriate pipelines of actions. For example, federated ActivityPub requests handled as follows: where the pipe_through(:activitypub) call is used to insert a custom pipeline: Controllers are used to group common functionality in the same (pluggable) module. Pleroma makes heavy use of controllers: almost every request is handled by a specific controller for any given protocol, e.g. MastodonAPIController or ActivityPubController. This makes it easy to identify the files to work on if you need to make a change to the code for a given protocol. For example, the ActivityPub post requests in the Router are handled by inbox function in the ActivityPubController: Views are used to control the rendering of templates. You create a view module, a template and a set of assigns, which are basically key-value pairs. Pleroma uses views for \"rendering\" JSON objects. For example in web/activity_pub/activity_pub_controller.ex there are lines like Here, UserView.render is defined in web/activity_pub/views/user_view.ex for a number of different \"*.json\" strings. These are not really templates, they are simply used to pattern match on the function definitions. The more conventional usage to create HTML is also used, e.g. the template web/templates/mastodon_api/mastodon/index.html.eex is used in web/mastodon_api/mastodon_api_controller.ex via the view web/mastodon_api/views/mastodon_view.ex: Templates are text files (typically html pages) with Elixir code to generate the specific values based on the assigns, included in <%= ... %>. For example, in Pleroma, the Mastodon front-end uses a template for the index.html file which has the code to show the name of the instance. Ecto Ecto is not a part of Phoenix, but it is an integral part of most web applications: Ecto is Elixir's main library for working with databases. It provides the tools to interact with databases under a common API. Ecto is split into 4 main components: Ecto.Repo - repositories are wrappers around the data store. Via the repository, we can create, update, destroy and query existing entries. A repository needs an adapter and credentials to communicate to the database Pleroma uses the PostgresQL database. Ecto.Schema - schemas are used mainly to map tables into Elixir data (there are other use cases too). Ecto.Changeset - changesets provide a way for developers to filter and cast external parameters, as well as a mechanism to track and validate changes before they are applied to your data Ecto.Query - written in Elixir syntax, queries are used to retrieve information from the database. GenServer Because Elixir, like Erlang, uses a processes-with-message-passing paradigm, client-server relationships are so common that they have been abstracted as a behaviour, which in Elixir is a specification for composable modules which have to implement specified public functions (a bit like an interface in Java or typeclass in Haskell). If we look at the Federator.enqueue function, its implementation actually reduces to a single line: GenServer is an Elixir behaviour module for implementing the server of a client-server relation. The cast call sends an asynchronous request to the server (synchronous requests use call). The server behaviour is implemented using the handle_cast callback, which handles cast calls. In Pleroma.Federator, these are implemented in the same module as the enqueue function, hence the use of __MODULE__ rather than the hardcoded module name. Applications, Workers and Supervisors Elixir borrows the concept of a \"supervision tree\" from Erlang/OTP. AN application consists of a tree of processes than can either be supervisors or workers. The task of a supervisors is to ensure that the worker processes do their work, including distributing the work and restarting the worker processes when they die. Supervisors can supervise either worker or other supervisors, so you can build a supervision tree. Elixir provides an Application behaviour module and a Supervisor module to make this easy. The Application module requires a start() function as entry point. Typical code to create a supervision tree is where start_link() spawns the top process of the tree, and it spawns all the child processes in the list children. Pleroma uses a convenient but deprecated module called Supervisor.Spec which provides worker() and supervisor() functions, for example: Every worker has this own start_link function, e.g. in web/federator/federator.ex we find: This means that the Federator module borrows the start_link from the GenServer module. This is a very common way to create a worker. Mix Mix is the build tool for Elixir, and its main advantage is that the build scripts are also written in Elixir. Some key mix actions are provided by Phoenix, for example to build and run the final Pleroma application the action is mix phx.server. Hacking Pleroma After this brief tour of Elixir and Phoenix I want to give an example of adding simple bot functionality to Pleroma. See my fork of Pleroma for the code. My bot parses incoming messages for @pixelbot, extracts a list of pixel from the message, modifies a canvas with the new pixels and creates a PNG image of the result. It then posts a link to the PNG image. Because updating the canvas and creating the PNG image could be time-consuming, especially if the canvas were large, I put this functionality in a separate server module, and added this to the list of workers for the main Pleroma application: The bot takes the size of the canvas from my config.exs using the helper function get_canvas_size(). The id: PixelBot allows to access the worker by name. When the application starts, it launches the PixelBot worker (bots/pixelbot.ex). The worker calls its init() function (part of the GenServer behaviour) which loads the last canvas from a file. One of the protocols used for federation is ActivityPub. The specification is long and not so easy to read. However, for the purpose of hacking Pleroma it mainly helps to understand the structure of an ActivityPub action (in this case a post): In my case, In Pleroma this activity is linked to the Ecto repository Pleroma.Repo (repo.ex) in the module Pleroma.Activity (activity.ex), which defines a schema. The bot only supports ActivityPub. As we have seen above, in Pleroma incoming messages are handled by inbox function in the ActivityPubController (in web/activity_pub/activity_pub_controller.ex), so I put in a little hook there to detect if a message is for @pixelbot and has an actual message body (content): As you can see, the content of a message for @pixelbot is passed on to the PixelBot worker for processing using the GenServer.cast(Pleroma.Bots.PixelBot,content) call. The PixelBot worker parses the message to extract any pixels from it (bots/pixelbot/parse_messages.ex). If there are any, it updates the canvas (which is just a list of lists). It and writes the content to a file, and calls an external program to create the final image. Finally, the bot posts a status to the public timeline (bots/pixelbot/pixelbot_post_status.ex). The status contains the current time and a link to the latest canvas. The function pixelbot_post_status() creates the status and wraps it in the correct structure required by ActivityPub. It also gets the user object based on the nickname via Pleroma.User.get_cached_by_nickname(nickname). Like the activity, this user object is defined via a schema and linked to the Ecto repository (in user.ex). So user in the code below is a complicated object, not a url or nickname. Finally, the function calls ActivityPub.create() which creates the activity, and in this case that means it posts a status. Pleroma source tree This is only a part of the Pleroma source tree, it shows on the files mentioned above. Updated April 19, 2018 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/hacking-pleroma.html"}, {"title":"Roles as Algebraic Data Types in Raku","text":" Roles as Algebraic Data Types in Raku I have been a lambdacamel, one of those who like Perl and functional programming, especially in Haskell, for a long time. I still write most of my code in either of these languages. I've also been a fan of Raku from long before it was called Raku, but I'd never used it much in real life. Recently though, I've been moving increasingly to Raku for code that I don't have to share with other people. It's a lovely language, and its functional heritage is very strong. It was therefore only natural to me to explore the limits of Raku's type system. Is this article for you? In this article I will introduce algebraic data types, a kind of static type system used in functional languages like Haskell, and a powerful mechanism for creating complex data structures. I will show a way to implement them in Raku using roles. You don't need to know Haskell at all and I only assume a slight familiarity with Raku (I've added a quick introduction), but I do assume you are familiar with basic programming. You may find this article interesting if you are curious about functional-style static typing or if your would like an alternative to object-oriented programming. Algebraic Data Types Datatypes (types for short) are just labels or containers for values in a program. Algebraic data types are composite types, they are formed by combining other types. They are called algebraic because they consist of alternatives (sums, also called disjoint unions) and record (products) of types. For more details see [1] or [2]. To give a rough intuition for the terms \"sum type\" and \"product type\": in Raku, with Booleans $a, $b and $c, you can write $a or $b or $c but you could also write $a + $b + $c and evaluate it as True or False. Similarly, $a and $b and $c can be written as $a * $b * $c. In other words, and and or behave in the same way as + and *. In a generalised way, the types in algebraic data type system can be composed using similar rules. Let's first give a few examples of algebraic data types. In this section I am not using a specific programming language syntax. Instead I use a minimal notation to illustrate the concepts. I use the datatype keyword to indicate that what follows is a declaration for an algebraic data type; for a sum type, I'll separate the alternatives with '|'; for a product type, I separate the components with a space. To declare a variable to be of some type, I will write the type name in front of it. We can define a Boolean value purely as a type: And we can use this as This means that ok is a variable of type Bool with a value of True. In an algebraic data type, the labels are called 'constructors'. So True is a constructor that takes no arguments. For a product type, we could for example create a type for an RGB colour triplet: The RGB label on the right-hand side is the constructor of the type. It takes three arguments of type Int: So aquamarine is a variable of type RGBColour with a value of RGB 127 255 212. The constructor identifies the type. Suppose we also have an HSL colour type with a variable chocolate of that type: then both RGB and HSL are triplets of Int but because of the different type constructors they are not the same type. Let's say we create an RGB Pixel type: then is fine but will be a type error because chocolate is of type HSLColour, not RGBColour. We could support both RGB and HSL using a sum type: and change make a Pixel type definition: And now we can say I can hear you say: but what about Int, it doesn't have constructors? And what about a string, how can that be an algebraic data type? These are interesting questions as they allow me to introduce two more concepts: recursive and polymorphic types. From a type perspective, you can look at an integer in two ways: if it is a fixed-size integer then the Int type can be seen as a sum type. For example, the type for an 8-bit unsigned integer could be In other words, every number is actually the name of a type constructor, as a generalisation of the Bool type. However, in the mathematical sense, integers are not finite. If we consider the case of the natural numbers, we can construct a type for them as follows: The Z stands for \"zero\", the S for \"successor of\". This is a recursive type, because the S constructor takes a Nat as argument. With this type, we can now create any natural number: This way of constructing the natural numbers is called Peano numbers. Now, what about strings? Enumerating all possible strings of any length is not practical. But from a type perspective, a string is a list of characters. So the question is then: what is the type of a list? For one thing, a list must be able to contain values of any type. (In the context of algebraic datatypes, all values must be the same, so our list is more like a typed array in Raku.) But that means we need types that can be parameterised by other types. This is called parametric polymorphism. So a list type must look something like where a is a type variable, i.e. it can be replaced by an arbitrary type. For example, assuming we define the Char type simply by enumerating all characters in the alphabet (because of course, at machine level, every character is represented by an integer number): Then we can type our string as: But what about List? We use a similar approach as for Nat above, using a recursive sum type: Now we can create a list of any length: Using the typical syntactic sugar for lists, we can write this as If I now invent an alias Str for List Char, and use double quotes instead of list notation, I can write So integers and strings can be expressed as algebraic data types, and now we have introduced recursive and parameterised types. These may seem like rather contrived examples, after all a language like Raku already has an Int and a Str type that work very well. So what is the use of these algebraic data types? Of course the purpose of static types is to provide type safety and make articles/debugging easier. But using algebraic data types also makes a different, more functional style of programming possible. One common use case is a list where you want to store values of different types: you can create a sum type that has an alternative for each of these types. Another common case is a recursive type, such as a tree. Finally, the polymorphism provides a convenient way to create custom containers. I will give examples of each of these in the next section. Time to move on to Raku! Algebraic data types in Raku As Raku is not a very well-known language (yet), here is a quick introduction of the features you'll need to follow the discussion below. Before Raku went its own way, it was meant to be the next iteration of Perl (hence the original name Perl 6). It is therefore more similar to Perl than to any other language. Raku is syntactically similar to C/C++, Java and JavaScript: block-based, with statements separated by semicolons, blocks demarcated by braces, and argument lists in parentheses and separated by commas. The main feature it shares with Perl is the use of sigils ('funny characters') which identify the type of a variable: $ for a scalar, @ for an array, % for a hash (map) and & for a subroutine. Variables also have keywords to identify their scope, I will only use my which marks the variable as lexically scoped. A subroutine is declared with the sub keyword, and subroutines can be named or anonymous: Raku also has twigils, secondary sigils that influence the scoping of a variable. For this article, the only twigil used in the code is . which is used to declare a role or class attribute with automatically generated accessors (like $.notes in the example below). Raku supports sigil-less variables, and uses the \\ syntax to declare them. For more on the difference between ordinary and sigil-less variables, see the Raku documentation. For example (say prints its argument followed by a newline): Raku has gradual typing: it allows both static and dynamic typing. That's a good start because we need static typing to support algebraic data types. It also has immutable variables and anonymous functions, and even (limited) laziness. And of course functions are first-class citizens, so we have everything we need for pure, statically-typed functional programming. But what about the algebraic data types? In Raku, enums are sum types: However, they are limited to type constructors that don't take any arguments. Classes can be seen as product types: However, classes do not support parametric polymorphism. This is where roles come in. According to the Raku documentation: Roles use the keyword role preceding the name of the role that is declared. Roles are mixed in using the does keyword preceding the name of the role that is mixed in. Roles are what in Python and Ruby is called mixins. So roles are basically classes that you can use to add behaviours to other classes without using inheritance. Here is a stripped-down example take from the Raku documentation (has declares an attribute, method a method) In particular, roles can be mixed into other roles, and that is one of the key features I will exploit. Furthermore, role constructors can take arguments and they are parametric. So we have everything we need to create proper algebraic data types. Let's look at a few examples. This is the example of a sum type for a Boolean as above, but implemented with roles. The first line declares the type as an empty role, this corresponds to the data type name on the left-hand side. The next lines define the alternatives, each alternative uses does OpinionatedBool to tie it to the OpinionatedBool role which functions purely as the type name. In Raku, types are values; and for a role with an empty body, you don't need the .new constructor call. In a sum type, the alternatives usually are labelled containers for values, but they can be empty containers as well. When that is the case, there is no need to create separate instances of them because there is only one way to have an empty container. Sum types can be used in combination with Raku's multi sub feature: Raku lets you provide several definitions for a function, with the same name but different signatures. With multi subs we can do what is known as pattern matching on types: Because we use a type as a value, to test if a value is AbsolutelyTrue or TotallyFalse, we can use either the smart match ~~, the container (type) identity =:= or the value (instance) identity === to test this (the smart match operator behaves like =:= if the right-hand side is a type and as === if it is an object instance). If we would create an instance like AbsolutelyTrue.new, this would not be the case. See the code example for more details. Here is the implementation of the Colour, XYCoord and Pixel types from above. The RGBColour type is an example of a product type. There are two differences with my notation from above: Because the role serves both as the type (RGBColour) and the instance constructor (RGB), they must have the same name. I only named them differently to make it easier to distinguish them so this is not an issue. The types that make up each field must be named with unique names in the role's argument list, and need to have a corresponding attributes declared. That is again not really a limitation, because accessors for record type fields are handy. So it looks like: (the role's parameters are in square brackets) And we create aquamarine like this: The definitions of HSLColour and XYCoord are analogous, you can find them in the code example. Let's look at the sum type to combine the RGB and HSL colour types: This is essentially the same approach as for the opinionated Boolean, but we don't have empty roles: the HSL alternative takes an argument of type HSLColour, and the RGB alternative takes an argument of type RGBColour. As in the product type, we use the role as a container to hold the values. The Pixel type from above looks like: And now we can create pixels with RGB and HSL colours: Above, I showed the Peano number type to illustrate type-level recursion. This works just fine with roles in Raku too: And we can combine this with type parameters as in the list example: (The prefix :: is the Raku syntax to declare type variables) There are some issues here: The EmptyList alternative must either be declared as above, with a type parameter, or as The EmptyList alternative must either be declared as above, with a type parameter, or as where the type also doesn't take a type variable. We can't write This is of course only a minor issue, resulting only in some redundancy. A more serious issue is that the type of lst must be List (or List[]) instead of List[a]. That is actually a problem, as it weakens the type checking. So it must be a bug in the current version of raku (2020.01). When I provide List[a] I get the following error: A more serious issue is that the type of lst must be List (or List[]) instead of List[a]. That is actually a problem, as it weakens the type checking. So it must be a bug in the current version of raku (2020.01). When I provide List[a] I get the following error: For the first example, I want to store values of different types in a typed array. They elements can be strings, labeled lists of strings, or undefined. I call this type Matches. Using the notation from above, it would be In Raku, it is defined as follows: This type uses type constructors with 0 (UndefinedMatch), 1 (Match) and 2 (TaggedMatches) arguments, and the latter is a recursive type: the second argument is a list of Matches. With this definition, we can create an array of matches like this: As you can see, the typed values are actually constructed by calling .new. It is a bit nicer to create constructor functions, and once Raku has a more developped macro system, we might be able to generate these automatically. Code for this example For the next example, I want to define a type called Either. This is a parametric sum type with two parameters, so a kind of generic tuple: In Raku, this can be done through the use of type variables as parameters for the role: Because Raku expects both type variables to be declared in each constructor, it is a little bit less nice than my more abstract notation. We can pattern match on this type with a multi sub: So we can write Code for this example As a final example, here is a simple binary tree. First, let's look at an example implementation using a role from the Raku documentation: This example contains quite a bit of Raku syntax: Raku allows dashes in names; the -> syntax is a foreach loop, iterating over all elements of the preceding list; The .. is array slicing; ::?CLASS is a compile-time type variable populated with the class you're in and :U is a type constraint which specifies that it should be interpreted as a type object. Finally, the : marks the argument to its left as the invocant. In other words, it allows us to write BinaryTree[Int].new-from-list(4, 5, 6) where BinaryTree[Int] is the value of::?CLASS`. This is the Raku way to create custom constructors. The * in front of the @el argument of new-from-list makes this a variadic function where @el contains all arguments; The => syntax allows to assign arguments by name rather than by position; ?? ... !! ... is Raku's syntax for C's ternary ? ... : ...; Raku allows dashes in names; the -> syntax is a foreach loop, iterating over all elements of the preceding list; The .. is array slicing; ::?CLASS is a compile-time type variable populated with the class you're in and :U is a type constraint which specifies that it should be interpreted as a type object. Finally, the : marks the argument to its left as the invocant. In other words, it allows us to write BinaryTree[Int].new-from-list(4, 5, 6) where BinaryTree[Int] is the value of::?CLASS`. This is the Raku way to create custom constructors. The * in front of the @el argument of new-from-list makes this a variadic function where @el contains all arguments; The => syntax allows to assign arguments by name rather than by position; ?? ... !! ... is Raku's syntax for C's ternary ? ... : ...; This example is written in Raku's object-oriented style, with methods acting on the attributes of the role. Let's see how we can write this in a functional style. The algebraic data type for this binary tree is: The Tip alternative is for the empty leaf nodes of the tree, which in the above example are left undefined. In Raku, we can implement this type as: Instead of the methods we use functions, implemented as multi subs. Most of the code is of course identical, but there is no need for conditionals to check if a leaf node has been reached. I have also used sigil-less immutable variables. One thing to note is that in the multi subs we don't have to match against the full type, for example in visit-preorder we match against Tip and Node rather than the full Tip[a] and Node[::a,BinaryTree[a],BinaryTree[a],a]. Code for this example Wrap-up Creating algebraic data types with Raku's roles is very straightforward. Any product type is simply a role with a number of typed attributes. The key idea for the sum type is to create an empty role and mix it in with other roles that become the type constructors for your alternatives. Because roles accept type parameters, we can have parametric polymorphism. And because a role can have attributes of its own type, we have recursive types as well. Combined with Raku's other functional programming features, this makes writing pure, statically typed functional code in Raku great fun. [1] \"The algebra (and calculus!) of algebraic data types\", by Joel Burget [2] \"The Algebra of Algebraic Data Types, Part 1\", by Chris Taylor Updated June 05, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/roles-as-adts-in-raku/index.html"}, {"title":"Writing faster Raku code","text":" Writing faster Raku code In an earlier article, I discussed the result of my attempts to optimize the performance of an expression parser which is part of my Perl-based Fortran source-to-source compiler. An expression parser takes strings representing expressions in a programming language (in my case Fortran) and turns it into a data structure called a parse tree, which the compiler uses for further analysis and code generation. I have recently been writing quite a bit of Raku code but so far I had not looked at its performance. Out of curiosity I decided to rewrite and optimise this Fortran expression parser in Raku. Expression parsing What I loosely call an expression parser is actually a combination of a lexer and a parser: it turns a string of source code into a tree-like data structure which expresses the structure of the expression and the purpose of its constituents. For example if the expression is 2*v+1, the result of the expression parser will be a data structure which identifies the top-level expression as a sum of a multiplication with the integer constant 1, and the multiplication of an integer constant 2 with a variable v. So how do we build a fast expression parser in Raku? It is not my intention to go into the computing science details, but instead to discuss the choices and trade-offs to be considered. Raku performance testing An easily-made argument is that if you want performance, you should not write your code in Raku but in C/C++. And it is of course true that compiled code will almost always be faster. However, often, rewriting in a compiled language is not an option, so it is important to know how to get the best possible performance in Raku. The Raku documentation has a page on performance which offers good advice in general terms. But for my needs I did not find the answers about the specific trade-offs that I might have to make. So I created some simple test cases to find out more. I used Raku version 2020.09 built on MoarVM version 2020.09, the most recent one when I ran the tests, but the results should be quite similar for slightly earlier and later versions. I test the performance using a series of small test benches with different cases, controlled by a command line argument, using the time command to obtain the wall clock time, and taking the average over 5 runs. For example, There is more than one way to do it, but only one will be the fastest Parsing involves taking strings and turning them into other data structures, so there are many decisions to be made about the data structures and the ways to turn strings into them and manipulate them. Here are some results of performance comparisons that influenced design decisions for the compiler. I was curious to see if they would turn out different in Raku. Fortran code essentially consists of a list of statements which can contain expressions, and in my compiler the statement parser labels each of the statements once using a hashmap. Every parsed line of code is stored as a pair of the original string $src_line with this hashmap, called $info: The labels and values stored in $info depend on the type of statement. It is not a priori clear if matching a pattern in $src_line using a regex is faster or slower than looking up the corresponding label in $info. So I tested the performance of hash key testing versus regexp matching, using some genuine FORTRAN 77 code, a READ I/O call, labelled in $info as ReadCall: Without the if-condition in its body (CASE==3), the for 1..NITERS loop takes 3 s on my laptop. The loop with with the hash key existence test takes 5 s; the regexp match condition takes 53 s. So the actual condition evaluation takes 2 s for hash key existence check and 50 s for regexp match. So testing hash keys is 25 times faster than simple regexp matching. So we trade some memory for computation: we identify the statement once using a regexp, an store the identifying label in $info for subsequent passes. Result: Testing hash keys is 25 times faster than simple regexp matching. So we trade some memory for computation: we identify the statement once using a regexp, an store the identifying label in $info for subsequent passes. The choice of the data structure for the parsed expression matters. As we need a tree-like ordered data structure, it would have to either an object or a list-like data structure. But objects in are slow, so I use a nested array. This data structure is fine if you don't need to do a lot of work on it. However, because every node is labelled with a string, testing against the node type is a string comparison. Simply testing against a constant string or integer is not good enough as the compiler might optimise this away. So I tested this as follows to make sure $str and $c get a new value on every iteration: I populate the string or integer based on the loop iterator and then perform a comparison to a constant string or integer. By subtracting the time taken for the assignment (cases 3 and 4) I obtain the actual time for the comparison. On my laptop, the version with string comparison takes 2 s net, the integer comparison 0.3 s. So doing string comparisons is at least 5 times slower than doing integer comparisons. Therefore my data structure uses integer labels. Also, I label the constants so that I can have different labels for string, integer and real constants, and because in this way all nodes are arrays. This avoids having to test if a node is an array or a scalar, which is a slow operation. So the example becomes : Less readable, but faster and easier to extend. In what follows, what I call the parse tree is this data structure. Result: String comparisons is at least 5 times slower than doing integer comparisons. I tested the cost of using higher-order functions for parse tree traversal (recursive descent). Basically, this is the choice between a generic traversal using a higher-order function which takes an arbitrary function that operates on the parse tree nodes: or a custom traversal: For the case of the parse tree data structures in my compiler, the higher-order implementation takes more than twice as long as the custom traversal, so for performance this is not a good choice. Therefore I don't use higher-order functions in the parser, but I do use them in the later refactoring passes. Result: Higher-order implementations of recursive descent take more than twice as long as custom traversals. The internal representation of a Fortran program in my compiler is an list of [ $src_line, $info ] pairs and the $info hash stores the parse tree as a nested array. So iterating through lists and arrays is a major factor in the performance. Raku has several ways to iterate through a list-like data structure. I tested six of them, as follows: The fastest way is to use list comprehension (case 5, 3.5 s), very closely followed by the suffix-style for (case 4, 3.7 s). The C-style loop construct (case 3) is the slowest (11 s). The map version performs the same as the index-based for loop (both 6.2 s). It is a bit odd that the list-based for loop, probably the most common loop construct, is slower than these two (7.9 s). Result: List comprehensions are fastest, almost twice as fast as for-loops or maps. C-style loop is very slow. Finally, we have to decide how to parse the expression string. The traditional way to build an expression parser is using a Finite State Machine, consuming one character at a time (if needed with one or more characters look-ahead) and keeping track of the identified portion of the string. This is very fast in a language such as C but in Raku I was not too sure, because in Raku a character is actually a string of length one, so every test against a character is a string comparison. On the other hand, Raku has a sophisticated regular expression engine. Yet another way is to turn the string into an array, and parse using list operations. Many possibilities to be tested: For the list-based version, the overhead is 1.6 s; for the string-based versions, 0.8s. The results are rather striking. Clearly the regexp version is by far the slowest. This was a surprise because in my Perl implementation, the regexp version was twice as fast as next best choice. From the other implementations, the string-based FSM which uses the index and substr methods is by far the fastest, without the overhead it takes 1.9s s, which is more that 50 times faster than the regexp version. The map based version comes second but is nearly twice as slow. What is surprising, and actually a bit disappointing, is that the reduce based version, which works the same as the map based one but works on immutable data, is also very slow, 64 s. In any case, the choice is clear. It is possible to make the fastest version marginally faster (1.6 s instead of 1.9 s) by not reducing the string but instead moving the index through the string. However, for the full parser I want to have the convenience of the trim-leading and starts-with methods, so I choose to consume the string. Result: Using index and substr methods is much faster than using regexps. A faster expression parser With the choices of string parsing and data structure made, I focused on the structure of the overall algorithm. The basic approach is to loop trough a number of states and in every state perform a specific action. In the Perl version this was very simple because we use regular expressions to identify tokens, so most of the state transitions are implicit. I wanted to keep this structure so I emulate the regexp s/// operation with comparisons, indexing and substring operations. The matching rules and operations are very simple (I use <pattern> and <integer> as placeholders for the actual values). Here is the Perl version for reference: prefix operations: prefix operations: terms: terms: operators: operators: In the Raku version I used the given/when construct, which is as fast as an if statement but a bit neater. prefix operations: prefix operations: terms: terms: operators: operators: One of the more complex patterns to match is the case of an identifier followed by an opening parenthesis with optional whitespace. Using regular expressions this pattern would be: Without regular expressions, we first check for a character between 'a' and 'z' using 'a' le .substr(0,1).lc le 'z'. If that matches, we remove it from $str and add it to $var. Then we go in a while loop for as long as there are characters that are alphanumeric or '_'. Then we strip any whitespace and test for '('. Another complex pattern is that for a floating point number. In Fortran, the pattern is more complicated because the sub-pattern .e can be part of a floating-point constant but could also be the part of the equality operator .eq.. Furthermore, the separator between the mantissa and the exponent can be not just e but also d or q. So the regular expression is rather involved: Without regular expression, the implementation is as follows. We first detect a character between 0 and 9 or a dot. Then we try to match the mantissa, separator, sign and exponent. The latter three are optional; if they are not present and the mantissa does not contain a dot, we have matched an integer. A final example of how to handle patterns is the case of whitespace in comparison and logical operators. Fortran has operators of the form <dot word dot>, for example .lt. and .xor.. But annoyingly, it allows whitespace between the dot and the word, e.g. . not .. Using regular expressions, this is of course easy to handle, for example: I check for a pattern starting with a dot and which contains a space before the next dot. Then I remove all spaces from that substring using trans and replace this original string with this trimmed version. Conclusion Overall the optimised expression parser in Raku is still very close to the Perl version. The key difference is that the Raku version does not use regular expressions. With the above examples I wanted to illustrate how it is possible to write code with the same functionality as a regular expression s/// operation, using some of Raku's built-in string operations: substr : substring index : location a a substring in a string trim-leading : strip leading whitespace starts-with ends-with trans : used to remove whitespace using the ' ' => '' pattern lc : used in range tests instead of testing against both upper and lower case le, lt, ge, gt: for very handy range comparisons, e.g. 'a' le $str le 'z' substr : substring index : location a a substring in a string trim-leading : strip leading whitespace starts-with ends-with trans : used to remove whitespace using the ' ' => '' pattern lc : used in range tests instead of testing against both upper and lower case le, lt, ge, gt: for very handy range comparisons, e.g. 'a' le $str le 'z' The resulting code is of course much longer but arguably more readable than regular expressions, and currently four times faster. I ran a lot more tests, and compared performance against Perl and Python as well, but that is another story. All code for the tests is available in my GitHub repo. Updated December 02, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/writing-faster-raku/index.html"}, {"title":"List-based parser combinators in Haskell and Raku","text":" List-based parser combinators in Haskell and Raku This is a follow-on from my article introducing algebraic data types and explaining how to implement them in Raku. If you are not familiar with algebraic data types, I suggest you read that article first. In this article I use algebraic data types to create a statically typed version of a list-based parser combinators library which I originally created for dynamic languages. The article introduces list-based parser combinators are and how to implement them in Raku and Haskell using algebraic data types. Perl, Haskell and Raku: a quick introduction The code examples in this article are written in Perl, Haskell and Raku. If you are familiar with these languages, you can skip this section. The code is written in a functional style and is not very idiomatic so you should be able to understand it easily if you know another programming language. Perl and Raku are syntactically similar to C/C++, Java and JavaScript: block-based, with statements separated by semicolons, blocks demarcated by braces, and argument lists in parentheses and separated by commas. The main feature that sets Perl and Raku apart is the use of sigils ('funny characters') which identify the type of a variable: $ for a scalar, @ for an array, % for a hash (map) and & for a subroutine. Variables also have keywords to identify their scope, I will only use my which marks the variable as lexically scoped. A subroutine is declared with the sub keyword, and subroutines can be named or anonymous: Haskell is whitespace-sensitive like Python, but has a markedly different syntax. Because everything is a function, there is no keyword to mark a function; because there is only lexical scope, there is no need for any special scope identifiers. Function arguments are separated by spaces; anonymous functions have a special syntax to identify them: Several of the examples use the let ... in ... construct, which behaves as a lexically scoped block: x0, x1 and x2 are in scope only in the expression after the in keyword. Haskell is statically typed, and the type of a function or variable is written in a separate annotation, for example: The isEmpty function has a type signature, identified by ::, that reads \"isEmpty is a function from a list of anything to a Boolean\". Types must be written with an initial capital. The a is a type variable which can take on any type, as explained in my previous post. Raku has optional typing: you can add type information as part of the declarations, for example: This function takes array of any type and an integer and returns a Boolean. Other specific bits of syntax or functionality will be explained for the particular examples. Parsers and parser combinators What I call a parser here is technically a combination of a lexer or tokeniser and a parser. The lexical analysis (splitting a sequence of characters into a sequence of tokens, strings with an identified meaning) meaning and parsing (syntactic analysis, analysing the sequence of tokens in terms of a formal grammar) are not separate stages. Parser combinators are building blocks to create parsers by combining small parsers into very complex ones. In Haskell they became popular because of the Parsec library. This library provides monadic parser combinators. My parser combinator library implements a subset of Parsec's functionality. I am not going to explain what monads are because the point of creating list-based parser combinators is precisely that they do not require monads. There is a connection however, and you can read about it in my paper if you're interested. I created the original version of the list-based parser combinators library for dynamically typed languages: there are versions in Perl, Python and LiveScript, a Haskell-like language which compiles to JavaScript. Because I like Raku and it has gradual typing, I was interested in what a statically typed Raku version would look like. As a little detour I first implemented them in Haskell, just for fun really. Raku has Grammars, which also let you build powerful parsers. If you are familiar with them it will be interesting to compare the parser combinator approach to the inheritance mechanism used to compose Grammars. List-based parser combinators So what are list-based parser combinators? Let's say I want to parse a string containing this very simple bit of code: We have an identifier, an assignment operator and a natural number, maybe preceded by whitespace, and with some whitespace that doesn't matter between these tokens. I am assuming that the string which we want to parse is code written in a language which is whitespace-insensitive. What I would like is that I can write a parser for as close as possible to the following: And when I apply assignParser to assignStr, it should return the parsed tokens, a status, and the remainder of the string, if any. So each parser takes a string and returns this triplet of values (I'll call it a tuple instead of a triplet). We'll define this more formally in the next sections. What we have here is that the list acts as the sequencing combinator for the specific parsers. The maybe is a combinator to make the token optional. We can provide more combinators, such as choice, (to try several parsers), many (to repeatedly apply a parser), etc. And because every parser is a function, complex parsers can easily be composed of smaller parsers expressed in terms of the building blocks. For example, if we had many assignments, we could have something like Now suppose that we want to extend our parser to include declarations, something like which we parse as then we need to add get the following parser: It is also essential that we can label tokens or groups of tokens, so that we can easily extract the relevant information from a parse tree, as the intermediate step in transforming the parse tree into an abstract syntax tree. In the above example, we are only interested in the variable name and the value. The whitespace and equal sign are not important. So we could label the relevant tokens, for example: Implementation in a dynamically typed language How do we implement the above mechanism in a dynamically typed language? A parser like identifier is simply a function which takes a string and returns a tuple. In Perl, the code looks like this: (In Perl, =~ /.../ is the regular expression matching syntax, and s/.../.../ is regular expression substitution.) But what about a parser like symbol? It takes the string representing the symbol as an argument, so in the example, symbol( \"=\" ) should be the actual parser. What we need is that a call to symbol will return a function to do the parsing, like this: This is of course not limited to string arguments, any argument of the outer function can be used inside the inner function. In particular, if a parser combinator takes parsers as arguments, like choice and maybe, then these parsers can be passed on to the inner function. This is fine as far as it goes, but what about the labelled parsers? and what about the lists of parsers? Neither of these can be directly applied to a string, but neither is a function that can generate a function either. So to apply them to a string, we will need to get the parsers out of label-parser pair and the list. We do that using a helper function which I call apply, and which in Perl looks like (The syntax $p->($str) applies the anonymous function referenced by $p to its arguments.) This function checks the type of $p using the ref built-in: it can either be code (i.e. a subroutine), an array or a hash. If it's subroutine it's applied directly to the string, otherwise it calls sequence or retag. The foldl function is my Perl version of the left-to-right reduction in Haskell or reduce in Raku. What retag does is taking the parser from the label pair (which is a single-element hash), apply it to the string, and label the resulting matches with the label of the parser: (The syntax %{$p} is dereferencing, a bit like the * prefix in C.) Here is a simple example of how to use the list-based parser combinators. Implementation in a language with algebraic data types All of the above is fine in a dynamically typed language, but in a statically typed language, the list can't contain a function and a hash and another list, as they all have different types. Also, only testing if an entry of the list is code, hash or list is rather weak, as it does not guarantee that the code is an actual parser. So let's see what it looks like in Haskell and Raku. list-based parser combinators In Haskell, we start (of course) by defining a few types: (In Haskell, data and newtype define a new algebraic datatype; type defines an alias for an existing type.) So our list of parsers will be a list of LComb, and this can be a parser, sequence of parsers or tagged pair. Because the tag eventually is used to label the matched string, the Match type also has a tagged variant. In principle, the return type of the parser could just be a tuple, but I define the MTup polymorphic type so I can make it an instance of a type class later on, e.g. to make it a monad. With these types we can define our parser combinators and the apply and sequence functions. Here is the symbol parser. Many of the parsers in the library are implemented using Perl-Compatible Regular Expressions (Text.Regex.PCRE), because what else can you expect of a lamdacamel. (The $ behaves like an opening parenthesis that does not need a closing parenthesis; ++ is the list concatenation operator, in Haskell strings are lists of characters.) The apply function pattern matches against the type alternatives for LComb. Because of the pattern matching there is not need for an untag function. It is clear from this implementation that we can write a sequence of parsers both as Seq [...] or sequence [...]. Apart from the static typing, the sequence function is very close to the Perl version. That is of course because I wrote the Perl code in a functional style. In Raku, I use roles as algebraic datatypes as explained in my previous post. Essentially, each alternative of a sum types mixes in an empty role which is only used to name the type; product types are just roles with some attributes that are declared in the role's parameter list. The way the Raku regular expressions are used in this implementation of symbol is closer to the Haskell version than the Perl 5 version. But the main difference with the Perl 5 version is that the combinator and the return tuple are statically typed. The function undef-match is a convenience to return an array with an undefined match. As explained in my previous post, we use Raku's multi subs for pattern matching on the types. In Haskell this is also possible, and the definition of apply can be rewritten as: The Raku version of apply is quite close to this Haskell version. For convenience, I use a function unmtup to unpack the MTup. Finally, the sequence code in Raku. It follows closely the structure of the Perl 5 and Haskell versions. Raku's reduce is equivalent to Haskell's foldl. There is still a minor issue with this definition of sequence: Because of the signature, we can't write Instead, we would have to write which is a bit tedious. So I rename sequence to sequence_ and wrap it in a new function sequence which has a 'slurpy' argument (i.e. it is variadic) like this: I do the same for all combinators that take a list of combinators as argument. If you wanted to type check the arguments of the wrapper function, you could do this with a where clause: The type constructor based tagging (Tag label parser) is nice in Haskell but in Raku it would look like Tag[label, parser].new which I don't like. Therefore, I wrap the constructor in a tag function so I can write tag(label, parser). An example of typical usage As an example, we can construct the following parser for a part of a Fortran 90-style variable declaration, apply it to the given string and get the parse tree: (In Raku, variables declared with a \\ are sigil-less ) For reference, here is the Haskell code: (If you wonder about the strange signature of main, the do keyword or the let without an in, the answers are here. Or you could take my free online course.) As is clear from this example, in both languages, list-based parser combinators provide a clean and highly composable way of constructing powerful and complex parsers. It is also quite easy to extend the library with additional parsers. I think this is a nice practical application of algebraic data types in particular and functional programming in general. You can find both the Haskell code and the Raku code in my repo. Updated June 22, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/list-based-parser-combinators/index.html"}, {"title":"Cleaner code with functional programming","text":" Cleaner code with functional programming Functional programming is a style of programming and modern languages support this style to a greater or lesser extent. In this article I want to explain how programming in a functional style provides you with powerful abstractions to make your code cleaner. I will illustrate this with examples in Raku and Python, which as we will see are both excellent languages for functional programming. Raku: a quick introduction The code examples in this article are written in Python and Raku. I assume most people are familiar with Python, but Raku is less well known, so I will explain the basics first. The code in this article is not very idiomatic so you should be able to understand it easily if you know another programming language. Raku is most similar to Perl. Both languages are syntactically similar to C/C++, Java and JavaScript: block-based, with statements separated by semicolons, blocks demarcated by braces, and argument lists in parentheses and separated by commas. The main feature that sets Perl and Raku apart from other languages is the use of sigils ('funny characters') which identify the type of a variable: $ for a scalar, @ for an array, % for a hash (map) and & for a subroutine. Variables also have keywords to identify their scope, I will only use my which marks the variable as lexically scoped. A subroutine is declared with the sub keyword, and subroutines can be named or anonymous: In Python this would be: Raku supports sigil-less variables, and uses the \\ syntax to declare them. For more on the difference between ordinary and sigil-less variables, see the Raku documentation. For example (say prints its argument followed by a newline): In the code in this article, I will use the sigil-less variables whenever possible. Raku has several types of sequence data structures. In the code below I will use lists and arrays and ranges. The main difference between a list and an array in Raku is that a list is immutable, which means that once created, it can't be modified. So it is a read-only data structure. To 'update' an immutable data structure, you need to create an updated copy. Arrays on the other hand are mutable, so we can update their elements, extend them, shrink them etc. All updates happen in place on the original. Raku's arrays are similar to Python's lists and Raku's lists are similar to Python's tuples, which are also immutable. Apart from the syntax, ranges in Raku are similar to ranges in Python, and both are immutable. The equivalent Python code would be Other specific bits of syntax or functionality will be explained for the particular examples. A function, by any other name — functions as values Functions are the essence of functional programming. As I explained in my article \"Everything is a function\", in a proper functional language, all constructs are built from functions. All modern programming languages have a notion of functions, procedures, subroutines or methods. They are an essential mechanism for code reuse. Typically, we think of a function as something that operates on some input values to produce one or more output values. The input values can be globally declared, attributes of a class or passed as arguments to the function. Similarly, the output values can be returned directly, to global variables, as class attributes or by modifying the input values. To benefit most from functional programming, it is best if functions are pure, which means that a call to the function always produces the same output for the same inputs. In practice, this is easier to achieve if the function only takes inputs as arguments and returns the output directly, but this is not essential. The crucial feature of functional programming is that the input and output values of a function can themselves be functions. So functions must be values in your language. Sometimes this is called \"functions must be first-class\", and a function that takes and/or returns a function is sometimes called a \"higher-order function\". If functions are values, it follows that we can assign them to variables. In particular we will assign them to the arguments of other functions. But we can also assign them to ordinary variables. Let's consider the following function, choose, which takes three arguments t, f and c. First let's call choose with strings as values for the first two arguments: Now let's try with functions as arguments: So our function choose took two functions as its first two arguments, and returned a function. In Raku we need the & sigil on the function names because otherwise they would be evaluated: a bare function name like tt is the same as calling the function without arguments, tt(). By assigning this function to a variable (res_f), we can now call res_f as a function and it will eventually call tt or ff depending of the choice. Functions don't need a name Now, if we can assign functions to variables, they don't really need a name themselves. So our functions can be anonymous. Most languages support anonymous functions. In functional languages they are usually called \"lambda functions\". In Raku, we have two ways to create anonymous functions: Using the sub (...) syntax: Or using the 'pointy block' syntax, which is a little bit more compact: Python uses the lambda keyword: So now we can say When we print out the variable to which the function is bound, Raku returns sub { } to indicate that the variable contains a function. In Python: Examples: map, grep and reduce Functions of functions have many uses, and I just want to highlight three examples that are available readily in Raku: map, reduce and grep. Python has map and filter, and provides reduce via the functools module. What these functions have in common is that they offer an alternative to for-loops over lists. map takes two arguments: a function and a list. It applies the function to all values in the list in order and returns the results, for example to square all values in a list: In Python we need to explicitly create the tuple, but apart from the syntax differences, the structure is quite the same: This is the functional alternative to the more conventional for-loop: Note that in both Raku and Python we need to use a mutable data structure for the for-loop version, whereas the map version uses immutable data structures. grep (called filter in Python) also takes arguments, a function and a list, but it only returns the values from the list for which the function returns true: We can of course write this using a for-loop and an if-statement, but that again requires a mutable data structure: What's nice about map and grep is that you can easily chain them together: This is because map and grep take a list and return a list, so as long as you need to operate on a list, you can do this by chaining the calls. reduce also takes a function and a list, but it uses the function to combine all elements of the list into a single result. So the function must take two arguments. The second argument is the element taken from the list, and the first argument is used as a state variable to combine all elements. For example, calculating the sum of a list of numbers: What happens here is that acc is first set to the first element of the list (1), and then the second element is added to it, so acc becomes 1+2=3; then the third element (3) is added to this, and so on. The effect is to consecutively sum all the numbers in list. To make this more clear, let's write our own version of reduce. In many functional languages, a distinction is made between a left-to-right (starting at the lowest index) and right-to-left (starting at the highest index) reduction. This matters because depending on the function doing the reducing, the result can be different if the list is consumed from the left or from the right. For example, suppose our reducing function is then it does not matter which direction we traverse the list. But consider the following function: ( ... ?? ... !! ... is the Raku syntax for the conditional operator which is ... ? ... : ... in most other languages and ... if ... else ... in Python) In this case the result will be different if the list is reduced from the left or from the right. In Raku and Python, reduce is a left-to-right reduction. Also, instead of using the first element of the list, the reduction function can take an additional argument, usually called the accumulator. In functional languages, reduce is usually called fold, so we can have a left fold and a right fold. Let's have a look how we could implement these. A straightforward way to implement a left fold (so the same as reduce) is to use a for-loop inside the function. That means we have to update the value of the accumulator on every iteration of the loop. In Raku, sigil-less variables are immutable (I am simplifying here, see the Raku documentation for the full story) so we need to use a sigiled variable, $acc. If we want to use immutable variables only, we can use recursion. Raku makes this easy because it allows multiple signatures for a subroutine (multi subs), and it will call the variant that matches the signature. In Python, there is the module multipledispatch that lets you do something similar to multi subs. Our foldl will consume the input list lst and use f combine its elements into the accumulator acc. When the list has been consumed, the computation is finished and we can return acc as the result. So our first variant says that if the input list is empty, we should return acc. The second variant takes an element elt from the list (see the Raku documentation for details on the *) and combines it with acc into f(acc,elt). It then calls foldl again with this new accumulator and the remainder of the list, rest. Python does not allow pattern matching of this kind so we need to write the recursion using a conditional: In this implementation, none of the variables is ever updated. So all variables can be immutable. The right fold is quite similar to the left fold. For the loop-based version, all we do is reverse the list. In the recursive version, we take the last element from the list instead of the first one. For details on the ..^ * - 1 syntax please see the Raku documentation. Now, what about map and grep? We can of course implement these with for-loops, but we can also implement them using our foldl: Because the function f is mappable, it only has a single argument. But foldl needs a function with two arguments, the first for the accumulator. So we call foldl with an anonymous function of two arguments. The accumulator itself is an empty list. Although we said earlier that a reduction combines all elements of the original list into a single return value, this return value can of course be any data type, so also a list. So we call f on every element of the original list and add it to the end of the accumulator list. (The | flattens the list, so (|acc,f(elt)) is a new list built from the elements of acc and result of f(elt).) In a similar way we can also define grep: Just like in the map implementation, we call foldl with an anonymous function. In this function we test if f(elt) is true for every elt in lst. If it is true we create a new list from acc and elt, otherwise we just return acc. Because map and grep operate on each element of the list separately, we could implement them using the right fold as well. With these examples I hope that both the concept of a function working on functions and the possible ways of implementing them has become more clear. The advantage of the recursive implementation is that it allows us to use immutable data structures. You may wonder why I focus on these immutable data structures. As we will have seen, functional programming works really well with immutable data structures. And they have one big advantage: you never have to worry if you have accidentally modified your data, or whether you should make a copy to be sure. So using immutable data structures make code less error-prone and easier to debug. They also have potential performance benefits. And as we'll see next, in Raku there is yet another advantage. Functions returning functions Functions can also return functions. This is in particular useful if we want to have a parametrisable function. As a trivial example, suppose we want a series of functions that increments a number with a fixed value: add1, add2 etc. We could of course write each of them separately: Or we could use a list filled with anonymous functions: We could do better and use a loop to fill an array with anonymous functions: We create a new anonymous function with every loop iteration, and add it to the array. But instead, we could use a function to create these anonymous functions, and then we could use map instead of a loop, and use an immutable data structure: In Raku, using a range has an additional benefit: we can set the end of the range to infinity, which in Raku can be written as ∞ (unicode 221E), * or Inf. This is an example of what is called \"lazy evaluation\", or laziness for short: Raku is not going to try (and fail) to process this infinite list. Instead, it will do the processing when we actually use an element of that list. The evaluation of the expression is delayed until the result is needed, so when we call add[244], what happens is that gen_add(244) is called to generate that function. Note that this will not work with the for-loop, because to use the for-loop we need a mutable data structure, and the lazy lists have to be immutable. So this is a nice example of how the functional programming style allows you to benefit from laziness. For the full story of laziness in Raku, please see the documentation. Python does not have lazy lists but is have a different form of laziness: the call to map (or filter) does not return the sequence of results but instead it returns a generator: It is only when we wrap the generator in a sequence constructor such as tuple() that the results are actually generated. Function composition We saw above that you can chain calls to map and grep together. Often you only need to chain map calls together, for example In that case, we can do this a little bit more efficient: rather than creating a list and then calling map on that list, we can do both computations at once by composing the functions. Raku provides a special operator for this: The operator ∘ (the \"ring operator\", unicode 2218, but you can also use a plain o) is the function composition operator, and it's pronounced \"after\", so f ∘ g is \"f after g\". What it does is create a new function by combining two existing functions: is the same as The advantage of the composition operator is that that it works for any function, including anonymous ones. But in fact, it is just another higher-order functions. It is simply the operator form of the following function: Python does not have a function composition operator, but you can easily have compose in Python too: Conclusion In this article I have used Raku and Python examples to introduce three key functional programming techniques: functions that operate on functions, functions that return functions and function composition. I have shown how you to use the functions map, reduce (fold) and grep (filter) to operate on immutable lists. I have explained how yo(u can implement such functions with and without recursion, and what the advantage is of the recursive implementation. Here is the code from the article, Raku and Python. There is of course a lot more to functional programming and I have written a few articles on more advanced topics. The concepts introduced in this article should provide a good basis for understanding those more advanced topics. If you want to learn more about functional programming, you might consider my free online course. Updated July 18, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2021 Jekyll theme Skinny Bones","tags":"","url":"articles/decluttering-with-functional-programming/index.html"}, ]};