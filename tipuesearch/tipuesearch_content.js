var tipuesearch = {"pages": [ {"title":"About","text":" About I am a Reader in Computing Science at the University of Glasgow. My research interests are compilers and runtime systems for heterogeneous architectures. I am particularly interested in FPGAs and acceleration of climate and weather simulations. My homepage at University of Glasgow Why I do Computing Science research My homepage at University of Glasgow Why I do Computing Science research Updated August 15, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2020 Jekyll theme Skinny Bones","tags":"","url":"about/index.html"}, {"title":"Imagine we had better weather forecasts","text":" Imagine we had better weather forecasts The last few days have shown that current weather forecasts are at the same time very advanced and yet not good enough. We had little warning of the very large amount of snow that would cripple our infrastructure. Earlier and more accurate warnings could help to limit the damage of such events (estimated at £470m a day just from the travel disruption). Also, better long-range predictions about the probabilities of such events in the future could help with investment and planning of infrastructure and services: should councils invest in more snow ploughs; should rail operators invest in making the network more resilient to extreme cold weather; how can the emergency services be kept running in such extreme conditions, etc.? So why are our forecasts not better? One of the main reasons is that the resolution of the weather forecasting computer models is at the moment still quite coarse. For example the MetOffice forecasting model, which is considered amongst the best in the world, divides the UK in squares of 1.5 km at its highest resolution, i.e. the simulation produces a single averaged value anywhere within this 1.5 km x 1.5 km area. The time resolution for the shortest-term forecast is 50 seconds. In contrast, for accurate simulation of local weather, a resolution of hundred metres and a time step of about a second are required. This would require a supercomputer a thousand times more powerful than the one currently in use by the MetOffice. A key problem with supercomputers is that they consume a lot of power. The current MetOffice supercomputer consumes 2.7 MW of electricity. A supercomputer a thousand times more powerful would need 2.7GW which is more than twice as much as all the electricity produced by the UK's largest nuclear power station, Hinkley Point B. To reduce the power consumption, new supercomputers have started using special hardware called accelerators. Already, both the fastest and most power-efficient supercomputers in the world use this technology. Unfortunately writing programs for such an accelerator-based supercomputer is very complicated. And existing programs can't benefit from accelerators without major changes. Weather forecasting models are very large and complex, with around a million lines of code. Rewriting such a program is extremely difficult and time consuming. So what's hindering progress? The holy grail is to develop a software technology that can automatically change legacy programs to make them suitable to the new, accelerator-based supercomputers. Many research groups, including my own, are working on such approaches. There is however a huge gap between a research proof-of-concept and a ready-for-use product and it takes considerable investment to bridge this gap. Unfortunately, a funding gap exists in this area: on the one hand, creating a product from a research proof-of-concept is not core research and can therefore not be funded by the Research Councils or through EU research funding. On the other hand, as there is no perceived commercial value in such a product (because the potential marked is very small), commercial funding is not an option. Imagine So imagine that the UK took a more joined-up view with investment to speed up the adoption of research. In the case of weather forecasting, this would help to minimize the impact on people and the economy of severe weather events like we experienced recently. It would be a thousandfold return on the investment. Updated March 04, 2018 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2020 Jekyll theme Skinny Bones","tags":"","url":"articles/better-forecasts/index.html"}, {"title":"The Winds of Kyoto","text":" The Winds of Kyoto I wrote a very short story for the Fifth Annual Kyoto Writing Competition. At the Disaster Prevention Research Institute of Kyoto University, researchers are modelling the flow of the wind over the city. The wind that blows over the tiny machiyas of Gion and around Kyoto station with its tall hotels and the Tower. In spring, it scatters a storm of sakura petals all over the city. In the rainy season, it blows down from Arashiyama along the waters of the Katsuragawa, and carries streams of clouds over the hills. When the temple ponds are full of lotus flowers, it stirs the lanterns and chases the incense smoke, and cools the faces of the teams that pull the Yamaboko along Shijou Doori. At the end of summer, when the higanbana is blooming, that same wind follows the railway line to Uji and blows in through the open window of the researchers’ offices, rifling the stacks of diagrams that reveal its very flow. But sometimes that flow gathers to a tremendous strength, and then a typhoon will lacerate the city. The models of the wind predicts this, to make sure the people of Kyoto are prepared. Then Kyoto hunkers down, and waits. When the storm has passed, the sky is a transparent clear and blue. The wind of Kyoto is once again cool and mild, blowing gently over the city. [1] Yoshida, T., Takemi, T. & Horiguchi, M. Large-Eddy-Simulation Study of the Effects of Building-Height Variability on Turbulent Flows over an Actual Urban Area. Boundary-Layer Meteorol 168, 127–153 (2018). Updated March 28, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2020 Jekyll theme Skinny Bones","tags":"","url":"articles/the-winds-of-kyoto/index.html"}, {"title":"Musings of an Accidental Computing Scientist","text":" Musings of an Accidental Computing Scientist August 07, 2020 Function types A brief introduction into function types, with a way to implement them in Raku and examples in many languages. July 18, 2020 Cleaner code with functional programming An introduction to some powerful functional programming techniques in Raku and Python. July 03, 2020 Everything is a function Although it might seem that a language like Haskell has a lot of different objects and constructs, they can all be reduced to functions. June 22, 2020 List-based parser combinators in Haskell and Raku This is a follow-on of my article on algebraic data types, with list-based parser combinators as a practical application. June 05, 2020 Roles as Algebraic Data Types in Raku Algebraic data types are great for building complex data structures, and easy to implement in Raku using roles. March 28, 2020 The Winds of Kyoto A very short story for the Fifth Annual Kyoto Writing Competition. April 27, 2019 Writing faster Perl code Sometimes your pure Perl code needs to be as fast as possible. What does it take? October 21, 2018 Why Europe needs hurricane contingency planning Hurricanes will become common in Europe in the next few decades. I discuss the evidence, the impact and the need for proper contingency planning. April 19, 2018 Hacking the Pleroma: Elixir, Phoenix and a bit of ActivityPub A brief guide into hacking Pleroma, a federated microblogging server software. March 13, 2018 A little maths puzzle in two parts Of which the outcome is a way to construct a regular pentagon using compass and ruler. March 05, 2018 A strategy for articles/debugging For a long time it has been my contention that for a developer, more that programming, debugging should be treated as a core skill. March 04, 2018 Imagine we had better weather forecasts Current weather forecasts are at the same time very advanced and yet not good enough. Earlier and more accurate warnings could help to limit the damage of su... Wim Vanderbauwhede About Wim Vanderbauwhede About © 2020 Jekyll theme Skinny Bones","tags":"","url":"articles/index.html"}, {"title":"A little maths puzzle in two parts","text":" A little maths puzzle in two parts I like solving little maths puzzles, deriving known results using nothing more complicated than secondary school level trigonometry, algebra and maybe a little calculus. The one below is actually two puzzles, but both have to do with regular polygons. Part 1 Approximating π by the perimeter of a regular polygon Can one approximate $\\pi$ by the perimeter of regular polygon with increasing numbers of sides inscribed in a unit circle, so starting with a triangle, then a hexagon, a dodecagon,... ? You may say that is obvious: the more sides, the closer the polygon approximates a circle. But I still wanted to work out the proof. From Figure 2 we can write down some straightforward relationships between the length of a side of a pentagon (b from the previous part) and the angle of the arc, $\\frac{2\\pi}{5}$. $$\\begin{equation} x=\\frac{b}{2}=sin\\frac{\\pi}{5}\\label{eq:2.1} \\end{equation}$$ $$\\begin{equation} y=cos\\frac{\\pi}{5}\\label{eq:2.2} \\end{equation}$$ $$\\begin{equation} x^{2}+y^{2}=1\\label{eq:2.3} \\end{equation}$$ $$\\begin{equation} q = 2sin\\frac{2\\pi}{5}=4sin\\frac{\\pi}{5}cos\\frac{\\pi}{5}\\label{eq:2.4}\\end{equation}$$ Substitution of Eqs. $\\ref{eq:2.1}$ and $\\ref{eq:2.2}$ gives $$\\begin{equation} q = 4xy\\label{eq:2.5}\\end{equation}$$ Now we consider the right triangle with hypothenuse q: $$\\begin{equation} q^{2} = x^{2}+(1+y)^{2}\\label{eq:2.6}\\end{equation}$$ Substitution of Eq. $\\ref{eq:2.3}$ in the RHS of Eq. $\\ref{eq:2.6}$ and refactoring gives: $$\\begin{equation} q^{2} = 2+2y\\label{eq:2.7}\\end{equation}$$ Substitution of Eq. $\\ref{eq:2.3}$ and Eq. $\\ref{eq:2.5}$ in the LHS of Eq. $\\ref{eq:2.7}$ and refactoring gives: $$\\begin{equation} (4-(2y)^{2})(2y)^{2} = 2+2y\\label{eq:2.8}\\end{equation}$$ Now we define $z=2y$ and rewrite Eq. $\\ref{eq:2.8}$ as: $$\\begin{equation} (4-z^{2}).z^{2}=2+z\\label{eq:2.9} \\end{equation}$$ Which after more refactoring finally gives $$\\begin{equation} z^{2}(2-z)-1=0\\label{eq:2.10} \\end{equation}$$ This is a third-order equation but fortunately there is an obvious root for $z=1$. After some factorization we obtain the remaining second-order equation: $$\\begin{equation} z^{2}-z-1=0\\label{eq:2.11} \\end{equation}$$ The roots of this equation are: $$\\begin{align} z & = & \\frac{1\\pm\\sqrt{(-1)^{2}-4.1.(-1)}}{2.1}\\label{eq:2.12}\\ & = & \\frac{1\\pm\\sqrt{5}}{2}\ onumber \\end{align}$$ This is actually a very famous equation and its positive root is known as the Golden ratio. $$\\begin{align} \\phi & = & \\frac{1}{a} & = & \\frac{a}{1-a}\\label{eq:goldenratio} \\end{align}$$ Clearly y as defined is positive so $y=\\frac{z}{2}=\\frac{\\phi}{2}$ or $$\\begin{equation} y=\\frac{1+\\sqrt{5}}{4}\\label{eq:2.13} \\end{equation}$$ From Eq. $\\ref{eq:2.13}$ we can express b in terms of y using Eqns $\\ref{eq:2.1}$, $\\ref{eq:2.2}$ and $\\ref{eq:2.3}$: $$\\begin{equation} b=2\\sqrt{1-y^{2}}\\label{eq:2.14} \\end{equation}$$ And so we obtain the expression for the length of the side of a pentagon as $$\\begin{equation} b=\\sqrt{\\frac{5-\\sqrt{5}}{2}}\\label{eq:2.15} \\end{equation}$$ The remaining question is then, how do we construct a line of length b using a rules and compass? We do this indirectly, by constructing a line of length y as shown in Figure 3. First, we construct a line of length 1/2. Then the hypothenuse of the right triangle with sides 1/2 and 1 has a lenght of $\\frac{\\sqrt{5}}{2}$. We add this to the 1/2 by drawing an arc of radius $\\frac{\\sqrt{5}}{2}$ using the compass. This way we get a line of length $\\phi = \\frac{1+\\sqrt{5}}{2}$. Dividing this into two gives y and through the way we constructed this, we immediately get b as well and so we can construct the pentagon using arcs of radius b. The equation we solved to obtain y (Eq. $\\ref{eq:2.10}$) is a third order equation, and its other positive root is $z=1$. This shows in a way the danger of transforming a geometry problem into algebra: only one of these roots, $z=\\phi$, corresponds to a solution of our geometric problem. But there is also a geometric interpretation for the root $z=1$. Substitution of $y=1/2$ in the equations for results in $q=\\sqrt{3}$ and $b=\\sqrt{3}$, in other words a regular triangle inscribed in the unit circle. Updated March 13, 2018 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2020 Jekyll theme Skinny Bones","tags":"","url":"articles/constructing-a-pentagon/index.html"}, {"title":"A strategy for articles/debugging","text":" A strategy for debugging For a long time it has been my contention that for a developer, more than programming, debugging should be treated as a core skill. A developer typically spends more time debugging code than writing code so it makes sense to try and optimize this process. Over the years I have developed a strategy for debugging. I see debugging as a generic, transferable skill that is applicable not only to coding but to any form of systems design. The mental models To debug a system we need a mental model, an understanding of the system in our mind. I believe this is the real cornerstone of debugging, and the common mistake is to spend too little time in constructing these models. The mental model should cover all aspects of the system that you need to understand. For example, if you want to understand why a program is slow, your mental model of the system should allow you to reason about the performance trade-offs. I distinguish three types of mental models, each of them corresponds to a different view of the system. The understanding of what the system does (or should do), how it should behave, is the behavioural model. A bug is observed through this model: the system behaves in a way that does not conform to the behavioural model. Usually (or at least if you're lucky) the behavioural system model is codified in a specification. The operational system model is your understanding of how the system works. This model allows us to formulate hypotheses about why the system does not behave as expected. This is the most important mental system model, and part of the debugging process is actually improving and refining this model. In many cases, the operational model is actually your model of how a program in a given language is compiled/interpreted and executed on the hardware. This model starts from the syntax and semantics of the programming language, and includes a model for any API used in the code. As a trivial example, in Python, the keys in a dictionary are unordered, whereas the default in a C++ map is ordered. The closer you are to the bare metal, or the more you care about performance or memory footprint, the more details your mental model will have to include about the actual hardware, to the extent that for e.g. running code on FPGAs you even need to have a detailed mental model for the memory controllers. For debugging in higher-level languages, usually the model can be much more abstract, with a basic notion of memory management and code execution. The structural system model is the model of where we should look to trace and fix a bug. For software, this model is our understanding of the code structure. In general, the structure of software systems tends to be hierarchical and relatively loosely coupled. This means we only need to focus on a fraction of the codebase at a time. If this were not the case, debugging time would grow more than linearly with the code size. Fortunately for most systems it's closer to logarithmic. The debugging activity Given the above mental system models, the activity of debugging is an iterative process involving several steps, and during the process we often jump between these steps. First, identify the bug. Is it really a bug or is your behavioural model incorrect, not specific enough or ambiguous? If necessary, adapt the model and re-iterate. Then there are essentially three stages in the process of finding the bug. We start by narrowing down through a process of exclusion: \"This bug can't be caused by X because of reason Y\". This process relies mostly on the operational system model, but sometimes also on the structural model, especially if you're not 100% certain: \"it is unlikely that the bug is in module X because of reason Y\". For example, it is unlikely that the cause of the bug is located in a standard library, compiler or interpreter. The chances that the bug is in your own code is much higher, so that possibility should be explored first. Once we cannot proceed any further through exclusion, we switch to the most interesting stage. We formulate a hypothesis \"Let's assume that the bug is caused by X\" and then we use this as the basis for further investigation. The main difference between exclusion and formulating a hypothesis is that when we formulate a hypothesis, we don't know if it is true or false, so we need to test it. With the exclusion process, we do know that our stated reason holds -- or at least we have a high degree of confidence -- so we don't test it. Quite frequently our hypothesis will prove to be false, and then we have one fewer possible cause for the bug. Equally frequently, when our hypothesis proves to be false, this indicates that our operational model is incomplete. In that case we should formulate additional hypotheses to improve our mental model. I believe this is an important step that is often skipped because it seems to detract from the real task, i.e. finding the bug. But without an accurate operational model, it is much harder to find bugs, so the time spent in improving your system knowledge is always well spent. To test a hypothesis we can either use emulation or observation of the system behaviour. This requires the structural model to tell us where to look. By emulation I mean that we mentally run part of a program using our operational model. In that case we assume that our operational model is accurate enough to produce the same result as the actual system. In general, this is a tricky approach to debugging because if our mental model is inaccurate we won't find the bug. However, it is generally the approach taken when we have narrowed down the location of the bug sufficiently. We can observe the system behaviour through compiler or interpreter warnings, by using a debugger, or by making the code generate additional information. This requires a good structural model to guide us to the locations that we want to inspect using the debugger or where we want to add the code to generate the debugging information. Either way, the result should be some information that helps to test the hypothesis. By emulation I mean that we mentally run part of a program using our operational model. In that case we assume that our operational model is accurate enough to produce the same result as the actual system. In general, this is a tricky approach to debugging because if our mental model is inaccurate we won't find the bug. However, it is generally the approach taken when we have narrowed down the location of the bug sufficiently. We can observe the system behaviour through compiler or interpreter warnings, by using a debugger, or by making the code generate additional information. This requires a good structural model to guide us to the locations that we want to inspect using the debugger or where we want to add the code to generate the debugging information. Either way, the result should be some information that helps to test the hypothesis. Example scenarios Debugging code you wrote and understand, and whose use case is intimately familiar to you, should be the easiest type of debugging. However, the problem with this kind of code is often that, precisely because it is your own code and you have a very precise behavioural model, and of course a perfect structural model, you never bothered to create an accurate operational model of the code. This may sound strange because after all, if you wrote it, you should know how it works. But the reality is that we often perform very limited mental verification, esp. of corner cases, on our own code. Debugging someone else's code is much harder because you typically lack all of the mental models. I often have to debug code written by my students, usually long after they have graduated. The main conclusion is that we should teach our students how to write maintainable code, i.e. code that makes it easy to understand the structural model. If the code is a Minimal, Complete, and Verifiable example MCVE then building your mental models is relatively easy because the code base should be small and self-contained. For such examples, the operational model is usually defined at the level of language semantics and standard library APIs. There is a nice detailed post about debugging small programs on Eric Lippert's blog. The main challenge with debugging a truly huge codebase (millions of lines of code) is that you need to build mental models that cover the overall system, even if you are looking to debug a very specific aspect of the system behaviour. For example, some years ago I modified the Weather Research and Forecasting model to run on GPUs, and debugged the changes. This is a numerical weather simulator with a codebase of about two million lines of Fortran 90. It is very well architected and there is reasonably good documentation. The main challenge in this system was actually to understand the build system first, because a large amount of code is generated at build time. Apart from that, I had to learn how a weather simulator works at the level of the physics, and how the code was parallelised. I modified the part of the code known as the advection kernel, to make it work on GPUs. As expected, the changes were not first-time-right, and debugging GPU code is difficult because it is hard to observe what happens inside the GPU. Nevertheless, I followed essentially the approach outlined above. In this case, the original, unmodified code provided the reference behavioural model. I built the structural model through the process of working out which part of the code needed to be modified. So the difficulty was as usual with the operational model, and in this case the bugs mostly originated from the fact that the GPU code is essentially C, and the host code Fortran, and they have different views on arrays and argument passing. Conclusion Debugging is difficult and time consuming but a strategy based on behavioural, operational and structural mental models can make the process more efficient in a variety of scenarios. I would like to thank Ahmed Fasih for motivating me to write this article and suggesting the example scenarios. Updated March 05, 2018 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2020 Jekyll theme Skinny Bones","tags":"","url":"debugging/index.html"}, {"title":"Musings of an Accidental Computing Scientist","text":" Musings of an Accidental Computing Scientist August 07, 2020 Function types A brief introduction into function types, with a way to implement them in Raku and examples in many languages. July 18, 2020 Cleaner code with functional programming An introduction to some powerful functional programming techniques in Raku and Python. July 03, 2020 Everything is a function Although it might seem that a language like Haskell has a lot of different objects and constructs, they can all be reduced to functions. June 22, 2020 List-based parser combinators in Haskell and Raku This is a follow-on of my article on algebraic data types, with list-based parser combinators as a practical application. June 05, 2020 Roles as Algebraic Data Types in Raku Algebraic data types are great for building complex data structures, and easy to implement in Raku using roles. March 28, 2020 The Winds of Kyoto A very short story for the Fifth Annual Kyoto Writing Competition. April 27, 2019 Writing faster Perl code Sometimes your pure Perl code needs to be as fast as possible. What does it take? October 21, 2018 Why Europe needs hurricane contingency planning Hurricanes will become common in Europe in the next few decades. I discuss the evidence, the impact and the need for proper contingency planning. April 19, 2018 Hacking the Pleroma: Elixir, Phoenix and a bit of ActivityPub A brief guide into hacking Pleroma, a federated microblogging server software. March 13, 2018 A little maths puzzle in two parts Of which the outcome is a way to construct a regular pentagon using compass and ruler. March 05, 2018 A strategy for articles/debugging For a long time it has been my contention that for a developer, more that programming, debugging should be treated as a core skill. March 04, 2018 Imagine we had better weather forecasts Current weather forecasts are at the same time very advanced and yet not good enough. Earlier and more accurate warnings could help to limit the damage of su... Wim Vanderbauwhede About Wim Vanderbauwhede About © 2020 Jekyll theme Skinny Bones","tags":"","url":"index.html"}, {"title":"Why Europe needs hurricane contingency planning","text":" Why Europe needs hurricane contingency planning As a result of global warming, hurricanes will become common in Europe in the next few decades. I discuss the evidence, the impact and the need for proper contingency planning. Some terminology A tropical cyclone is a rapidly rotating storm system with a low-pressure centre, strong winds, and a spiral arrangement of thunderstorms that produce heavy rain. They are called \"tropical\" because they form almost exclusively over tropical or sub-tropical waters. \"Cyclone\" refers to their winds moving in a circle, rotating around a central clear eye. The winds blow counterclockwise in the Northern Hemisphere and clockwise in the Southern Hemisphere. For completeness, an anticyclone is a large-scale circulation of winds around a central region of high atmospheric pressure, clockwise in the Northern Hemisphere, counterclockwise in the Southern Hemisphere\". Anticyclones are not storm systems, and in Belgium, where I used to live, the famous[^1] Azores Anticyclone usually lead to nice weather. The terms \"hurricane\" and \"typhoon\" refer both to powerful tropical cyclones. A hurricane[^2] occurs in the Atlantic Ocean and northeastern Pacific Ocean, and a typhoon[^3] occurs in the northwestern Pacific Ocean[^4]. The diameter of the hurricane is typically of the order of hundreds of kilometers. Last month's hurricane Florence was over 600 km in diameter, and typhoon Trami about 500 km. <!-- , The opposite direction of circulation is due to the Coriolis effect. Tropical cyclones typically form over large bodies of relatively warm water. --> <!-- In the south Pacific or Indian Ocean, comparable storms are referred to simply as \"tropical cyclones\". --> A \"tropical storm\" is \"tropical cyclone\" that is less strong than a hurricane or typhoon. Strength and potential damage The Saffir-Simpson Hurricane Wind Scale is a scale from 1 to 5 based on a hurricane's sustained wind speed, used to estimate potential property damage. Hurricanes reaching Category 3 and higher are considered major hurricanes because of their potential for significant loss of life and damage. However, Category 1 and 2 hurricanes are still much more dangerous than ordinary tropical storms, and require preventive measures. The amount of energy in a hurricane is very large. NOAA (US National Oceanic and Atmospheric Administration) notes that the energy released by an average hurricane \"is equivalent to 200 times the world-wide electrical generating capacity\". <!-- but converting to megatons, a hurricane releases 12,400 megatons of energy per day. For reference, the most powerful US nuclear weapon in active service is a 1.2 megaton bomb. --> The table below is adapted from NOAA page linked above. |Category|Sustained wind speed|Extent of damage| |--------|-------------------------|----------------| |1|119-153 km/h|Very dangerous winds will produce some damage:Houses could have damage to roof, shingles and gutters. Large branches of trees will snap and shallowly rooted trees may be toppled. Extensive damage to power lines and poles likely will result in power outages that could last a few to several days.| |--------|--------------------|--------| |2|154-177 km/h|Extremely dangerous winds will cause extensive damage:Houses could sustain major roof and siding damage. Many shallowly rooted trees will be snapped or uprooted and block numerous roads. Near-total power loss is expected with outages that could last from several days to weeks.| |--------|--------------------|--------| |3|178-208 km/h|Devastating damage will occur:Houses may incur major damage or removal of roof decking and gable ends. Many trees will be snapped or uprooted, blocking numerous roads. Electricity and water will be unavailable for several days to weeks after the storm passes.| |--------|--------------------|--------| |4|209-251 km/h|Catastrophic damage will occur:Houses can sustain severe damage with loss of most of the roof structure and/or some exterior walls. Most trees will be snapped or uprooted and electricity poles downed. Fallen trees and electricity poles will isolate residential areas. Power outages will last weeks to possibly months. Most of the area will be uninhabitable for weeks or months.| |--------|--------------------|--------| |5|252 km/h or higher|Catastrophic damage will occur:A high percentage of houses will be destroyed, with total roof failure and wall collapse. Fallen trees and electricity poles will isolate residential areas. Power outages will last for weeks to possibly months. Most of the area will be uninhabitable for weeks or months.| Note that this table uses sustained wind speeds. The gusts occuring during a tropical cyclone can be a lot stronger than this (typically about 30% stronger), and are usually what is quoted in the media. Global warming causes hurricanes to get stronger Slightly simplifying, hurricanes get their energy from the heat in the surface sea water. As the planet gets warmer, the sea surface gets warmer, which leads to stronger tropical cyclones. There is plenty evidence for this trend. For example, very recent work by my friend and colleague Prof. Takemi of the Disaster Prevention Research Institute (DPRI) of Kyoto University [1] used observation data of historical typhoons to reproduce them in simulation, and then simulated the effects of future warmer conditions. They conclude that both wind speed and precipitation would increase significantly. When I visited DPRI in September, a researcher explained me their latest simulations. With 2 degrees warming, wind speeds and rainfall during a typhoon could double. The recent Guardian article \"Is climate change making hurricanes worse?\" provides a good overview. Note that this trend refers to all tropical cyclones, not just these that made landfall. Hurricanes are coming to Europe Historically, none of the tropical storms in Europe in the last century except hurricane Vince in 2005 have been hurricanes, no matter how severe they might have seemed. However, this is about to change. As hurricanes get more powerful and last longer, the chance that they can reach Europe grows. Again, there is evidence for this. Already in 2013 Reindert Haarsma, Senior Scientist at the Royal Netherlands Meteorological Institute wrote the article \"The future will bring hurricanes to Europe\" which gives a good overview. The article is based on Haarsma's scientific research [2]. Other researchers predict similar trends [3,4,5]. Impact and contingency planning Hurricanes do not only cause damage because of the strong winds. They also lead to flash floods because of the very heavy rainfall that they cause (often 30-50 cm/h), and because of the storm surges which are several meters in height (e.g. 5 m for hurricane Michael a few weeks ago). To put this into perspective, <!--one of the worst rainfall events and floods in the UK was in 2015, when rainfall of 5 to 8 cm over 6 to 9 hours was enough to cause a \"red\" severe weather warning in the north of England. This event was caused by--> in December 2015 storm Desmond broke the United Kingdom's 24-hour rainfall record with 34 cm of rainfall in 24 hours, and led to widespread flooding in the UK and Ireland. The only storm surge on record comparable to those caused by hurricanes was the notorious Great Storm of 1953. In Scotland, where storms are common, this was the worst storm[^5] in 500 years. Better flood defenses are therefore absolutely crucial to deal with future hurricanes. As a result of the 1953 disaster, much was done in the UK, the Netherlands and Belgium to strenghten flood defenses, but these focus on the North Sea. Similar works on the coasts facing the Atlantic will be necessary. Furthermore because of the increased damage, power outages and disruption of supplies will last much longer than for the storms we have now and therefore contingency plans will have to be put in place. A capability for accurate forecasting of hurricane trajectories is necessary for timely evacuation of people in the affected areas. The good news is that Europe can benefit from the extensive know-how developed for example in the US and Japan, both in predictions and in dealing with the effects of such severe weather events. [1] Kanada S, Takemi T, Kato M, Yamasaki S, Fudeyasu H, Tsuboki K, Arakawa O, Takayabu I. A multimodel intercomparison of an intense typhoon in future, warmer climates by four 5-km-mesh models. Journal of Climate. 2017 Aug;30(15):6017-36. [2] Haarsma RJ, Hazeleger W, Severijns C, De Vries H, Sterl A, Bintanja R, Van Oldenborgh GJ, van den Brink HW. More hurricanes to hit western Europe due to global warming. Geophysical Research Letters. 2013 May 16;40(9):1783-8. [3] Baker A, Hodges K, Schiemann R, Vidale PL. North Atlantic post-tropical cyclones in reanalysis datasets. In EGU General Assembly Conference Abstracts, 2018 Apr (Vol. 20, p. 14606). [4] Dekker MM, Haarsma RJ, de Vries H, Baatsen M, van Delden AJ. Characteristics and development of European cyclones with tropical origin in reanalysis data. Climate Dynamics. 2018 Jan 1;50(1-2):445-55. [5] Mousavi ME, Irish JL, Frey AE, Olivera F, Edge BL. Global warming and hurricanes: the potential impact of hurricane intensification and sea level rise on coastal flooding. Climatic Change. 2011 Feb 1;104(3-4):575-97. The banner image shows typhoon Halong approaching Japan in September 2014, © NASA Terra/MODIS 2014 [^1]: It is so famous that a travel book shop in Brussels took it as its name. [^2]: The term \"hurricane\" derives from the Spanish word huracán, which in turn probably derives from the Taino (an indigenous people of the Caribbean) word hurakán \"god of the storm\". [^3]: In Japan they are called 台風 (taifuu) and are given numbers rather than names. [^4]: In the northwestern Pacific, the term \"super typhoon\" is used for tropical cyclones with sustained winds exceeding 240 km/h. [^5]: This was a European windstorm, a type of extratropical cyclone, caused by different weather phenomena than hurricanes. There is evidence that this type of storms is also getting stronger [5]. Updated October 21, 2018 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2020 Jekyll theme Skinny Bones","tags":"","url":"articles/prepare-for-hurricanes/index.html"}, {"title":"Writing faster Perl code","text":" Writing faster Perl code As part of my research I have been developing a Fortran source-to-source compiler — in Perl. The compiler transforms legacy FORTRAN 77 scientific code into more modern Fortran 95. For the reasons to target FORTRAN 77, please read my paper. The compiler is written in Perl because it is available on any Linux-like system and can be executed without the need for a build toolchain. The compiler has no external dependencies at all, so that it is very simply to install and run. This is crucial because my target users are weather and climate scientists, not programmers or computer scientists. Python would have been a viable alternative but I personally prefer Perl. Perl performance as we know it An often-made argument is that if you want performance, you should not write your code in Perl. And it is of course true that compiled code will almost always be faster. However, often rewriting in a compiled language is not an option, so it is important to know how to get the best possible performance in pure Perl. The Perl documentation has perlperf which offers good advice and these tips provide some more detail. But for my needs I did not find the answers there, nor anywhere else. So I created some simple test cases to find out. I used Perl version 5.28, the most recent one, but the results should be quite similar for earlier versions. Testing some hunches Before going into the details on the performance bottleneck in my compiler, here are some results of performance comparisons that influenced design decisions for the compiler. The compiler is written in a functional style — I am after all a Haskell programmer — but performance matters more than functional purity. Fortran code essentially consists of a list of statements which can contain expressions, and the parser labels each of the statements once using a hashmap, ever the workhorse data structure in Perl. Every parsed line of code is stored as a pair with this hashmap (which I call $info): This means than in principle I can choose to match a pattern in $line using a regex or use one of the lables in $info. So I tested the performance of hash key testing versus regexp matching, using some genuine FORTRAN 77 code: Without the if-condidion, the loop takes 3.1 s on my laptop. The loop with the regexp match condition takes 10.1 s; with the hash key existence test it takes 5.6 s. So the actual condition evaluation takes 7 s for regexp and 2.5 s for hash key existence check. So testing hash keys is alsmost three times faster than simple regexp matching. I tested the cost of using higher-order functions for tree traversal. Basically, this is the choice between a generic traversal which takes an arbitrary function that operates on the tree nodes: or a custom traversal: For the case of the tree data structures in my compiler, the higher-order implementation takes twice as long as the custom traversal, so for performance this is not a good choice. Therefore I don't use higher-order functions in the parser, but I do use them in the later refactoring passes. Finally I tested the cost of using map instead of a foreach-loop: The foreach-loop version takes 2.6 s, the map version 3.3 s, so the map is 25% slower. For reference, the index-based for-loop version takes 3.8 s and the C-style for-loop version 4.4 s — don't do that! Because the map is slower, again I did not use it in the parser, and I implemented my own higher-order functions which use foreach-loops internally for the refactoring passes. Compiler bottleneck: expression parsing As the compiler grew in capabilities, it became noticeably slower. Perl has a great profiling tool, Devel::NYTProf, and I used it to identify the bottleneck. As you can see from the flame graph in the banner image, it turned out to be the expression parser. This part of the code was based on Math::Expression::Evaluator because it was convenient to reuse. But it was not built for performance, and also not to parse Fortran. So I finally bit the bullet and wrote my own. What I loosely call an expression parser is actually a combination of a lexer and a parser: it turns a string of source code into a tree-like datastructure which expresses the structure of the expression and the purpose of its constituents. For example if the expression is 2*v+1, the result of the expression parser will be a data structure which identifies the top-level expression as a sum of a multplication with the integer constant 1, and the multiplication of an integer constant 2 with a variable v. So how do we build a fast expression parser? It is not my intention to go into the computing science details, but instead to discuss the choices to be considered. Testing some more hunches First, the choice of the data structure matters. As we need a tree-like ordered data structure, it would have to either an object or a list. But objects in Perl are slow, so I use a nested list. As it happens, Math::Expression already uses nested lists. Using the parser from Math::Expression, the above expression would be turned into: This data structure is fine if you don't need to do a lot of work on it. However, because every node is labeled with a string, testing against the node type is a string comparison. I did a quick test: On my laptop, the version with string comparison takes 5.3 s, the integer comparison 4.6 s. Without the if-statement, the code takes 3.1 s. In other words, the actual if with string comparison takes 2.2 s, with integer comparison 1.5 s. So doing string comparisons is 50% slower than doing integer comparisons. Therefore my data structure uses integer labels. Also, I label the constants so that I can have different labels for string, integer and real constants, and because in this way all nodes are arrays. This avoids having to test if a node is an array or a scalar, which is a slow operation. So the example becomes : Less readable, but faster and easier to extend. Then we have to decide how to parse the expression string. The traditional way to build an expression parser is using a Finite State Machine, consuming one character at a time (if needed with one or more characters look-ahead) and keeping track of the identified portion of the string. This is very fast in a language such as C but in Perl I was not too sure, because in Perl a character is actually a string of length one, so every test against a character is a string comparison. On the other hand, Perl has a famously efficient regular expression engine. So I created a little testbench to see which approach was faster: On my laptop, the FSM version takes 3.25 s, the regex version 1.45 s (mean over 10 runs), so the regexp version is twice as fast — the choice is clear. A faster expression parser With the choices of string parsing and data structure made, I focused on the structure of the overall algorithm. The basic approach is to loop trough a number of states and in every state perform a specific action. This is very simple because we use regular expressions to identify tokens, so most of the state transitions are implicit: The matching rules and operations are very simple (I use <pattern> and <integer> as placeholders for the actual values): prefix operations: perl if ( $str=~s/^<pattern>// ) { $state=<integer>; } terms: perl if ( $str=~s/^(<pattern>)// ) { $expr_ast=[<integer>,$1]; } operators: perl $prev_lev=$lev; if ( $str=~s/^<pattern>// ) { $lev=<integer>; $op=<integer>; } prefix operations: perl if ( $str=~s/^<pattern>// ) { $state=<integer>; } terms: perl if ( $str=~s/^(<pattern>)// ) { $expr_ast=[<integer>,$1]; } operators: perl $prev_lev=$lev; if ( $str=~s/^<pattern>// ) { $lev=<integer>; $op=<integer>; } Operators have precedence and associativity, and Fortran requires twelve precedence levels. In the \"Append to AST\" state, the parser uses $lev and $prev_lev to work out how the previously matched $expr_ast and $op should be appended to the @ast array. The prefix operations are handled by setting a state which is checked after term matching. The actual code is a bit more complicated because we need to parse array index expressions and function calls as well. This is done recursively during term matching; if a function call has multiple arguments, the parser is put into a new $state. So the end result is a minimally recursive parser, i.e. it only uses recursion when it is really necessary. This is because Perl is not efficient in doing recursive function calls (nor in fact for non-recursive ones). There is a lot of repetition of the patterns for matching terms and operators because if I would instead abstract the <pattern> and <integer> values by e.g. storing them in an array, the array accesses would considerably reduce the performance. I do store the precedence levels in an array because there are so many of them that the logic for appending terms to the AST would otherwise become very hard to read and update. Expression parser performance I tested the new expression parser on a set of 50 different expressions taken from a weather simulation code. The old expression parser takes 45 s to run this test a thousand times; the new expression parser takes only 2 s. In other words, the new parser is more than twenty times faster than the old one. It is also quite easy to maintain and adapt despite its minimal use of abstractions, and because it is Fortran-specific, the rest of the code has become a lot cleaner too. You can find the code in my GitHub repo. Summary Here is a summary of all optimisations I tested. The tests were run using Perl v5.28 on a MacBook Pro (late 2013), timings are averages over 5 runs and measured using time. | --- | --- | | Optimisation | Speed-up | | --- | --- | | Hash key testing is faster than regexp matching | 3× | | Custom tree traversals are faster than generic ones | 2× | | foreach is faster than map | 1.3× | | foreach is faster than indexed for | 1.4× | | foreach is faster than C-style for | 1.7× | | Integer comparison is faster than string comparison | 1.5× | | Regexp matching is faster than successive string comparisons | 2.2× | Updated April 27, 2019 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2020 Jekyll theme Skinny Bones","tags":"","url":"articles/writing-faster-perl/index.html"}, {"title":"Everything is a function","text":" Everything is a function This is an article I wrote several years ago. It is part of the \"Functional Programming in Haskell\" online course. It discusses one of the aspects of functional programming that I like in particular, the fact that the entire language can be build starting from the lambda calculus. In a functional language, there are only functions Although it might seem that a language like Haskell has a lot of different objects and constructs, they can all be reduced to functions. We will demonstrate how variables, tuples, lists, conditionals, Booleans and numbers can all be constructed from lambda functions. The article assumes some familiarity with Haskell, but here is a quick introduction. Haskell is whitespace-sensitive like Python, but has a markedly different syntax. Because everything is a function, there is no keyword to mark a function; because there is only lexical scope, there is no need for any special scope identifiers. Function arguments are separated by spaces; anonymous functions are called lambda functions and have a special syntax to identify them: Several of the examples use the let ... in ... construct, which behaves as a lexically scoped block: x0, x1 and x2 are in scope only in the expression after the in keyword. Haskell is statically typed, and the type of a function or variable is written in a separate annotation, for example: The isEmpty function has a type signature, identified by ::, that reads \"isEmpty is a function from a list of anything to a Boolean\". Types must be written with an initial capital. The a is a type variable which can take on any type, as explained in my post on algebraic data types. Most of the time, you don't need to write type declarations as Haskell works them out automatically. In the rest of the article, I'm not focusing on the types so I have omitted them. Haskell uses let expressions to define variables used in a final expression, for example: We can rewrite this to use only one variable per let: Now we rewrite any named functions (f) as lambda functions: Then we rewrite the let expressions themselves as lambdas, first the inner let: We do this by turning the variable in the let part of the expression (f) into a parameter of a lambda function (\\f -> ...). The body of the function is the expression after the in (f n). Then we apply this lambda function to the expression bound to the variable (\\x -> x+1). Then we rewrite outer let in the same way: This expression consists only of lambda expressions, which shows that variables and let-expressions are just syntactic sugar for lambda expressions. Haskell has tuples, also called record types or product types, ordered collections of expressions of potentially different types: The tuple notation is syntactic sugar for a function application: The tuple construction function can again be defined purely using lambdas: What we do here is to use the elements of the tuple as the arguments of a lambda function. So what mkTup returns is also a lambda function, in other words mkTup is a higher-order function. Now we rewrite the mkTup named function as lambda function as well: So our tuples are now also encoded purely as lambda functions. The same goes for the tuple accessor functions: Let's see what happens here: the argument tp of fst is a function: \ -> t x' y' z'. We now apply this function to another function, \\x y z -> x: Applying the function gives: And so the result will of course be x', which is indeed the first element of the tuple. Lists can be defined in terms of the empty lists [] and the cons operation (:). Rewriting this using : and []: Or using cons explicitly: We can define cons using only lambda functions as We've used the same approach as for the tuples: cons returns a lambda function. So we can write a list as: We can also define head and tail using only lambdas, similar to what we did for fst and snd above: We can define the empty list as follows: This is a lambda function which always returns true, regardless of its argument. The definitions for true and false are given below under Booleans. With this definition we can check if a list is empty or not: Let's see how this works. A non-empty list is always defined as: which with our definition of (:) (i.e. cons) is: And therefore: And so we have a pure-lambda definition of lists, including construction, access and testing for empty. Now that we can test for the empty list we can define recursions on lists such as foldl, map etc.: and with The definitions of foldl and map use an if-then-else expression which is defined below under Conditionals. With foldl and reverse it is easy to express list concatenation: To compute the length of a list we need integers, they are defined below. We increment the lent counter for every element of the list consumed by the fold. We have used conditionals in the above expressions: Here cond is an expression returning either true or false, these are defined below. We can write the if-then-else clause as a pure function: To evaluate the condition we need to define Booleans as lambda functions: The Boolean is a function selecting the expression corresponding to true or false. With this definition, the if-then-else becomes simply: Using ifthenelse we can define and, or and not: We note that to test equality of Booleans we can use xnor, and we can of course define xor in terms of and, or and not: and The common way to define integers in the lambda calculus is as Church numerals. Here we take a different approach, but it is of course equivalent. We define an integer as a list of Booleans, using thermometer code, and with the following definitions: We define unsigned 0 as a 1-element list containing false. To get signed integers we simply define the first bit of the list as the sign bit. We define unsigned and signed versions of 0: For convenience we define also: The definition of 0 makes the integer equality (==) easier: We can also easily define negation: For convenience we define also define increment and decrement operations: General addition is quite easy: In the same way, subtraction is also straightforward: An easy way to define multiplication is by defining the replicate and sum operations: Then multiplication simply becomes In a similar way we can define integer division and modulo. We note that floating-point numbers and characters use an integer representation, and strings are simply lists of characters. So we don't need to do any additional work to represent them, and the operations on them are analogous to the ones defined above. Conclusion In this way, we have defined a language with variables, (higher-order) functions, conditionals and recursion. We can manipulate lists and tuples of integers, floats, chars and strings. And yet it consists of nothing more than lambda functions! Updated July 03, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2020 Jekyll theme Skinny Bones","tags":"","url":"articles/everything-is-a-function/index.html"}, {"title":"Function types","text":" Function types This article builds on my earlier articles on algebraic data types in on Raku and their use in the practical example of list-based parser combinators. In this article I want to look at function types in some detail, and show a way to create well typed functions in Raku. If you are not familiar with functional programming (or with Raku), I suggest you read my introduction \"Cleaner code with functional programming\". If you are not familiar with algebraic data types, you might want to read the other two articles as well. For most of the article, I provide examples in Raku, Python, Rust and Haskell. There is also some C and even some Fortran. Type signatures A function's type signature consists of the types of each of its arguments and the return type. In most typed languages, the type is part of the function signature. For example in C or in Fortran or in Rust or in Raku or in Python, using the typing module: The type of a function of functions But what happens if we want to provide a function argument that is itself a function, or return a function (so-called higher-order functions)? This is possible in most languages, but what I am interested in is the type information: what is the type signature of such a function of functions? C supports functions-of-functions indirectly through function pointers, by creating a function type through a typedef. Maybe surprisingly, venerable old Fortran does support passing functions and subroutines as arguments. Functions are typed by their return type; subroutines are not typed. In Rust you can provide the complete type of a function-as-argument: In Python we can use Callable, which also allow for the complete type to be expressed. The same example in Raku becomes Here the only type information is that we are passing a subroutine. An equivalent way to write this is using the & sigil which imposes the Callable type constraint: We will see further how we can create function types for Raku that keep the type signatures of the functions passed as arguments. Introducing the arrow All of the above ways to express function type signatures are perfectly adequate in their respective languages. However, they all share the problem that these function-of-function type signatures don't compose very well: what if we want to write a function-of-function-of-function type? This is less far-fetched than it may seem. I would like to introduce a notation used in type theory. It is at the same time simple and powerful. If you are familiar with functional languages like Haskell, Idris or Agda, you already know it. Instead of mixing the type with the function declaration, it is written separately. The name of the function is followed by a colon and the list types of the arguments and the return value. Each argument is separated by an arrow. The above example of a function of two integer arguments returning an integer would be: The function-of-a-function introduced above has as type: The parentheses group the type of the function that is the only argument of ten_times. In this notation, the arrow can be interpreted as an operator which creates a function type from the two types that are its arguments. The important property of this operator is that it is right associative. What this means is that for example is the same as and as and for completeness A detour into partial application The above groupings imply that our function f can be interpreted in three ways, as a function of: 3 arguments of types t1,t2,t3, returning a result of type t4; 2 arguments of types t1,t2, returning a result of type t3 -> t4; 1 argument of types t1, returning a result of type t2->t3->t4. 3 arguments of types t1,t2,t3, returning a result of type t4; 2 arguments of types t1,t2, returning a result of type t3 -> t4; 1 argument of types t1, returning a result of type t2->t3->t4. Let's say we have values v1,v2,v3 for the arguments and v4 as the result: But suppose we only apply v1 and v2: We get a new function pf1 which takes a single argument v3: And in the same way we can create pf2 and pf3: Because pf1, pf2 and pf3 are functions and the above is true for all values of v1, v2, v3 and v4, it follows that For completeness, we can also apply pf2 directly to two arguments: This concept of creating a new function by not providing values for some of the arguments is called partial application, and many languages support it. Here are examples in Haskell, Raku, Python and Rust. In case you are not familiar with Haskell, this is what you need to know: it is whitespace-sensitive like Python, but has a markedly different syntax. Because everything is a function, there is no keyword to mark a function. Because there is only lexical scope, there is no need for any special scope identifiers. Function arguments are separated by spaces. Lambda functions (anonymous functions) start with a \\, chosen because it looks a bit like the Greek letter lambda, λ. The type of sqsum is haskell sqsum :: Int -> Int -> Int (Haskell uses :: rather than : for the type signature) In Haskell, partial application works exactly as in the examples above. So our function sqsum can be partially applied like this: We can apply sqsum4 to the remaining argument: This is very neat. But suppose you want to apply the second argument, rather than the first one? The Haskell Prelude library provides the function flip, which simply flips the arguments: That is fine as far as it goes, but let's do a somewhat contrived example. Let's say we have a function of four arguments: and we want to apply the 1st and 4th argument but not the others, something like g v1 _ _ v4. One way to do this is to create yet another function (of course!): And with this function we can partially apply the 1st and 4th argument of g: This example mainly serves to illustrate the power of the arrow-based function type notation: it lays out the type of apply14 clearly and concisely. Raku provides the method assuming, which acts as a generalised version of our apply14: The return type of assuming is a Callable. This is a role for objects which support calling them. Thus, g14 can be called as if it was a regular function. Python's functools provide the partial function: The return type of partial is a partial object, which has an attribute partial.func, a callable object or function. Calls to the partial object will be forwarded to func with new arguments and keywords, so you can say g14(v2,v3) instead of g14.func(v2,v3). Rust provides the partial! macro via its partial_application crate. Its behaviour is very similar to our apply14: \"partial!(some_fn => arg0, _, arg2, _) returns the closure |x1, x3| some_fn(arg0, x1, arg2, x3)\". Back to the function types Suppose we want a type like the one we defined in C using a typedef, which encapsulates the function type: In Haskell, that would be and we can generalise this to be a generic function of two arguments by using type variables instead of concrete types: So how would we use this? Let's create an instance This is fine, but to apply the function we first must unwrap the type constructor: That is not very handy. A better way is to use the record type syntax which gives us an accessor function: Now I have applied this to integer, but the type of the function is Num a => a -> a -> a, so this works for any type in the Num typeclass. Proper function types for Raku In Raku, we can follow a similar approach of wrapping a function signature in a type, and it is actually simpler than in Haskell. We create a parametric role which takes the function as a parameter, and has a method with the signature of the function: But what is the benefit of doing this? Surely we could just have done For this simple example, that would indeed be enough as we don't have functions of functions. But what we gain is that we can now create a function with arguments of type Fun2NumArgs: In other words, we can now have explicitly typed function signatures in Raku. Recall that without this approach, the type of a function would be Code or any dependant in the Code type graph. With the role-based type, the function must have the type of the method unF. Furthermore, these function types can be nested. Let's create another type, for a function with two arguments of any type: We create two instances of Fun2NumArgs: And a function of these two functions using Fun2Args: We can now call the returned function like this: Having to call the unF method is not optimal. A better way is to can make the object itself callable instead, by defining the submethod CALL-ME instead of the method unF: In this way, we can do: This is almost what we want. But we can remove the . as well, by making fof2 of type Callable. We can indicate this with the & sigil. But with the current definition of Fun2Args, this will result in a type error because Fun2Args is not callable. However, Callable is a role so all we need to do is mix it in: In this way we have created something very similar to a function object, but using a role rather than a class. And now we can write: To summarize, we create a parametric callable role where the parameter is the function to be called, and the signature of the CALL-ME submethod provides the type constraint to that function. Passing a function with a different signature will give a type error. I think this is a nice way to have more type safety in your functional Raku code. Bonus Tracks \"Call Me\" by Blondie \"CALL ME\" (「コール・ミー」) by Drop's \"Call Me\" by Blondie \"CALL ME\" (「コール・ミー」) by Drop's Updated August 07, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2020 Jekyll theme Skinny Bones","tags":"","url":"articles/function-types/index.html"}, {"title":"Hacking the Pleroma: Elixir, Phoenix and a bit of ActivityPub","text":" Hacking the Pleroma: Elixir, Phoenix and a bit of ActivityPub Pleroma \"is a microblogging server software that can federate (= exchange messages with) other servers that support the same federation standards (OStatus and ActivityPub). What that means is that you can host a server for yourself or your friends and stay in control of your online identity, but still exchange messages with people on larger servers. Pleroma will federate with all servers that implement either OStatus or ActivityPub, like GNU Social, Friendica, Hubzilla and Mastodon.\" (stolen from Lain's blog post). Recently I modified my Pleroma instance to support bot services: parse a posted message, take an action, post the result. To get there I had to learn Elixir, the language in which Pleroma is written, as well as Phoenix, the web framework Elixir uses, and a little bit about ActivityPub, the protocol for exchanging messages. What I want to explain here in particular is the architecture of Pleroma, so that you can hack it more easily, for fun or if you want to participate in the development. Elixir As Pleroma is written in Elixir you'll need to learn that language to some extent. If you are familiar with Ruby (or Perl, for that matter) and with the idea of functional programming (everything is a function), then it is quite easy to learn and understand. The documentation and guides are very good. If you've never hear of functional programming, the main difference with e.g. Ruby or Java is that Elixir does not use an object-oriented programming model. Instead, there are functions that manipulate data structures and other functions. A particular consequence of the functional model is that there are no for- or while-loops. Instead, there are what is called higher-order functions which e.g. apply another function to a list. Elixir programs also make a lot more use of recursion. Another point about Elixir as a web programming language is that it is built on a system where processes communicate by passing messages to one another, and it is built in such a way that if a process dies it will normally be restarted automatically. This approach makes it very easy to offload work to separate worker processes etc. All this comes courtesy of Erlang, the language on which Elixir is built, with its powerfull OTP framework for building applications and its BEAM virtual machine, which manages the processes. Phoenix A lot of the groundwork of Pleroma is done by Phoenix, a very easy-to-use web server framework. Essentially, what happens is that the end user accesses the application using a specific url, typically via a web browser, and based on this url the application performs a number of actions, which in the end result in a change in the state of the application and usually in what is shown in the browser window. In Phoenix, there are five stages or components between the connection and the resulting action by the application: The endpoint is the boundary where all requests to your web application start. It is also the interface your application provides to the underlying web servers. Pleroma's endpoint is web/endpoint.ex. If you look at the source you see several occurrences of plug(Plug...). Plug is a specification for composable modules in between web applications, and it is very heavily used in Pleroma. For example, to serve only specific static files/folders from priv/static: Another very nice feature of Phoenis is that you can edit your code while your server is running. It gets automatically recompiled and the affected processes are automatically restarted, courtesy of the Phoenix.CodeReloader: Routers are the main hubs of Phoenix applications. They match HTTP requests to controller actions, wire up real-time channel handlers, and define a series of pipeline transformations for scoping middleware to sets of routes. Pleroma's router is web/router.ex. The key function in the router is the pipeline which lets you create pipelines of plugs. Other functions are scope, get, post, pipe_through, all of these let you match on the url and whether you are dealing with a get or post request, and define appropriate pipelines of actions. For example, federated ActivityPub requests handled as follows: where the pipe_through(:activitypub) call is used to insert a custom pipeline: Controllers are used to group common functionality in the same (pluggable) module. Pleroma makes heavy use of controllers: almost every request is handled by a specific controller for any given protocol, e.g. MastodonAPIController or ActivityPubController. This makes it easy to identify the files to work on if you need to make a change to the code for a given protocol. For example, the ActivityPub post requests in the Router are handled by inbox function in the ActivityPubController: Views are used to control the rendering of templates. You create a view module, a template and a set of assigns, which are basically key-value pairs. Pleroma uses views for \"rendering\" JSON objects. For example in web/activity_pub/activity_pub_controller.ex there are lines like Here, UserView.render is defined in web/activity_pub/views/user_view.ex for a number of different \"*.json\" strings. These are not really templates, they are simply used to pattern match on the function definitions. The more conventional usage to create HTML is also used, e.g. the template web/templates/mastodon_api/mastodon/index.html.eex is used in web/mastodon_api/mastodon_api_controller.ex via the view web/mastodon_api/views/mastodon_view.ex: Templates are text files (typically html pages) with Elixir code to generate the specific values based on the assigns, included in <%= ... %>. For example, in Pleroma, the Mastodon front-end uses a template for the index.html file which has the code to show the name of the instance. Ecto Ecto is not a part of Phoenix, but it is an integral part of most web applications: Ecto is Elixir's main library for working with databases. It provides the tools to interact with databases under a common API. Ecto is split into 4 main components: Ecto.Repo - repositories are wrappers around the data store. Via the repository, we can create, update, destroy and query existing entries. A repository needs an adapter and credentials to communicate to the database Pleroma uses the PostgresQL database. Ecto.Schema - schemas are used mainly to map tables into Elixir data (there are other use cases too). Ecto.Changeset - changesets provide a way for developers to filter and cast external parameters, as well as a mechanism to track and validate changes before they are applied to your data Ecto.Query - written in Elixir syntax, queries are used to retrieve information from the database. GenServer Because Elixir, like Erlang, uses a processes-with-message-passing paradigm, client-server relationships are so common that they have been abstracted as a behaviour, which in Elixir is a specification for composable modules which have to implement specified public functions (a bit like an interface in Java or typeclass in Haskell). If we look at the Federator.enqueue function, its implementation actually reduces to a single line: GenServer is an Elixir behaviour module for implementing the server of a client-server relation. The cast call sends an asynchronous request to the server (synchronous requests use call). The server behaviour is implemented using the handle_cast callback, which handles cast calls. In Pleroma.Federator, these are implemented in the same module as the enqueue function, hence the use of __MODULE__ rather than the hardcoded module name. Applications, Workers and Supervisors Elixir borrows the concept of a \"supervision tree\" from Erlang/OTP. AN application consists of a tree of processes than can either be supervisors or workers. The task of a supervisors is to ensure that the worker processes do their work, including distributing the work and restarting the worker processes when they die. Supervisors can supervise either worker or other supervisors, so you can build a supervision tree. Elixir provides an Application behaviour module and a Supervisor module to make this easy. The Application module requires a start() function as entry point. Typical code to create a supervision tree is where start_link() spawns the top process of the tree, and it spawns all the child processes in the list children. Pleroma uses a convenient but deprecated module called Supervisor.Spec which provides worker() and supervisor() functions, for example: Every worker has this own start_link function, e.g. in web/federator/federator.ex we find: This means that the Federator module borrows the start_link from the GenServer module. This is a very common way to create a worker. Mix Mix is the build tool for Elixir, and its main advantage is that the build scripts are also written in Elixir. Some key mix actions are provided by Phoenix, for example to build and run the final Pleroma application the action is mix phx.server. Hacking Pleroma After this brief tour of Elixir and Phoenix I want to give an example of adding simple bot functionality to Pleroma. See my fork of Pleroma for the code. My bot parses incoming messages for @pixelbot, extracts a list of pixel from the message, modifies a canvas with the new pixels and creates a PNG image of the result. It then posts a link to the PNG image. Because updating the canvas and creating the PNG image could be time-consuming, especially if the canvas were large, I put this functionality in a separate server module, and added this to the list of workers for the main Pleroma application: The bot takes the size of the canvas from my config.exs using the helper function get_canvas_size(). The id: PixelBot allows to access the worker by name. When the application starts, it launches the PixelBot worker (bots/pixelbot.ex). The worker calls its init() function (part of the GenServer behaviour) which loads the last canvas from a file. One of the protocols used for federation is ActivityPub. The specification is long and not so easy to read. However, for the purpose of hacking Pleroma it mainly helps to understand the structure of an ActivityPub action (in this case a post): In my case, In Pleroma this activity is linked to the Ecto repository Pleroma.Repo (repo.ex) in the module Pleroma.Activity (activity.ex), which defines a schema. The bot only supports ActivityPub. As we have seen above, in Pleroma incoming messages are handled by inbox function in the ActivityPubController (in web/activity_pub/activity_pub_controller.ex), so I put in a little hook there to detect if a message is for @pixelbot and has an actual message body (content): As you can see, the content of a message for @pixelbot is passed on to the PixelBot worker for processing using the GenServer.cast(Pleroma.Bots.PixelBot,content) call. The PixelBot worker parses the message to extract any pixels from it (bots/pixelbot/parse_messages.ex). If there are any, it updates the canvas (which is just a list of lists). It and writes the content to a file, and calls an external program to create the final image. Finally, the bot posts a status to the public timeline (bots/pixelbot/pixelbot_post_status.ex). The status contains the current time and a link to the latest canvas. The function pixelbot_post_status() creates the status and wraps it in the correct structure required by ActivityPub. It also gets the user object based on the nickname via Pleroma.User.get_cached_by_nickname(nickname). Like the activity, this user object is defined via a schema and linked to the Ecto repository (in user.ex). So user in the code below is a complicated object, not a url or nickname. Finally, the function calls ActivityPub.create() which creates the activity, and in this case that means it posts a status. Pleroma source tree This is only a part of the Pleroma source tree, it shows on the files mentioned above. Updated April 19, 2018 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2020 Jekyll theme Skinny Bones","tags":"","url":"articles/hacking-pleroma.html"}, {"title":"Cleaner code with functional programming","text":" Cleaner code with functional programming Functional programming is a style of programming and modern languages support this style to a greater or lesser extent. In this article I want to explain how programming in a functional style provides you with powerful abstractions to make your code cleaner. I will illustrate this with examples in Raku and Python, which as we will see are both excellent languages for functional programming. Raku: a quick introduction The code examples in this article are written in Python and Raku. I assume most people are familiar with Python, but Raku is less well known, so I will explain the basics first. The code in this article is not very idiomatic so you should be able to understand it easily if you know another programming language. Raku is most similar to Perl. Both languages are syntactically similar to C/C++, Java and JavaScript: block-based, with statements separated by semicolons, blocks demarcated by braces, and argument lists in parentheses and separated by commas. The main feature that sets Perl and Raku apart from other languages is the use of sigils ('funny characters') which identify the type of a variable: $ for a scalar, @ for an array, % for a hash (map) and & for a subroutine. Variables also have keywords to identify their scope, I will only use my which marks the variable as lexically scoped. A subroutine is declared with the sub keyword, and subroutines can be named or anonymous: In Python this would be: Raku supports sigil-less variables, and uses the \\ syntax to declare them. For more on the difference between ordinary and sigil-less variables, see the Raku documentation. For example (say prints its argument followed by a newline): In the code in this article, I will use the sigil-less variables whenever possible. Raku has several types of sequence data structures. In the code below I will use lists and arrays and ranges. The main difference between a list and an array in Raku is that a list is immutable, which means that once created, it can't be modified. So it is a read-only data structure. To 'update' an immutable data structure, you need to create an updated copy. Arrays on the other hand are mutable, so we can update their elements, extend them, shrink them etc. All updates happen in place on the original. Raku's arrays are similar to Python's lists and Raku's lists are similar to Python's tuples, which are also immutable. Apart from the syntax, ranges in Raku are similar to ranges in Python, and both are immutable. The equivalent Python code would be Other specific bits of syntax or functionality will be explained for the particular examples. A function, by any other name — functions as values Functions are the essence of functional programming. As I explained in my article \"Everything is a function\", in a proper functional language, all constructs are built from functions. All modern programming languages have a notion of functions, procedures, subroutines or methods. They are an essential mechanism for code reuse. Typically, we think of a function as something that operates on some input values to produce one or more output values. The input values can be globally declared, attributes of a class or passed as arguments to the function. Similarly, the output values can be returned directly, to global variables, as class attributes or by modifying the input values. To benefit most from functional programming, it is best if functions are pure, which means that a call to the function always produces the same output for the same inputs. In practice, this is easier to achieve if the function only takes inputs as arguments and returns the output directly, but this is not essential. The crucial feature of functional programming is that the input and output values of a function can themselves be functions. So functions must be values in your language. Sometimes this is called \"functions must be first-class\", and a function that takes and/or returns a function is sometimes called a \"higher-order function\". If functions are values, it follows that we can assign them to variables. In particular we will assign them to the arguments of other functions. But we can also assign them to ordinary variables. Let's consider the following function, choose, which takes three arguments t, f and c. First let's call choose with strings as values for the first two arguments: Now let's try with functions as arguments: So our function choose took two functions as its first two arguments, and returned a function. In Raku we need the & sigil on the function names because otherwise they would be evaluated: a bare function name like tt is the same as calling the function without arguments, tt(). By assigning this function to a variable (res_f), we can now call res_f as a function and it will eventually call tt or ff depending of the choice. Functions don't need a name Now, if we can assign functions to variables, they don't really need a name themselves. So our functions can be anonymous. Most languages support anonymous functions. In functional languages they are usually called \"lambda functions\". In Raku, we have two ways to create anonymous functions: Using the sub (...) syntax: Or using the 'pointy block' syntax, which is a little bit more compact: Python uses the lambda keyword: So now we can say When we print out the variable to which the function is bound, Raku returns sub { } to indicate that the variable contains a function. In Python: Examples: map, grep and reduce Functions of functions have many uses, and I just want to highlight three examples that are available readily in Raku: map, reduce and grep. Python has map and filter, and provides reduce via the functools module. What these functions have in common is that they offer an alternative to for-loops over lists. map takes two arguments: a function and a list. It applies the function to all values in the list in order and returns the results, for example to square all values in a list: In Python we need to explicitly create the tuple, but apart from the syntax differences, the structure is quite the same: This is the functional alternative to the more conventional for-loop: Note that in both Raku and Python we need to use a mutable data structure for the for-loop version, whereas the map version uses immutable data structures. grep (called filter in Python) also takes arguments, a function and a list, but it only returns the values from the list for which the function returns true: We can of course write this using a for-loop and an if-statement, but that again requires a mutable data structure: What's nice about map and grep is that you can easily chain them together: This is because map and grep take a list and return a list, so as long as you need to operate on a list, you can do this by chaining the calls. reduce also takes a function and a list, but it uses the function to combine all elements of the list into a single result. So the function must take two arguments. The second argument is the element taken from the list, and the first argument is used as a state variable to combine all elements. For example, calculating the sum of a list of numbers: What happens here is that acc is first set to the first element of the list (1), and then the second element is added to it, so acc becomes 1+2=3; then the third element (3) is added to this, and so on. The effect is to consecutively sum all the numbers in list. To make this more clear, let's write our own version of reduce. In many functional languages, a distinction is made between a left-to-right (starting at the lowest index) and right-to-left (starting at the highest index) reduction. This matters because depending on the function doing the reducing, the result can be different if the list is consumed from the left or from the right. For example, suppose our reducing function is then it does not matter which direction we traverse the list. But consider the following function: ( ... ?? ... !! ... is the Raku syntax for the conditional operator which is ... ? ... : ... in most other languages and ... if ... else ... in Python) In this case the result will be different if the list is reduced from the left or from the right. In Raku and Python, reduce is a left-to-right reduction. Also, instead of using the first element of the list, the reduction function can take an additional argument, usually called the accumulator. In functional languages, reduce is usually called fold, so we can have a left fold and a right fold. Let's have a look how we could implement these. A straightforward way to implement a left fold (so the same as reduce) is to use a for-loop inside the function. That means we have to update the value of the accumulator on every iteration of the loop. In Raku, sigil-less variables are immutable (I am simplifying here, see the Raku documentation for the full story) so we need to use a sigiled variable, $acc. If we want to use immutable variables only, we can use recursion. Raku makes this easy because it allows multiple signatures for a subroutine (multi subs), and it will call the variant that matches the signature. In Python, there is the module multipledispatch that lets you do something similar to multi subs. Our foldl will consume the input list lst and use f combine its elements into the accumulator acc. When the list has been consumed, the computation is finished and we can return acc as the result. So our first variant says that if the input list is empty, we should return acc. The second variant takes an element elt from the list (see the Raku documentation for details on the *) and combines it with acc into f(acc,elt). It then calls foldl again with this new accumulator and the remainder of the list, rest. Python does not allow pattern matching of this kind so we need to write the recursion using a conditional: In this implementation, none of the variables is ever updated. So all variables can be immutable. The right fold is quite similar to the left fold. For the loop-based version, all we do is reverse the list. In the recursive version, we take the last element from the list instead of the first one. For details on the ..^ * - 1 syntax please see the Raku documentation. Now, what about map and grep? We can of course implement these with for-loops, but we can also implement them using our foldl: Because the function f is mappable, it only has a single argument. But foldl needs a function with two arguments, the first for the accumulator. So we call foldl with an anonymous function of two arguments. The accumulator itself is an empty list. Although we said earlier that a reduction combines all elements of the original list into a single return value, this return value can of course be any data type, so also a list. So we call f on every element of the original list and add it to the end of the accumulator list. (The | flattens the list, so (|acc,f(elt)) is a new list built from the elements of acc and result of f(elt).) In a similar way we can also define grep: Just like in the map implementation, we call foldl with an anonymous function. In this function we test if f(elt) is true for every elt in lst. If it is true we create a new list from acc and elt, otherwise we just return acc. Because map and grep operate on each element of the list separately, we could implement them using the right fold as well. With these examples I hope that both the concept of a function working on functions and the possible ways of implementing them has become more clear. The advantage of the recursive implementation is that it allows us to use immutable data structures. You may wonder why I focus on these immutable data structures. As we will have seen, functional programming works really well with immutable data structures. And they have one big advantage: you never have to worry if you have accidentally modified your data, or whether you should make a copy to be sure. So using immutable data structures make code less error-prone and easier to debug. They also have potential performance benefits. And as we'll see next, in Raku there is yet another advantage. Functions returning functions Functions can also return functions. This is in particular useful if we want to have a parametrisable function. As a trivial example, suppose we want a series of functions that increments a number with a fixed value: add1, add2 etc. We could of course write each of them separately: Or we could use a list filled with anonymous functions: We could do better and use a loop to fill an array with anonymous functions: We create a new anonymous function with every loop iteration, and add it to the array. But instead, we could use a function to create these anonymous functions, and then we could use map instead of a loop, and use an immutable data structure: In Raku, using a range has an additional benefit: we can set the end of the range to infinity, which in Raku can be written as ∞ (unicode 221E), * or Inf. This is an example of what is called \"lazy evaluation\", or laziness for short: Raku is not going to try (and fail) to process this infinite list. Instead, it will do the processing when we actually use an element of that list. The evaluation of the expression is delayed until the result is needed, so when we call add[244], what happens is that gen_add(244) is called to generate that function. Note that this will not work with the for-loop, because to use the for-loop we need a mutable data structure, and the lazy lists have to be immutable. So this is a nice example of how the functional programming style allows you to benefit from laziness. For the full story of laziness in Raku, please see the documentation. Python does not have lazy lists but is have a different form of laziness: the call to map (or filter) does not return the sequence of results but instead it returns a generator: It is only when we wrap the generator in a sequence constructor such as tuple() that the results are actually generated. Function composition We saw above that you can chain calls to map and grep together. Often you only need to chain map calls together, for example In that case, we can do this a little bit more efficient: rather than creating a list and then calling map on that list, we can do both computations at once by composing the functions. Raku provides a special operator for this: The operator ∘ (the \"ring operator\", unicode 2218, but you can also use a plain o) is the function composition operator, and it's pronounced \"after\", so f ∘ g is \"f after g\". What it does is create a new function by combining two existing functions: is the same as The advantage of the composition operator is that that it works for any function, including anonymous ones. But in fact, it is just another higher-order functions. It is simply the operator form of the following function: Python does not have a function composition operator, but you can easily have compose in Python too: Conclusion In this article I have used Raku and Python examples to introduce three key functional programming techniques: functions that operate on functions, functions that return functions and function composition. I have shown how you to use the functions map, reduce (fold) and grep (filter) to operate on immutable lists. I have explained how yo(u can implement such functions with and without recursion, and what the advantage is of the recursive implementation. Here is the code from the article, Raku and Python. There is of course a lot more to functional programming and I have written a few articles on more advanced topics. The concepts introduced in this article should provide a good basis for understanding those more advanced topics. If you want to learn more about functional programming, you might consider my free online course. Updated July 18, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2020 Jekyll theme Skinny Bones","tags":"","url":"articles/decluttering-with-functional-programming/index.html"}, {"title":"Roles as Algebraic Data Types in Raku","text":" Roles as Algebraic Data Types in Raku I have been a lambdacamel, one of those who like Perl and functional programming, especially in Haskell, for a long time. I still write most of my code in either of these languages. I've also been a fan of Raku from long before it was called Raku, but I'd never used it much in real life. Recently though, I've been moving increasingly to Raku for code that I don't have to share with other people. It's a lovely language, and its functional heritage is very strong. It was therefore only natural to me to explore the limits of Raku's type system. Is this article for you? In this article I will introduce algebraic data types, a kind of static type system used in functional languages like Haskell, and a powerful mechanism for creating complex data structures. I will show a way to implement them in Raku using roles. You don't need to know Haskell at all and I only assume a slight familiarity with Raku (I've added a quick introduction), but I do assume you are familiar with basic programming. You may find this article interesting if you are curious about functional-style static typing or if your would like an alternative to object-oriented programming. Algebraic Data Types Datatypes (types for short) are just labels or containers for values in a program. Algebraic data types are composite types, they are formed by combining other types. They are called algebraic because they consist of alternatives (sums, also called disjoint unions) and record (products) of types. For more details see [1] or [2]. To give a rough intuition for the terms \"sum type\" and \"product type\": in Raku, with Booleans $a, $b and $c, you can write $a or $b or $c but you could also write $a + $b + $c and evaluate it as True or False. Similarly, $a and $b and $c can be written as $a * $b * $c. In other words, and and or behave in the same way as + and *. In a generalised way, the types in algebraic data type system can be composed using similar rules. Let's first give a few examples of algebraic data types. In this section I am not using a specific programming language syntax. Instead I use a minimal notation to illustrate the concepts. I use the datatype keyword to indicate that what follows is a declaration for an algebraic data type; for a sum type, I'll separate the alternatives with '|'; for a product type, I separate the components with a space. To declare a variable to be of some type, I will write the type name in front of it. We can define a Boolean value purely as a type: And we can use this as This means that ok is a variable of type Bool with a value of True. In an algebraic data type, the labels are called 'constructors'. So True is a constructor that takes no arguments. For a product type, we could for example create a type for an RGB colour triplet: The RGB label on the right-hand side is the constructor of the type. It takes three arguments of type Int: So aquamarine is a variable of type RGBColour with a value of RGB 127 255 212. The constructor identifies the type. Suppose we also have an HSL colour type with a variable chocolate of that type: then both RGB and HSL are triplets of Int but because of the different type constructors they are not the same type. Let's say we create an RGB Pixel type: then is fine but will be a type error because chocolate is of type HSLColour, not RGBColour. We could support both RGB and HSL using a sum type: and change make a Pixel type definition: And now we can say I can hear you say: but what about Int, it doesn't have constructors? And what about a string, how can that be an algebraic data type? These are interesting questions as they allow me to introduce two more concepts: recursive and polymorphic types. From a type perspective, you can look at an integer in two ways: if it is a fixed-size integer then the Int type can be seen as a sum type. For example, the type for an 8-bit unsigned integer could be In other words, every number is actually the name of a type constructor, as a generalisation of the Bool type. However, in the mathematical sense, integers are not finite. If we consider the case of the natural numbers, we can construct a type for them as follows: The Z stands for \"zero\", the S for \"successor of\". This is a recursive type, because the S constructor takes a Nat as argument. With this type, we can now create any natural number: This way of constructing the natural numbers is called Peano numbers. Now, what about strings? Enumerating all possible strings of any length is not practical. But from a type perspective, a string is a list of characters. So the question is then: what is the type of a list? For one thing, a list must be able to contain values of any type. (In the context of algebraic datatypes, all values must be the same, so our list is more like a typed array in Raku.) But that means we need types that can be parameterised by other types. This is called parametric polymorphism. So a list type must look something like where a is a type variable, i.e. it can be replaced by an arbitrary type. For example, assuming we define the Char type simply by enumerating all characters in the alphabet (because of course, at machine level, every character is represented by an integer number): Then we can type our string as: But what about List? We use a similar approach as for Nat above, using a recursive sum type: Now we can create a list of any length: Using the typical syntactic sugar for lists, we can write this as If I now invent an alias Str for List Char, and use double quotes instead of list notation, I can write So integers and strings can be expressed as algebraic data types, and now we have introduced recursive and parameterised types. These may seem like rather contrived examples, after all a language like Raku already has an Int and a Str type that work very well. So what is the use of these algebraic data types? Of course the purpose of static types is to provide type safety and make articles/debugging easier. But using algebraic data types also makes a different, more functional style of programming possible. One common use case is a list where you want to store values of different types: you can create a sum type that has an alternative for each of these types. Another common case is a recursive type, such as a tree. Finally, the polymorphism provides a convenient way to create custom containers. I will give examples of each of these in the next section. Time to move on to Raku! Algebraic data types in Raku As Raku is not a very well-known language (yet), here is a quick introduction of the features you'll need to follow the discussion below. Before Raku went its own way, it was meant to be the next iteration of Perl (hence the original name Perl 6). It is therefore more similar to Perl than to any other language. Raku is syntactically similar to C/C++, Java and JavaScript: block-based, with statements separated by semicolons, blocks demarcated by braces, and argument lists in parentheses and separated by commas. The main feature it shares with Perl is the use of sigils ('funny characters') which identify the type of a variable: $ for a scalar, @ for an array, % for a hash (map) and & for a subroutine. Variables also have keywords to identify their scope, I will only use my which marks the variable as lexically scoped. A subroutine is declared with the sub keyword, and subroutines can be named or anonymous: Raku also has twigils, secondary sigils that influence the scoping of a variable. For this article, the only twigil used in the code is . which is used to declare a role or class attribute with automatically generated accessors (like $.notes in the example below). Raku supports sigil-less variables, and uses the \\ syntax to declare them. For more on the difference between ordinary and sigil-less variables, see the Raku documentation. For example (say prints its argument followed by a newline): Raku has gradual typing: it allows both static and dynamic typing. That's a good start because we need static typing to support algebraic data types. It also has immutable variables and anonymous functions, and even (limited) laziness. And of course functions are first-class citizens, so we have everything we need for pure, statically-typed functional programming. But what about the algebraic data types? In Raku, enums are sum types: However, they are limited to type constructors that don't take any arguments. Classes can be seen as product types: However, classes do not support parametric polymorphism. This is where roles come in. According to the Raku documentation: Roles use the keyword role preceding the name of the role that is declared. Roles are mixed in using the does keyword preceding the name of the role that is mixed in. Roles are what in Python and Ruby is called mixins. So roles are basically classes that you can use to add behaviours to other classes without using inheritance. Here is a stripped-down example take from the Raku documentation (has declares an attribute, method a method) In particular, roles can be mixed into other roles, and that is one of the key features I will exploit. Furthermore, role constructors can take arguments and they are parametric. So we have everything we need to create proper algebraic data types. Let's look at a few examples. This is the example of a sum type for a Boolean as above, but implemented with roles. The first line declares the type as an empty role, this corresponds to the data type name on the left-hand side. The next lines define the alternatives, each alternative uses does OpinionatedBool to tie it to the OpinionatedBool role which functions purely as the type name. In Raku, types are values; and for a role with an empty body, you don't need the .new constructor call. In a sum type, the alternatives usually are labelled containers for values, but they can be empty containers as well. When that is the case, there is no need to create separate instances of them because there is only one way to have an empty container. Sum types can be used in combination with Raku's multi sub feature: Raku lets you provide several definitions for a function, with the same name but different signatures. With multi subs we can do what is known as pattern matching on types: Because we use a type as a value, to test if a value is AbsolutelyTrue or TotallyFalse, we can use either the smart match ~~, the container (type) identity =:= or the value (instance) identity === to test this (the smart match operator behaves like =:= if the right-hand side is a type and as === if it is an object instance). If we would create an instance like AbsolutelyTrue.new, this would not be the case. See the code example for more details. Here is the implementation of the Colour, XYCoord and Pixel types from above. The RGBColour type is an example of a product type. There are two differences with my notation from above: Because the role serves both as the type (RGBColour) and the instance constructor (RGB), they must have the same name. I only named them differently to make it easier to distinguish them so this is not an issue. The types that make up each field must be named with unique names in the role's argument list, and need to have a corresponding attributes declared. That is again not really a limitation, because accessors for record type fields are handy. So it looks like: (the role's parameters are in square brackets) And we create aquamarine like this: The definitions of HSLColour and XYCoord are analogous, you can find them in the code example. Let's look at the sum type to combine the RGB and HSL colour types: This is essentially the same approach as for the opinionated Boolean, but we don't have empty roles: the HSL alternative takes an argument of type HSLColour, and the RGB alternative takes an argument of type RGBColour. As in the product type, we use the role as a container to hold the values. The Pixel type from above looks like: And now we can create pixels with RGB and HSL colours: Above, I showed the Peano number type to illustrate type-level recursion. This works just fine with roles in Raku too: And we can combine this with type parameters as in the list example: (The prefix :: is the Raku syntax to declare type variables) There are some issues here: The EmptyList alternative must either be declared as above, with a type parameter, or as The EmptyList alternative must either be declared as above, with a type parameter, or as where the type also doesn't take a type variable. We can't write This is of course only a minor issue, resulting only in some redundancy. A more serious issue is that the type of lst must be List (or List[]) instead of List[a]. That is actually a problem, as it weakens the type checking. So it must be a bug in the current version of raku (2020.01). When I provide List[a] I get the following error: A more serious issue is that the type of lst must be List (or List[]) instead of List[a]. That is actually a problem, as it weakens the type checking. So it must be a bug in the current version of raku (2020.01). When I provide List[a] I get the following error: For the first example, I want to store values of different types in a typed array. They elements can be strings, labeled lists of strings, or undefined. I call this type Matches. Using the notation from above, it would be In Raku, it is defined as follows: This type uses type constructors with 0 (UndefinedMatch), 1 (Match) and 2 (TaggedMatches) arguments, and the latter is a recursive type: the second argument is a list of Matches. With this definition, we can create an array of matches like this: As you can see, the typed values are actually constructed by calling .new. It is a bit nicer to create constructor functions, and once Raku has a more developped macro system, we might be able to generate these automatically. Code for this example For the next example, I want to define a type called Either. This is a parametric sum type with two parameters, so a kind of generic tuple: In Raku, this can be done through the use of type variables as parameters for the role: Because Raku expects both type variables to be declared in each constructor, it is a little bit less nice than my more abstract notation. We can pattern match on this type with a multi sub: So we can write Code for this example As a final example, here is a simple binary tree. First, let's look at an example implementation using a role from the Raku documentation: This example contains quite a bit of Raku syntax: Raku allows dashes in names; the -> syntax is a foreach loop, iterating over all elements of the preceding list; The .. is array slicing; ::?CLASS is a compile-time type variable populated with the class you're in and :U is a type constraint which specifies that it should be interpreted as a type object. Finally, the : marks the argument to its left as the invocant. In other words, it allows us to write BinaryTree[Int].new-from-list(4, 5, 6) where BinaryTree[Int] is the value of::?CLASS`. This is the Raku way to create custom constructors. The * in front of the @el argument of new-from-list makes this a variadic function where @el contains all arguments; The => syntax allows to assign arguments by name rather than by position; ?? ... !! ... is Raku's syntax for C's ternary ? ... : ...; Raku allows dashes in names; the -> syntax is a foreach loop, iterating over all elements of the preceding list; The .. is array slicing; ::?CLASS is a compile-time type variable populated with the class you're in and :U is a type constraint which specifies that it should be interpreted as a type object. Finally, the : marks the argument to its left as the invocant. In other words, it allows us to write BinaryTree[Int].new-from-list(4, 5, 6) where BinaryTree[Int] is the value of::?CLASS`. This is the Raku way to create custom constructors. The * in front of the @el argument of new-from-list makes this a variadic function where @el contains all arguments; The => syntax allows to assign arguments by name rather than by position; ?? ... !! ... is Raku's syntax for C's ternary ? ... : ...; This example is written in Raku's object-oriented style, with methods acting on the attributes of the role. Let's see how we can write this in a functional style. The algebraic data type for this binary tree is: The Tip alternative is for the empty leaf nodes of the tree, which in the above example are left undefined. In Raku, we can implement this type as: Instead of the methods we use functions, implemented as multi subs. Most of the code is of course identical, but there is no need for conditionals to check if a leaf node has been reached. I have also used sigil-less immutable variables. One thing to note is that in the multi subs we don't have to match against the full type, for example in visit-preorder we match against Tip and Node rather than the full Tip[a] and Node[::a,BinaryTree[a],BinaryTree[a],a]. Code for this example Wrap-up Creating algebraic data types with Raku's roles is very straightforward. Any product type is simply a role with a number of typed attributes. The key idea for the sum type is to create an empty role and mix it in with other roles that become the type constructors for your alternatives. Because roles accept type parameters, we can have parametric polymorphism. And because a role can have attributes of its own type, we have recursive types as well. Combined with Raku's other functional programming features, this makes writing pure, statically typed functional code in Raku great fun. [1] \"The algebra (and calculus!) of algebraic data types\", by Joel Burget [2] \"The Algebra of Algebraic Data Types, Part 1\", by Chris Taylor Updated June 05, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2020 Jekyll theme Skinny Bones","tags":"","url":"articles/roles-as-adts-in-raku/index.html"}, {"title":"List-based parser combinators in Haskell and Raku","text":" List-based parser combinators in Haskell and Raku This is a follow-on from my article introducing algebraic data types and explaining how to implement them in Raku. If you are not familiar with algebraic data types, I suggest you read that article first. In this article I use algebraic data types to create a statically typed version of a list-based parser combinators library which I originally created for dynamic languages. The article introduces list-based parser combinators are and how to implement them in Raku and Haskell using algebraic data types. Perl, Haskell and Raku: a quick introduction The code examples in this article are written in Perl, Haskell and Raku. If you are familiar with these languages, you can skip this section. The code is written in a functional style and is not very idiomatic so you should be able to understand it easily if you know another programming language. Perl and Raku are syntactically similar to C/C++, Java and JavaScript: block-based, with statements separated by semicolons, blocks demarcated by braces, and argument lists in parentheses and separated by commas. The main feature that sets Perl and Raku apart is the use of sigils ('funny characters') which identify the type of a variable: $ for a scalar, @ for an array, % for a hash (map) and & for a subroutine. Variables also have keywords to identify their scope, I will only use my which marks the variable as lexically scoped. A subroutine is declared with the sub keyword, and subroutines can be named or anonymous: Haskell is whitespace-sensitive like Python, but has a markedly different syntax. Because everything is a function, there is no keyword to mark a function; because there is only lexical scope, there is no need for any special scope identifiers. Function arguments are separated by spaces; anonymous functions have a special syntax to identify them: Several of the examples use the let ... in ... construct, which behaves as a lexically scoped block: x0, x1 and x2 are in scope only in the expression after the in keyword. Haskell is statically typed, and the type of a function or variable is written in a separate annotation, for example: The isEmpty function has a type signature, identified by ::, that reads \"isEmpty is a function from a list of anything to a Boolean\". Types must be written with an initial capital. The a is a type variable which can take on any type, as explained in my previous post. Raku has optional typing: you can add type information as part of the declarations, for example: This function takes array of any type and an integer and returns a Boolean. Other specific bits of syntax or functionality will be explained for the particular examples. Parsers and parser combinators What I call a parser here is technically a combination of a lexer or tokeniser and a parser. The lexical analysis (splitting a sequence of characters into a sequence of tokens, strings with an identified meaning) meaning and parsing (syntactic analysis, analysing the sequence of tokens in terms of a formal grammar) are not separate stages. Parser combinators are building blocks to create parsers by combining small parsers into very complex ones. In Haskell they became popular because of the Parsec library. This library provides monadic parser combinators. My parser combinator library implements a subset of Parsec's functionality. I am not going to explain what monads are because the point of creating list-based parser combinators is precisely that they do not require monads. There is a connection however, and you can read about it in my paper if you're interested. I created the original version of the list-based parser combinators library for dynamically typed languages: there are versions in Perl, Python and LiveScript, a Haskell-like language which compiles to JavaScript. Because I like Raku and it has gradual typing, I was interested in what a statically typed Raku version would look like. As a little detour I first implemented them in Haskell, just for fun really. Raku has Grammars, which also let you build powerful parsers. If you are familiar with them it will be interesting to compare the parser combinator approach to the inheritance mechanism used to compose Grammars. List-based parser combinators So what are list-based parser combinators? Let's say I want to parse a string containing this very simple bit of code: We have an identifier, an assignment operator and a natural number, maybe preceded by whitespace, and with some whitespace that doesn't matter between these tokens. I am assuming that the string which we want to parse is code written in a language which is whitespace-insensitive. What I would like is that I can write a parser for as close as possible to the following: And when I apply assignParser to assignStr, it should return the parsed tokens, a status, and the remainder of the string, if any. So each parser takes a string and returns this triplet of values (I'll call it a tuple instead of a triplet). We'll define this more formally in the next sections. What we have here is that the list acts as the sequencing combinator for the specific parsers. The maybe is a combinator to make the token optional. We can provide more combinators, such as choice, (to try several parsers), many (to repeatedly apply a parser), etc. And because every parser is a function, complex parsers can easily be composed of smaller parsers expressed in terms of the building blocks. For example, if we had many assignments, we could have something like Now suppose that we want to extend our parser to include declarations, something like which we parse as then we need to add get the following parser: It is also essential that we can label tokens or groups of tokens, so that we can easily extract the relevant information from a parse tree, as the intermediate step in transforming the parse tree into an abstract syntax tree. In the above example, we are only interested in the variable name and the value. The whitespace and equal sign are not important. So we could label the relevant tokens, for example: Implementation in a dynamically typed language How do we implement the above mechanism in a dynamically typed language? A parser like identifier is simply a function which takes a string and returns a tuple. In Perl, the code looks like this: (In Perl, =~ /.../ is the regular expression matching syntax, and s/.../.../ is regular expression substitution.) But what about a parser like symbol? It takes the string representing the symbol as an argument, so in the example, symbol( \"=\" ) should be the actual parser. What we need is that a call to symbol will return a function to do the parsing, like this: This is of course not limited to string arguments, any argument of the outer function can be used inside the inner function. In particular, if a parser combinator takes parsers as arguments, like choice and maybe, then these parsers can be passed on to the inner function. This is fine as far as it goes, but what about the labelled parsers? and what about the lists of parsers? Neither of these can be directly applied to a string, but neither is a function that can generate a function either. So to apply them to a string, we will need to get the parsers out of label-parser pair and the list. We do that using a helper function which I call apply, and which in Perl looks like (The syntax $p->($str) applies the anonymous function referenced by $p to its arguments.) This function checks the type of $p using the ref built-in: it can either be code (i.e. a subroutine), an array or a hash. If it's subroutine it's applied directly to the string, otherwise it calls sequence or retag. The foldl function is my Perl version of the left-to-right reduction in Haskell or reduce in Raku. What retag does is taking the parser from the label pair (which is a single-element hash), apply it to the string, and label the resulting matches with the label of the parser: (The syntax %{$p} is dereferencing, a bit like the * prefix in C.) Here is a simple example of how to use the list-based parser combinators. Implementation in a language with algebraic data types All of the above is fine in a dynamically typed language, but in a statically typed language, the list can't contain a function and a hash and another list, as they all have different types. Also, only testing if an entry of the list is code, hash or list is rather weak, as it does not guarantee that the code is an actual parser. So let's see what it looks like in Haskell and Raku. list-based parser combinators In Haskell, we start (of course) by defining a few types: (In Haskell, data and newtype define a new algebraic datatype; type defines an alias for an existing type.) So our list of parsers will be a list of LComb, and this can be a parser, sequence of parsers or tagged pair. Because the tag eventually is used to label the matched string, the Match type also has a tagged variant. In principle, the return type of the parser could just be a tuple, but I define the MTup polymorphic type so I can make it an instance of a type class later on, e.g. to make it a monad. With these types we can define our parser combinators and the apply and sequence functions. Here is the symbol parser. Many of the parsers in the library are implemented using Perl-Compatible Regular Expressions (Text.Regex.PCRE), because what else can you expect of a lamdacamel. (The $ behaves like an opening parenthesis that does not need a closing parenthesis; ++ is the list concatenation operator, in Haskell strings are lists of characters.) The apply function pattern matches against the type alternatives for LComb. Because of the pattern matching there is not need for an untag function. It is clear from this implementation that we can write a sequence of parsers both as Seq [...] or sequence [...]. Apart from the static typing, the sequence function is very close to the Perl version. That is of course because I wrote the Perl code in a functional style. In Raku, I use roles as algebraic datatypes as explained in my previous post. Essentially, each alternative of a sum types mixes in an empty role which is only used to name the type; product types are just roles with some attributes that are declared in the role's parameter list. The way the Raku regular expressions are used in this implementation of symbol is closer to the Haskell version than the Perl 5 version. But the main difference with the Perl 5 version is that the combinator and the return tuple are statically typed. The function undef-match is a convenience to return an array with an undefined match. As explained in my previous post, we use Raku's multi subs for pattern matching on the types. In Haskell this is also possible, and the definition of apply can be rewritten as: The Raku version of apply is quite close to this Haskell version. For convenience, I use a function unmtup to unpack the MTup. Finally, the sequence code in Raku. It follows closely the structure of the Perl 5 and Haskell versions. Raku's reduce is equivalent to Haskell's foldl. There is still a minor issue with this definition of sequence: Because of the signature, we can't write Instead, we would have to write which is a bit tedious. So I rename sequence to sequence_ and wrap it in a new function sequence which has a 'slurpy' argument (i.e. it is variadic) like this: I do the same for all combinators that take a list of combinators as argument. If you wanted to type check the arguments of the wrapper function, you could do this with a where clause: The type constructor based tagging (Tag label parser) is nice in Haskell but in Raku it would look like Tag[label, parser].new which I don't like. Therefore, I wrap the constructor in a tag function so I can write tag(label, parser). An example of typical usage As an example, we can construct the following parser for a part of a Fortran 90-style variable declaration, apply it to the given string and get the parse tree: (In Raku, variables declared with a \\ are sigil-less ) For reference, here is the Haskell code: (If you wonder about the strange signature of main, the do keyword or the let without an in, the answers are here. Or you could take my free online course.) As is clear from this example, in both languages, list-based parser combinators provide a clean and highly composable way of constructing powerful and complex parsers. It is also quite easy to extend the library with additional parsers. I think this is a nice practical application of algebraic data types in particular and functional programming in general. You can find both the Haskell code and the Raku code in my repo. Updated June 22, 2020 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2020 Jekyll theme Skinny Bones","tags":"","url":"articles/list-based-parser-combinators/index.html"}, ]};